{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Input, Normalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading & Processing input data after extraction from the .mlx script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.txt')\n",
    "df_val = pd.read_csv('val.txt')\n",
    "df_test = pd.read_csv('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "df_train['X_real'] = df_train['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_train['X_img'] = df_train['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "df_train['y_real'] = df_train['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_train['y_img'] = df_train['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "\n",
    "# test\n",
    "df_test['X_real'] = df_test['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_test['X_img'] = df_test['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "df_test['y_real'] = df_test['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_test['y_img'] = df_test['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "\n",
    "# val\n",
    "df_val['X_real'] = df_val['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_val['X_img'] = df_val['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "df_val['y_real'] = df_val['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_val['y_img'] = df_val['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[['X_real','X_img']].to_numpy()\n",
    "y_train = df_train[['y_real','y_img']].to_numpy()\n",
    "\n",
    "X_test = df_test[['X_real','X_img']].to_numpy()\n",
    "y_test = df_test[['y_real','y_img']].to_numpy()\n",
    "\n",
    "X_val = df_val[['X_real','X_img']].to_numpy()\n",
    "y_val = df_val[['y_real','y_img']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(arr, n_previous = 4):\n",
    "    \"\"\"\n",
    "    Preprocesses the input array by computing the L1 norm, adding padding, \n",
    "    and creating sliding windows.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    arr : numpy.ndarray\n",
    "        A 2D array of shape n*2 where each row represents a data sample, two columns for real & imaginary parts of the signal.\n",
    "    n_previous : int, optional\n",
    "        The number of previous samples to include in each window (default is 4).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    windows : numpy.ndarray\n",
    "        A 2D array where each row contains a flattened window of `n_previous + 1` \n",
    "        consecutive samples from the padded array, including the computed L1 norm \n",
    "        for each original sample.\n",
    "    \"\"\"\n",
    "    array_with_l1_norm  = np.hstack((arr, np.sum(np.abs(arr),axis=1)[:, np.newaxis]))\n",
    "\n",
    "    padding = np.zeros((n_previous, array_with_l1_norm.shape[1]))\n",
    "    padded_array = np.vstack((padding, array_with_l1_norm ))\n",
    "\n",
    "    windows = sliding_window_view(padded_array, window_shape=(n_previous + 1, array_with_l1_norm.shape[1]))\n",
    "    windows = windows.reshape(windows.shape[0], -1)\n",
    "    \n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the additional processing\n",
    "X_train = preprocess(X_train)\n",
    "X_test = preprocess(X_test)\n",
    "X_val = preprocess(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131520, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helperNMSE(y_true, y_pred):\n",
    "    \n",
    "    diff = y_pred - y_true\n",
    "    mse = tf.reduce_mean(tf.norm(diff,axis=1)**2) # NOTE THIS IS NOT A GOOD PRACICE AS TF.NORM ALREADY GETS THE SQUARED ERROR THEN SQRT IT\n",
    "    factor = tf.reduce_mean(tf.norm(y_true,axis=1)**2)\n",
    "    # nmse = 10 * tf.math.log(mse / factor) / tf.math.log(tf.constant(10,dtype=tf.float32))\n",
    "    \n",
    "    return mse\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Compute the L1 norms\n",
    "    norm_true = tf.reduce_sum(tf.abs(y_true), axis=1)\n",
    "    norm_pred = tf.reduce_sum(tf.abs(y_pred), axis=1)\n",
    "    \n",
    "    # Reshape to be compatible with the helperNMSE function\n",
    "    norm_true = tf.reshape(norm_true, (-1, 1))\n",
    "    norm_pred = tf.reshape(norm_pred, (-1, 1))\n",
    "    \n",
    "    # Compute the NMSE\n",
    "    loss = helperNMSE(norm_true, norm_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining layers\n",
    "n_nerurons = 30\n",
    "factor = 0.8\n",
    "\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "dense_layer_1 = Dense(units = n_nerurons, activation = LeakyReLU(alpha=0.01))(input_layer) \n",
    "dense_layer_2 = Dense(units = n_nerurons*factor, activation = LeakyReLU(alpha=0.01))(dense_layer_1)\n",
    "dense_layer_3 = Dense(units = n_nerurons*(factor**2), activation = LeakyReLU(alpha=0.01))(dense_layer_2)\n",
    "\n",
    "\n",
    "#Y1 output (real part of the signal)\n",
    "y1_output = Dense(units = 1, activation = \"linear\", name = \"y1_output\")(dense_layer_3)\n",
    "\n",
    "#Y2 output (imaginary part of the signal)\n",
    "y2_output = Dense(units = 1, activation = \"linear\", name = \"y2_output\")(dense_layer_3)\n",
    "\n",
    "#Define the model with the input layer and a list of outputs\n",
    "model = Model(inputs = input_layer, outputs = [y1_output, y2_output])\n",
    "\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=5, verbose=1, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='bestmodel.keras',\n",
    "    monitor='val_loss',       # Metric to monitor\n",
    "    save_best_only=True,      # Only save the model if it is the best\n",
    "    verbose=0                 # Verbosity mode\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#specify the optimizer and compile with the loss function for both outputs\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=4e-4)\n",
    "\n",
    "model.compile(optimizer = optimizer,\n",
    "              loss = {'y1_output':'mse','y2_output':'mse'},\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "maxEpochs = 700 \n",
    "miniBatchSize = 1024\n",
    "iterPerEpoch = len(X_train) // miniBatchSize\n",
    "validation_freq = 2 * iterPerEpoch\n",
    "\n",
    "\n",
    "history = model.fit(X_train, (y_train.y_real, y_train.y_img), epochs = maxEpochs, batch_size = 10,\n",
    "                    validation_data = (X_val, (y_val.y_real, y_val.y_img)))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, (y_train[:,0], y_train[:,1]),\n",
    "    epochs=maxEpochs,\n",
    "    batch_size=miniBatchSize,\n",
    "    callbacks=[checkpoint_callback,reduce_lr],\n",
    "    validation_data = (X_val, (y_val[:,0], y_val[:,1])),\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "model.save('tf_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after many training attemps, the best model achieved -33.55dB accuracy, thus loaded here\n",
    "model.load_weights('tf_model_33_5dB_nprev_4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the model to the y_test data and saving the output for input into the simulated PA in the .mlx script, as showing in the following test bench\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model test bench](tb.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output processing\n",
    "tf_output = model.predict(preprocess(y_test))\n",
    "tf_output = np.concatenate((tf_output[0],tf_output[1]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "\n",
    "for i in range(len(tf_output)):\n",
    "    arr.append(f'{tf_output[i][0]} + {tf_output[i][1]}i')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(arr).to_csv('tf_model_output_test.csv',index=False,header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the model output to .mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "# Function to parse complex numbers from the given format\n",
    "def parse_complex_number(s):\n",
    "    real, imag = s.split(' + ')\n",
    "    real = float(real)\n",
    "    imag = float(imag.replace('i', ''))\n",
    "    return np.complex64(real + 1j * imag)\n",
    "\n",
    "# Read the CSV file\n",
    "filename = './tf_model_output_test.csv'  # Replace with your CSV filename\n",
    "with open(filename, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Parse the lines into complex numbers\n",
    "complex_numbers = np.array([parse_complex_number(line.strip()) for line in lines], dtype=np.complex64)\n",
    "\n",
    "# Save the array to a .mat file\n",
    "output_filename = 'complex_data.mat'\n",
    "scipy.io.savemat(output_filename, {'complex_data': complex_numbers})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
