{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Input, Normalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tf.config.list_physical_devices())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.txt')\n",
    "df_val = pd.read_csv('val.txt')\n",
    "df_test = pd.read_csv('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "df_train['X_real'] = df_train['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_train['X_img'] = df_train['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "df_train['y_real'] = df_train['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_train['y_img'] = df_train['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "\n",
    "# test\n",
    "df_test['X_real'] = df_test['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_test['X_img'] = df_test['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "df_test['y_real'] = df_test['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_test['y_img'] = df_test['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "\n",
    "# val\n",
    "df_val['X_real'] = df_val['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_val['X_img'] = df_val['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "df_val['y_real'] = df_val['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_val['y_img'] = df_val['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[['X_real','X_img']].to_numpy()\n",
    "y_train = df_train[['y_real','y_img']].to_numpy()\n",
    "\n",
    "X_test = df_test[['X_real','X_img']].to_numpy()\n",
    "y_test = df_test[['y_real','y_img']].to_numpy()\n",
    "\n",
    "X_val = df_val[['X_real','X_img']].to_numpy()\n",
    "y_val = df_val[['y_real','y_img']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(arr, n_previous = 6):\n",
    "    array_with_l1_norm  = np.hstack((arr, np.sum(np.abs(arr),axis=1)[:, np.newaxis]))\n",
    "\n",
    "    padding = np.zeros((n_previous, array_with_l1_norm.shape[1]))\n",
    "    padded_array = np.vstack((padding, array_with_l1_norm ))\n",
    "\n",
    "    windows = sliding_window_view(padded_array, window_shape=(n_previous + 1, array_with_l1_norm.shape[1]))\n",
    "    windows = windows.reshape(windows.shape[0], -1)\n",
    "    \n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess(X_train)\n",
    "X_test = preprocess(X_test)\n",
    "X_val = preprocess(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131520, 21)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "\n",
    "# x_min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# y_min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# X_train = x_min_max_scaler.fit_transform(X_train)\n",
    "# y_train = y_min_max_scaler.fit_transform(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val = x_min_max_scaler.transform(X_val)\n",
    "# y_val = y_min_max_scaler.transform(y_val)\n",
    "\n",
    "# X_test = x_min_max_scaler.transform(X_test)\n",
    "# y_test = y_min_max_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the model using Sequential API\n",
    "# model = Sequential([\n",
    "#     Dense(30),\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     Dense(30),\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     Dense(30),\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     Dense(2)\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=Adam(learning_rate=4e-4), \n",
    "#               loss='mean_squared_error', \n",
    "#               metrics=['mean_squared_error'])\n",
    "\n",
    "# # Define training parameters\n",
    "# maxEpochs = 200\n",
    "# miniBatchSize = 1024\n",
    "# iterPerEpoch = len(X_train) // miniBatchSize\n",
    "# validation_freq = 2 * iterPerEpoch\n",
    "\n",
    "# # Callbacks for learning rate adjustment, early stopping, and model checkpoint\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=5, verbose=1, mode='auto')\n",
    "# # early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     epochs=maxEpochs,\n",
    "#     batch_size=miniBatchSize,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     callbacks=[reduce_lr],\n",
    "#     shuffle=True,\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helperNMSE(y_true, y_pred):\n",
    "    \n",
    "    diff = y_pred - y_true\n",
    "    mse = tf.reduce_mean(tf.norm(diff,axis=1)**2) # NOTE THIS IS NOT A GOOD PRACICE AS TF.NORM ALREADY GETS THE SQUARED ERROR THEN SQRT IT\n",
    "    factor = tf.reduce_mean(tf.norm(y_true,axis=1)**2)\n",
    "    # nmse = 10 * tf.math.log(mse / factor) / tf.math.log(tf.constant(10,dtype=tf.float32))\n",
    "    \n",
    "    return mse\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Compute the L1 norms\n",
    "    norm_true = tf.reduce_sum(tf.abs(y_true), axis=1)\n",
    "    norm_pred = tf.reduce_sum(tf.abs(y_pred), axis=1)\n",
    "    \n",
    "    # Reshape to be compatible with the helperNMSE function\n",
    "    norm_true = tf.reshape(norm_true, (-1, 1))\n",
    "    norm_pred = tf.reshape(norm_pred, (-1, 1))\n",
    "    \n",
    "    # Compute the NMSE\n",
    "    loss = helperNMSE(norm_true, norm_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a  = tf.convert_to_tensor([[2,1],[5,6]],dtype=tf.float32)\n",
    "# b = tf.convert_to_tensor([[2,5],[2,4]],dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helperNMSE(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abood\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# defining layers\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "dense_layer_1 = Dense(units = 30, activation = LeakyReLU(alpha=0.01))(input_layer) \n",
    "dense_layer_2 = Dense(units = 24, activation = LeakyReLU(alpha=0.01))(dense_layer_1)\n",
    "dense_layer_3 = Dense(units = 19, activation = LeakyReLU(alpha=0.01))(dense_layer_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Y1 output\n",
    "y1_output = Dense(units = 1, activation = \"linear\", name = \"y1_output\")(dense_layer_3)\n",
    "\n",
    "#Y2 output\n",
    "y2_output = Dense(units = 1, activation = \"linear\", name = \"y2_output\")(dense_layer_3)\n",
    "\n",
    "#Define the model with the input layer and a list of outputs\n",
    "model = Model(inputs = input_layer, outputs = [y1_output, y2_output])\n",
    "\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=5, verbose=1, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='bestmodel.keras',\n",
    "    monitor='val_loss',       # Metric to monitor\n",
    "    save_best_only=True,      # Only save the model if it is the best\n",
    "    verbose=0                 # Verbosity mode\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#specify the optimizer and compile with the loss function for both outputs\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=4e-4)\n",
    "\n",
    "model.compile(optimizer = optimizer,\n",
    "              loss = {'y1_output':'mse','y2_output':'mse'},\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('tf_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.8865e-05 - val_loss: 4.7298e-06 - learning_rate: 4.0000e-04\n",
      "Epoch 2/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8754e-06 - val_loss: 2.1625e-06 - learning_rate: 4.0000e-04\n",
      "Epoch 3/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8957e-06 - val_loss: 1.2123e-06 - learning_rate: 4.0000e-04\n",
      "Epoch 4/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0942e-06 - val_loss: 7.7483e-07 - learning_rate: 4.0000e-04\n",
      "Epoch 5/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.6369e-07 - val_loss: 6.6832e-07 - learning_rate: 4.0000e-04\n",
      "Epoch 6/700\n",
      "\u001b[1m 96/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.2019e-07\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00037999999040039256.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1018e-07 - val_loss: 5.1128e-07 - learning_rate: 4.0000e-04\n",
      "Epoch 7/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0844e-07 - val_loss: 4.5547e-07 - learning_rate: 3.8000e-04\n",
      "Epoch 8/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6449e-07 - val_loss: 4.0768e-07 - learning_rate: 3.8000e-04\n",
      "Epoch 9/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1908e-07 - val_loss: 3.9134e-07 - learning_rate: 3.8000e-04\n",
      "Epoch 10/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8866e-07 - val_loss: 3.8716e-07 - learning_rate: 3.8000e-04\n",
      "Epoch 11/700\n",
      "\u001b[1m 93/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7432e-07\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0003609999839682132.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7211e-07 - val_loss: 3.3740e-07 - learning_rate: 3.8000e-04\n",
      "Epoch 12/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3991e-07 - val_loss: 3.0015e-07 - learning_rate: 3.6100e-04\n",
      "Epoch 13/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1093e-07 - val_loss: 2.9346e-07 - learning_rate: 3.6100e-04\n",
      "Epoch 14/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9893e-07 - val_loss: 2.9474e-07 - learning_rate: 3.6100e-04\n",
      "Epoch 15/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8230e-07 - val_loss: 2.7395e-07 - learning_rate: 3.6100e-04\n",
      "Epoch 16/700\n",
      "\u001b[1m104/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6919e-07\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.00034294998476980254.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6872e-07 - val_loss: 3.0264e-07 - learning_rate: 3.6100e-04\n",
      "Epoch 17/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6657e-07 - val_loss: 2.3273e-07 - learning_rate: 3.4295e-04\n",
      "Epoch 18/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4859e-07 - val_loss: 2.2404e-07 - learning_rate: 3.4295e-04\n",
      "Epoch 19/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3886e-07 - val_loss: 2.1754e-07 - learning_rate: 3.4295e-04\n",
      "Epoch 20/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3357e-07 - val_loss: 2.1353e-07 - learning_rate: 3.4295e-04\n",
      "Epoch 21/700\n",
      "\u001b[1m 94/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.2213e-07\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00032580249244347216.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2308e-07 - val_loss: 2.1954e-07 - learning_rate: 3.4295e-04\n",
      "Epoch 22/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2244e-07 - val_loss: 1.9625e-07 - learning_rate: 3.2580e-04\n",
      "Epoch 23/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0777e-07 - val_loss: 1.9155e-07 - learning_rate: 3.2580e-04\n",
      "Epoch 24/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9400e-07 - val_loss: 2.1784e-07 - learning_rate: 3.2580e-04\n",
      "Epoch 25/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9519e-07 - val_loss: 2.1380e-07 - learning_rate: 3.2580e-04\n",
      "Epoch 26/700\n",
      "\u001b[1m 98/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.0019e-07\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00030951235676184296.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9904e-07 - val_loss: 1.7755e-07 - learning_rate: 3.2580e-04\n",
      "Epoch 27/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8190e-07 - val_loss: 1.7944e-07 - learning_rate: 3.0951e-04\n",
      "Epoch 28/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7633e-07 - val_loss: 1.6789e-07 - learning_rate: 3.0951e-04\n",
      "Epoch 29/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7985e-07 - val_loss: 1.9291e-07 - learning_rate: 3.0951e-04\n",
      "Epoch 30/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7904e-07 - val_loss: 1.5953e-07 - learning_rate: 3.0951e-04\n",
      "Epoch 31/700\n",
      "\u001b[1m 90/129\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.6357e-07\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00029403674998320636.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6470e-07 - val_loss: 1.5164e-07 - learning_rate: 3.0951e-04\n",
      "Epoch 32/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5931e-07 - val_loss: 1.4425e-07 - learning_rate: 2.9404e-04\n",
      "Epoch 33/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5462e-07 - val_loss: 1.5406e-07 - learning_rate: 2.9404e-04\n",
      "Epoch 34/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5583e-07 - val_loss: 1.3778e-07 - learning_rate: 2.9404e-04\n",
      "Epoch 35/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4849e-07 - val_loss: 1.7341e-07 - learning_rate: 2.9404e-04\n",
      "Epoch 36/700\n",
      "\u001b[1m 97/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.4587e-07\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.00027933491801377384.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4634e-07 - val_loss: 1.6203e-07 - learning_rate: 2.9404e-04\n",
      "Epoch 37/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4723e-07 - val_loss: 1.2885e-07 - learning_rate: 2.7933e-04\n",
      "Epoch 38/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3878e-07 - val_loss: 1.2721e-07 - learning_rate: 2.7933e-04\n",
      "Epoch 39/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4068e-07 - val_loss: 1.2657e-07 - learning_rate: 2.7933e-04\n",
      "Epoch 40/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3621e-07 - val_loss: 1.5897e-07 - learning_rate: 2.7933e-04\n",
      "Epoch 41/700\n",
      "\u001b[1m 88/129\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3428e-07\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00026536818040767685.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3398e-07 - val_loss: 1.2322e-07 - learning_rate: 2.7933e-04\n",
      "Epoch 42/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2736e-07 - val_loss: 1.1792e-07 - learning_rate: 2.6537e-04\n",
      "Epoch 43/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2234e-07 - val_loss: 1.3420e-07 - learning_rate: 2.6537e-04\n",
      "Epoch 44/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3127e-07 - val_loss: 1.1578e-07 - learning_rate: 2.6537e-04\n",
      "Epoch 45/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2349e-07 - val_loss: 1.3044e-07 - learning_rate: 2.6537e-04\n",
      "Epoch 46/700\n",
      "\u001b[1m 89/129\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2476e-07\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.00025209976447513325.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2336e-07 - val_loss: 1.2196e-07 - learning_rate: 2.6537e-04\n",
      "Epoch 47/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1946e-07 - val_loss: 1.1870e-07 - learning_rate: 2.5210e-04\n",
      "Epoch 48/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1726e-07 - val_loss: 1.1894e-07 - learning_rate: 2.5210e-04\n",
      "Epoch 49/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2313e-07 - val_loss: 1.0776e-07 - learning_rate: 2.5210e-04\n",
      "Epoch 50/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1728e-07 - val_loss: 1.1706e-07 - learning_rate: 2.5210e-04\n",
      "Epoch 51/700\n",
      "\u001b[1m 94/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1621e-07\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.00023949477763380854.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1584e-07 - val_loss: 1.0653e-07 - learning_rate: 2.5210e-04\n",
      "Epoch 52/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0848e-07 - val_loss: 1.0211e-07 - learning_rate: 2.3949e-04\n",
      "Epoch 53/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0692e-07 - val_loss: 1.0938e-07 - learning_rate: 2.3949e-04\n",
      "Epoch 54/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0652e-07 - val_loss: 1.0057e-07 - learning_rate: 2.3949e-04\n",
      "Epoch 55/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1058e-07 - val_loss: 1.0492e-07 - learning_rate: 2.3949e-04\n",
      "Epoch 56/700\n",
      "\u001b[1m 91/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0550e-07\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.000227520041516982.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0541e-07 - val_loss: 9.9549e-08 - learning_rate: 2.3949e-04\n",
      "Epoch 57/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0490e-07 - val_loss: 1.0104e-07 - learning_rate: 2.2752e-04\n",
      "Epoch 58/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0298e-07 - val_loss: 1.1206e-07 - learning_rate: 2.2752e-04\n",
      "Epoch 59/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0301e-07 - val_loss: 9.8474e-08 - learning_rate: 2.2752e-04\n",
      "Epoch 60/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0285e-07 - val_loss: 9.5164e-08 - learning_rate: 2.2752e-04\n",
      "Epoch 61/700\n",
      "\u001b[1m 93/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.9407e-08\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.00021614403667626902.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.9964e-08 - val_loss: 9.2921e-08 - learning_rate: 2.2752e-04\n",
      "Epoch 62/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.6067e-08 - val_loss: 1.0269e-07 - learning_rate: 2.1614e-04\n",
      "Epoch 63/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.8493e-08 - val_loss: 9.7622e-08 - learning_rate: 2.1614e-04\n",
      "Epoch 64/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.9784e-08 - val_loss: 9.5355e-08 - learning_rate: 2.1614e-04\n",
      "Epoch 65/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.5713e-08 - val_loss: 8.8458e-08 - learning_rate: 2.1614e-04\n",
      "Epoch 66/700\n",
      "\u001b[1m 91/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.3510e-08\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 0.00020533683346002363.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.3690e-08 - val_loss: 8.7905e-08 - learning_rate: 2.1614e-04\n",
      "Epoch 67/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.1705e-08 - val_loss: 9.2613e-08 - learning_rate: 2.0534e-04\n",
      "Epoch 68/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.8307e-08 - val_loss: 9.3109e-08 - learning_rate: 2.0534e-04\n",
      "Epoch 69/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.1855e-08 - val_loss: 8.5102e-08 - learning_rate: 2.0534e-04\n",
      "Epoch 70/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.4655e-08 - val_loss: 8.6474e-08 - learning_rate: 2.0534e-04\n",
      "Epoch 71/700\n",
      "\u001b[1m 84/129\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.9499e-08\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0001950699952431023.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9031e-08 - val_loss: 8.5459e-08 - learning_rate: 2.0534e-04\n",
      "Epoch 72/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7685e-08 - val_loss: 8.9969e-08 - learning_rate: 1.9507e-04\n",
      "Epoch 73/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.8956e-08 - val_loss: 9.5368e-08 - learning_rate: 1.9507e-04\n",
      "Epoch 74/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9239e-08 - val_loss: 8.2707e-08 - learning_rate: 1.9507e-04\n",
      "Epoch 75/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6104e-08 - val_loss: 1.0139e-07 - learning_rate: 1.9507e-04\n",
      "Epoch 76/700\n",
      "\u001b[1m 95/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.7502e-08\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0001853164954809472.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7441e-08 - val_loss: 9.5689e-08 - learning_rate: 1.9507e-04\n",
      "Epoch 77/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.4598e-08 - val_loss: 8.1687e-08 - learning_rate: 1.8532e-04\n",
      "Epoch 78/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.5112e-08 - val_loss: 8.3070e-08 - learning_rate: 1.8532e-04\n",
      "Epoch 79/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3789e-08 - val_loss: 8.4100e-08 - learning_rate: 1.8532e-04\n",
      "Epoch 80/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3265e-08 - val_loss: 8.8572e-08 - learning_rate: 1.8532e-04\n",
      "Epoch 81/700\n",
      "\u001b[1m 95/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.1486e-08\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 0.00017605067623662762.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1936e-08 - val_loss: 8.5510e-08 - learning_rate: 1.8532e-04\n",
      "Epoch 82/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3904e-08 - val_loss: 7.6497e-08 - learning_rate: 1.7605e-04\n",
      "Epoch 83/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1338e-08 - val_loss: 7.9997e-08 - learning_rate: 1.7605e-04\n",
      "Epoch 84/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8696e-08 - val_loss: 7.7293e-08 - learning_rate: 1.7605e-04\n",
      "Epoch 85/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9251e-08 - val_loss: 7.5222e-08 - learning_rate: 1.7605e-04\n",
      "Epoch 86/700\n",
      "\u001b[1m 92/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.8309e-08\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 0.00016724813758628443.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8830e-08 - val_loss: 8.0180e-08 - learning_rate: 1.7605e-04\n",
      "Epoch 87/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8294e-08 - val_loss: 7.5883e-08 - learning_rate: 1.6725e-04\n",
      "Epoch 88/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7236e-08 - val_loss: 8.1793e-08 - learning_rate: 1.6725e-04\n",
      "Epoch 89/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7904e-08 - val_loss: 8.6915e-08 - learning_rate: 1.6725e-04\n",
      "Epoch 90/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0271e-08 - val_loss: 7.1948e-08 - learning_rate: 1.6725e-04\n",
      "Epoch 91/700\n",
      "\u001b[1m 92/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.5009e-08\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 0.00015888572379481046.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.4955e-08 - val_loss: 7.2309e-08 - learning_rate: 1.6725e-04\n",
      "Epoch 92/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5360e-08 - val_loss: 8.1019e-08 - learning_rate: 1.5889e-04\n",
      "Epoch 93/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.6669e-08 - val_loss: 7.2844e-08 - learning_rate: 1.5889e-04\n",
      "Epoch 94/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2885e-08 - val_loss: 7.0278e-08 - learning_rate: 1.5889e-04\n",
      "Epoch 95/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3852e-08 - val_loss: 8.1627e-08 - learning_rate: 1.5889e-04\n",
      "Epoch 96/700\n",
      "\u001b[1m119/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.4552e-08\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 0.00015094144036993383.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.4419e-08 - val_loss: 7.4002e-08 - learning_rate: 1.5889e-04\n",
      "Epoch 97/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2615e-08 - val_loss: 7.2570e-08 - learning_rate: 1.5094e-04\n",
      "Epoch 98/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2013e-08 - val_loss: 6.9771e-08 - learning_rate: 1.5094e-04\n",
      "Epoch 99/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2311e-08 - val_loss: 7.8609e-08 - learning_rate: 1.5094e-04\n",
      "Epoch 100/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1554e-08 - val_loss: 7.4942e-08 - learning_rate: 1.5094e-04\n",
      "Epoch 101/700\n",
      "\u001b[1m121/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.1762e-08\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 0.00014339437111630105.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1702e-08 - val_loss: 6.7894e-08 - learning_rate: 1.5094e-04\n",
      "Epoch 102/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0568e-08 - val_loss: 6.7161e-08 - learning_rate: 1.4339e-04\n",
      "Epoch 103/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8455e-08 - val_loss: 6.5673e-08 - learning_rate: 1.4339e-04\n",
      "Epoch 104/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9535e-08 - val_loss: 7.3732e-08 - learning_rate: 1.4339e-04\n",
      "Epoch 105/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8866e-08 - val_loss: 6.9823e-08 - learning_rate: 1.4339e-04\n",
      "Epoch 106/700\n",
      "\u001b[1m113/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.7414e-08\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 0.00013622465048683806.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7458e-08 - val_loss: 6.5322e-08 - learning_rate: 1.4339e-04\n",
      "Epoch 107/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0299e-08 - val_loss: 6.4363e-08 - learning_rate: 1.3622e-04\n",
      "Epoch 108/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6097e-08 - val_loss: 6.5879e-08 - learning_rate: 1.3622e-04\n",
      "Epoch 109/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6750e-08 - val_loss: 7.3841e-08 - learning_rate: 1.3622e-04\n",
      "Epoch 110/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7886e-08 - val_loss: 6.7654e-08 - learning_rate: 1.3622e-04\n",
      "Epoch 111/700\n",
      "\u001b[1m100/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.5459e-08\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 0.00012941342210979201.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5473e-08 - val_loss: 7.0568e-08 - learning_rate: 1.3622e-04\n",
      "Epoch 112/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5464e-08 - val_loss: 6.5388e-08 - learning_rate: 1.2941e-04\n",
      "Epoch 113/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4740e-08 - val_loss: 6.7512e-08 - learning_rate: 1.2941e-04\n",
      "Epoch 114/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7461e-08 - val_loss: 6.4424e-08 - learning_rate: 1.2941e-04\n",
      "Epoch 115/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3376e-08 - val_loss: 6.4273e-08 - learning_rate: 1.2941e-04\n",
      "Epoch 116/700\n",
      "\u001b[1m 88/129\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.5511e-08\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 0.00012294275584281422.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5216e-08 - val_loss: 6.2648e-08 - learning_rate: 1.2941e-04\n",
      "Epoch 117/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3878e-08 - val_loss: 6.3020e-08 - learning_rate: 1.2294e-04\n",
      "Epoch 118/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3702e-08 - val_loss: 7.1884e-08 - learning_rate: 1.2294e-04\n",
      "Epoch 119/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5572e-08 - val_loss: 7.3494e-08 - learning_rate: 1.2294e-04\n",
      "Epoch 120/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3432e-08 - val_loss: 6.5640e-08 - learning_rate: 1.2294e-04\n",
      "Epoch 121/700\n",
      "\u001b[1m 98/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1773e-08\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 0.00011679562012432142.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1734e-08 - val_loss: 6.0436e-08 - learning_rate: 1.2294e-04\n",
      "Epoch 122/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1285e-08 - val_loss: 6.0313e-08 - learning_rate: 1.1680e-04\n",
      "Epoch 123/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.2308e-08 - val_loss: 5.9155e-08 - learning_rate: 1.1680e-04\n",
      "Epoch 124/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0747e-08 - val_loss: 5.9049e-08 - learning_rate: 1.1680e-04\n",
      "Epoch 125/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3238e-08 - val_loss: 6.0002e-08 - learning_rate: 1.1680e-04\n",
      "Epoch 126/700\n",
      "\u001b[1m 89/129\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.0772e-08\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 0.0001109558405005373.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0846e-08 - val_loss: 5.8456e-08 - learning_rate: 1.1680e-04\n",
      "Epoch 127/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0483e-08 - val_loss: 6.0248e-08 - learning_rate: 1.1096e-04\n",
      "Epoch 128/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0419e-08 - val_loss: 5.9152e-08 - learning_rate: 1.1096e-04\n",
      "Epoch 129/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0246e-08 - val_loss: 6.0097e-08 - learning_rate: 1.1096e-04\n",
      "Epoch 130/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9848e-08 - val_loss: 6.1358e-08 - learning_rate: 1.1096e-04\n",
      "Epoch 131/700\n",
      "\u001b[1m103/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 5.8586e-08\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 0.00010540805124037433.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8687e-08 - val_loss: 5.8890e-08 - learning_rate: 1.1096e-04\n",
      "Epoch 132/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9393e-08 - val_loss: 5.6759e-08 - learning_rate: 1.0541e-04\n",
      "Epoch 133/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9060e-08 - val_loss: 6.8856e-08 - learning_rate: 1.0541e-04\n",
      "Epoch 134/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.8544e-08 - val_loss: 5.9533e-08 - learning_rate: 1.0541e-04\n",
      "Epoch 135/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8401e-08 - val_loss: 5.7359e-08 - learning_rate: 1.0541e-04\n",
      "Epoch 136/700\n",
      "\u001b[1m 91/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8689e-08\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 0.00010013764695031567.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9052e-08 - val_loss: 5.7862e-08 - learning_rate: 1.0541e-04\n",
      "Epoch 137/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8444e-08 - val_loss: 5.8369e-08 - learning_rate: 1.0014e-04\n",
      "Epoch 138/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8516e-08 - val_loss: 5.7836e-08 - learning_rate: 1.0014e-04\n",
      "Epoch 139/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7282e-08 - val_loss: 5.7151e-08 - learning_rate: 1.0014e-04\n",
      "Epoch 140/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7599e-08 - val_loss: 5.5960e-08 - learning_rate: 1.0014e-04\n",
      "Epoch 141/700\n",
      "\u001b[1m 92/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5896e-08\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 9.5130761837936e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6090e-08 - val_loss: 5.6477e-08 - learning_rate: 1.0014e-04\n",
      "Epoch 142/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6395e-08 - val_loss: 5.5797e-08 - learning_rate: 9.5131e-05\n",
      "Epoch 143/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7044e-08 - val_loss: 5.7151e-08 - learning_rate: 9.5131e-05\n",
      "Epoch 144/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6592e-08 - val_loss: 5.5190e-08 - learning_rate: 9.5131e-05\n",
      "Epoch 145/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6437e-08 - val_loss: 5.5359e-08 - learning_rate: 9.5131e-05\n",
      "Epoch 146/700\n",
      "\u001b[1m117/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5257e-08\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 9.037422132678329e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5343e-08 - val_loss: 5.4164e-08 - learning_rate: 9.5131e-05\n",
      "Epoch 147/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5186e-08 - val_loss: 5.6536e-08 - learning_rate: 9.0374e-05\n",
      "Epoch 148/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6288e-08 - val_loss: 6.0951e-08 - learning_rate: 9.0374e-05\n",
      "Epoch 149/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6662e-08 - val_loss: 5.4426e-08 - learning_rate: 9.0374e-05\n",
      "Epoch 150/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5205e-08 - val_loss: 5.4935e-08 - learning_rate: 9.0374e-05\n",
      "Epoch 151/700\n",
      "\u001b[1m 91/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5295e-08\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 8.585550749558024e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5400e-08 - val_loss: 5.5147e-08 - learning_rate: 9.0374e-05\n",
      "Epoch 152/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5207e-08 - val_loss: 5.3423e-08 - learning_rate: 8.5856e-05\n",
      "Epoch 153/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4593e-08 - val_loss: 5.3163e-08 - learning_rate: 8.5856e-05\n",
      "Epoch 154/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3550e-08 - val_loss: 5.4058e-08 - learning_rate: 8.5856e-05\n",
      "Epoch 155/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4317e-08 - val_loss: 5.6890e-08 - learning_rate: 8.5856e-05\n",
      "Epoch 156/700\n",
      "\u001b[1m115/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.6093e-08\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 8.156273142958525e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5966e-08 - val_loss: 5.3772e-08 - learning_rate: 8.5856e-05\n",
      "Epoch 157/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3361e-08 - val_loss: 5.2641e-08 - learning_rate: 8.1563e-05\n",
      "Epoch 158/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5052e-08 - val_loss: 5.3035e-08 - learning_rate: 8.1563e-05\n",
      "Epoch 159/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3702e-08 - val_loss: 5.3387e-08 - learning_rate: 8.1563e-05\n",
      "Epoch 160/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4833e-08 - val_loss: 5.7943e-08 - learning_rate: 8.1563e-05\n",
      "Epoch 161/700\n",
      "\u001b[1m106/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 958us/step - loss: 5.3877e-08\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 7.748459174763411e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3824e-08 - val_loss: 5.6452e-08 - learning_rate: 8.1563e-05\n",
      "Epoch 162/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3342e-08 - val_loss: 5.1641e-08 - learning_rate: 7.7485e-05\n",
      "Epoch 163/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1959e-08 - val_loss: 5.2075e-08 - learning_rate: 7.7485e-05\n",
      "Epoch 164/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2790e-08 - val_loss: 5.6515e-08 - learning_rate: 7.7485e-05\n",
      "Epoch 165/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3505e-08 - val_loss: 5.4748e-08 - learning_rate: 7.7485e-05\n",
      "Epoch 166/700\n",
      "\u001b[1m 99/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3300e-08\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 7.361036077782046e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3228e-08 - val_loss: 5.2164e-08 - learning_rate: 7.7485e-05\n",
      "Epoch 167/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2328e-08 - val_loss: 5.1361e-08 - learning_rate: 7.3610e-05\n",
      "Epoch 168/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2108e-08 - val_loss: 5.1244e-08 - learning_rate: 7.3610e-05\n",
      "Epoch 169/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1873e-08 - val_loss: 5.2056e-08 - learning_rate: 7.3610e-05\n",
      "Epoch 170/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3815e-08 - val_loss: 5.1630e-08 - learning_rate: 7.3610e-05\n",
      "Epoch 171/700\n",
      "\u001b[1m128/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2038e-08\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 6.992984308453742e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2031e-08 - val_loss: 5.4224e-08 - learning_rate: 7.3610e-05\n",
      "Epoch 172/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.1648e-08 - val_loss: 5.0915e-08 - learning_rate: 6.9930e-05\n",
      "Epoch 173/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1685e-08 - val_loss: 5.1698e-08 - learning_rate: 6.9930e-05\n",
      "Epoch 174/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2023e-08 - val_loss: 5.1263e-08 - learning_rate: 6.9930e-05\n",
      "Epoch 175/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2125e-08 - val_loss: 5.2956e-08 - learning_rate: 6.9930e-05\n",
      "Epoch 176/700\n",
      "\u001b[1m 96/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0639e-08\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 6.643334781983866e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0782e-08 - val_loss: 5.3203e-08 - learning_rate: 6.9930e-05\n",
      "Epoch 177/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1881e-08 - val_loss: 5.2684e-08 - learning_rate: 6.6433e-05\n",
      "Epoch 178/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0691e-08 - val_loss: 5.1675e-08 - learning_rate: 6.6433e-05\n",
      "Epoch 179/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1570e-08 - val_loss: 5.3742e-08 - learning_rate: 6.6433e-05\n",
      "Epoch 180/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1625e-08 - val_loss: 6.1530e-08 - learning_rate: 6.6433e-05\n",
      "Epoch 181/700\n",
      "\u001b[1m126/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2908e-08\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 6.311168181127869e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2862e-08 - val_loss: 4.9889e-08 - learning_rate: 6.6433e-05\n",
      "Epoch 182/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0604e-08 - val_loss: 5.0273e-08 - learning_rate: 6.3112e-05\n",
      "Epoch 183/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0183e-08 - val_loss: 4.9528e-08 - learning_rate: 6.3112e-05\n",
      "Epoch 184/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0887e-08 - val_loss: 5.0632e-08 - learning_rate: 6.3112e-05\n",
      "Epoch 185/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0384e-08 - val_loss: 5.0823e-08 - learning_rate: 6.3112e-05\n",
      "Epoch 186/700\n",
      "\u001b[1m 93/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0214e-08\n",
      "Epoch 186: ReduceLROnPlateau reducing learning rate to 5.995610117679462e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0115e-08 - val_loss: 5.1563e-08 - learning_rate: 6.3112e-05\n",
      "Epoch 187/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9765e-08 - val_loss: 5.0958e-08 - learning_rate: 5.9956e-05\n",
      "Epoch 188/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0602e-08 - val_loss: 5.1169e-08 - learning_rate: 5.9956e-05\n",
      "Epoch 189/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0032e-08 - val_loss: 5.0127e-08 - learning_rate: 5.9956e-05\n",
      "Epoch 190/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9680e-08 - val_loss: 5.0112e-08 - learning_rate: 5.9956e-05\n",
      "Epoch 191/700\n",
      "\u001b[1m 96/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9545e-08\n",
      "Epoch 191: ReduceLROnPlateau reducing learning rate to 5.695829750038683e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9483e-08 - val_loss: 5.3295e-08 - learning_rate: 5.9956e-05\n",
      "Epoch 192/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0237e-08 - val_loss: 4.9880e-08 - learning_rate: 5.6958e-05\n",
      "Epoch 193/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9641e-08 - val_loss: 4.8866e-08 - learning_rate: 5.6958e-05\n",
      "Epoch 194/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9858e-08 - val_loss: 4.9363e-08 - learning_rate: 5.6958e-05\n",
      "Epoch 195/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9992e-08 - val_loss: 4.8536e-08 - learning_rate: 5.6958e-05\n",
      "Epoch 196/700\n",
      "\u001b[1m 99/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8215e-08\n",
      "Epoch 196: ReduceLROnPlateau reducing learning rate to 5.411038400779944e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8450e-08 - val_loss: 5.0085e-08 - learning_rate: 5.6958e-05\n",
      "Epoch 197/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9119e-08 - val_loss: 4.8748e-08 - learning_rate: 5.4110e-05\n",
      "Epoch 198/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8636e-08 - val_loss: 4.9294e-08 - learning_rate: 5.4110e-05\n",
      "Epoch 199/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8732e-08 - val_loss: 4.8477e-08 - learning_rate: 5.4110e-05\n",
      "Epoch 200/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9162e-08 - val_loss: 4.8191e-08 - learning_rate: 5.4110e-05\n",
      "Epoch 201/700\n",
      "\u001b[1m112/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 913us/step - loss: 4.9155e-08\n",
      "Epoch 201: ReduceLROnPlateau reducing learning rate to 5.1404864461801476e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9183e-08 - val_loss: 4.7757e-08 - learning_rate: 5.4110e-05\n",
      "Epoch 202/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9030e-08 - val_loss: 4.8936e-08 - learning_rate: 5.1405e-05\n",
      "Epoch 203/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8297e-08 - val_loss: 4.7733e-08 - learning_rate: 5.1405e-05\n",
      "Epoch 204/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7889e-08 - val_loss: 4.8366e-08 - learning_rate: 5.1405e-05\n",
      "Epoch 205/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8800e-08 - val_loss: 4.8708e-08 - learning_rate: 5.1405e-05\n",
      "Epoch 206/700\n",
      "\u001b[1m 95/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8917e-08\n",
      "Epoch 206: ReduceLROnPlateau reducing learning rate to 4.883462279394734e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8890e-08 - val_loss: 4.8218e-08 - learning_rate: 5.1405e-05\n",
      "Epoch 207/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8455e-08 - val_loss: 4.7164e-08 - learning_rate: 4.8835e-05\n",
      "Epoch 208/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8182e-08 - val_loss: 4.9185e-08 - learning_rate: 4.8835e-05\n",
      "Epoch 209/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7537e-08 - val_loss: 4.7096e-08 - learning_rate: 4.8835e-05\n",
      "Epoch 210/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7177e-08 - val_loss: 4.7917e-08 - learning_rate: 4.8835e-05\n",
      "Epoch 211/700\n",
      "\u001b[1m115/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 889us/step - loss: 4.8457e-08\n",
      "Epoch 211: ReduceLROnPlateau reducing learning rate to 4.639289199985796e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8416e-08 - val_loss: 4.8656e-08 - learning_rate: 4.8835e-05\n",
      "Epoch 212/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8119e-08 - val_loss: 4.6859e-08 - learning_rate: 4.6393e-05\n",
      "Epoch 213/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7650e-08 - val_loss: 4.6792e-08 - learning_rate: 4.6393e-05\n",
      "Epoch 214/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7750e-08 - val_loss: 5.3818e-08 - learning_rate: 4.6393e-05\n",
      "Epoch 215/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8532e-08 - val_loss: 5.5753e-08 - learning_rate: 4.6393e-05\n",
      "Epoch 216/700\n",
      "\u001b[1m 80/129\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9841e-08\n",
      "Epoch 216: ReduceLROnPlateau reducing learning rate to 4.407324722706107e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9176e-08 - val_loss: 4.9972e-08 - learning_rate: 4.6393e-05\n",
      "Epoch 217/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7357e-08 - val_loss: 4.8947e-08 - learning_rate: 4.4073e-05\n",
      "Epoch 218/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8296e-08 - val_loss: 4.6612e-08 - learning_rate: 4.4073e-05\n",
      "Epoch 219/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6786e-08 - val_loss: 4.7245e-08 - learning_rate: 4.4073e-05\n",
      "Epoch 220/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7377e-08 - val_loss: 4.7124e-08 - learning_rate: 4.4073e-05\n",
      "Epoch 221/700\n",
      "\u001b[1m115/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 887us/step - loss: 4.7125e-08\n",
      "Epoch 221: ReduceLROnPlateau reducing learning rate to 4.1869585038512015e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7096e-08 - val_loss: 4.6838e-08 - learning_rate: 4.4073e-05\n",
      "Epoch 222/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7313e-08 - val_loss: 5.0033e-08 - learning_rate: 4.1870e-05\n",
      "Epoch 223/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7858e-08 - val_loss: 4.6221e-08 - learning_rate: 4.1870e-05\n",
      "Epoch 224/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7102e-08 - val_loss: 4.8032e-08 - learning_rate: 4.1870e-05\n",
      "Epoch 225/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7238e-08 - val_loss: 5.1449e-08 - learning_rate: 4.1870e-05\n",
      "Epoch 226/700\n",
      "\u001b[1m111/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 913us/step - loss: 4.7030e-08\n",
      "Epoch 226: ReduceLROnPlateau reducing learning rate to 3.9776106132194396e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7017e-08 - val_loss: 4.7140e-08 - learning_rate: 4.1870e-05\n",
      "Epoch 227/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6988e-08 - val_loss: 4.6557e-08 - learning_rate: 3.9776e-05\n",
      "Epoch 228/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7119e-08 - val_loss: 4.5888e-08 - learning_rate: 3.9776e-05\n",
      "Epoch 229/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6677e-08 - val_loss: 4.6289e-08 - learning_rate: 3.9776e-05\n",
      "Epoch 230/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6857e-08 - val_loss: 4.6093e-08 - learning_rate: 3.9776e-05\n",
      "Epoch 231/700\n",
      "\u001b[1m105/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 4.6760e-08\n",
      "Epoch 231: ReduceLROnPlateau reducing learning rate to 3.778730151680065e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6701e-08 - val_loss: 4.5710e-08 - learning_rate: 3.9776e-05\n",
      "Epoch 232/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6853e-08 - val_loss: 4.6777e-08 - learning_rate: 3.7787e-05\n",
      "Epoch 233/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6803e-08 - val_loss: 4.6257e-08 - learning_rate: 3.7787e-05\n",
      "Epoch 234/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6626e-08 - val_loss: 4.5597e-08 - learning_rate: 3.7787e-05\n",
      "Epoch 235/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7157e-08 - val_loss: 4.5751e-08 - learning_rate: 3.7787e-05\n",
      "Epoch 236/700\n",
      "\u001b[1m107/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 955us/step - loss: 4.6634e-08\n",
      "Epoch 236: ReduceLROnPlateau reducing learning rate to 3.589793523133266e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6577e-08 - val_loss: 4.6726e-08 - learning_rate: 3.7787e-05\n",
      "Epoch 237/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6391e-08 - val_loss: 4.5341e-08 - learning_rate: 3.5898e-05\n",
      "Epoch 238/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5817e-08 - val_loss: 4.6103e-08 - learning_rate: 3.5898e-05\n",
      "Epoch 239/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5803e-08 - val_loss: 4.5521e-08 - learning_rate: 3.5898e-05\n",
      "Epoch 240/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6147e-08 - val_loss: 4.5529e-08 - learning_rate: 3.5898e-05\n",
      "Epoch 241/700\n",
      "\u001b[1m113/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 906us/step - loss: 4.6126e-08\n",
      "Epoch 241: ReduceLROnPlateau reducing learning rate to 3.410303743294207e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6120e-08 - val_loss: 4.5372e-08 - learning_rate: 3.5898e-05\n",
      "Epoch 242/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5691e-08 - val_loss: 4.5242e-08 - learning_rate: 3.4103e-05\n",
      "Epoch 243/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5892e-08 - val_loss: 4.5665e-08 - learning_rate: 3.4103e-05\n",
      "Epoch 244/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5745e-08 - val_loss: 4.5356e-08 - learning_rate: 3.4103e-05\n",
      "Epoch 245/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5678e-08 - val_loss: 4.6022e-08 - learning_rate: 3.4103e-05\n",
      "Epoch 246/700\n",
      "\u001b[1m112/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 910us/step - loss: 4.6036e-08\n",
      "Epoch 246: ReduceLROnPlateau reducing learning rate to 3.239788711653091e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6020e-08 - val_loss: 4.5426e-08 - learning_rate: 3.4103e-05\n",
      "Epoch 247/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5765e-08 - val_loss: 4.4882e-08 - learning_rate: 3.2398e-05\n",
      "Epoch 248/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5448e-08 - val_loss: 4.4879e-08 - learning_rate: 3.2398e-05\n",
      "Epoch 249/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5819e-08 - val_loss: 4.4934e-08 - learning_rate: 3.2398e-05\n",
      "Epoch 250/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5022e-08 - val_loss: 4.5496e-08 - learning_rate: 3.2398e-05\n",
      "Epoch 251/700\n",
      "\u001b[1m 98/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6274e-08\n",
      "Epoch 251: ReduceLROnPlateau reducing learning rate to 3.0777991378272417e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6143e-08 - val_loss: 4.4888e-08 - learning_rate: 3.2398e-05\n",
      "Epoch 252/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4972e-08 - val_loss: 4.5002e-08 - learning_rate: 3.0778e-05\n",
      "Epoch 253/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5182e-08 - val_loss: 4.5114e-08 - learning_rate: 3.0778e-05\n",
      "Epoch 254/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5326e-08 - val_loss: 4.5696e-08 - learning_rate: 3.0778e-05\n",
      "Epoch 255/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5126e-08 - val_loss: 4.6682e-08 - learning_rate: 3.0778e-05\n",
      "Epoch 256/700\n",
      "\u001b[1m106/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 961us/step - loss: 4.4961e-08\n",
      "Epoch 256: ReduceLROnPlateau reducing learning rate to 2.9239092327770777e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4981e-08 - val_loss: 4.4830e-08 - learning_rate: 3.0778e-05\n",
      "Epoch 257/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5138e-08 - val_loss: 4.4455e-08 - learning_rate: 2.9239e-05\n",
      "Epoch 258/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4823e-08 - val_loss: 4.4616e-08 - learning_rate: 2.9239e-05\n",
      "Epoch 259/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4606e-08 - val_loss: 4.5605e-08 - learning_rate: 2.9239e-05\n",
      "Epoch 260/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5208e-08 - val_loss: 4.5071e-08 - learning_rate: 2.9239e-05\n",
      "Epoch 261/700\n",
      "\u001b[1m 98/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5301e-08  \n",
      "Epoch 261: ReduceLROnPlateau reducing learning rate to 2.7777137711382237e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5212e-08 - val_loss: 4.4573e-08 - learning_rate: 2.9239e-05\n",
      "Epoch 262/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4415e-08 - val_loss: 4.4582e-08 - learning_rate: 2.7777e-05\n",
      "Epoch 263/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4486e-08 - val_loss: 4.4251e-08 - learning_rate: 2.7777e-05\n",
      "Epoch 264/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5008e-08 - val_loss: 4.4711e-08 - learning_rate: 2.7777e-05\n",
      "Epoch 265/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4557e-08 - val_loss: 4.4775e-08 - learning_rate: 2.7777e-05\n",
      "Epoch 266/700\n",
      "\u001b[1m 77/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5546e-08\n",
      "Epoch 266: ReduceLROnPlateau reducing learning rate to 2.6388280912215122e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5304e-08 - val_loss: 4.4261e-08 - learning_rate: 2.7777e-05\n",
      "Epoch 267/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5474e-08 - val_loss: 4.5471e-08 - learning_rate: 2.6388e-05\n",
      "Epoch 268/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4843e-08 - val_loss: 4.4492e-08 - learning_rate: 2.6388e-05\n",
      "Epoch 269/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5301e-08 - val_loss: 4.5787e-08 - learning_rate: 2.6388e-05\n",
      "Epoch 270/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4884e-08 - val_loss: 4.4267e-08 - learning_rate: 2.6388e-05\n",
      "Epoch 271/700\n",
      "\u001b[1m111/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 925us/step - loss: 4.4603e-08\n",
      "Epoch 271: ReduceLROnPlateau reducing learning rate to 2.5068867125810355e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4605e-08 - val_loss: 4.3915e-08 - learning_rate: 2.6388e-05\n",
      "Epoch 272/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4198e-08 - val_loss: 4.3912e-08 - learning_rate: 2.5069e-05\n",
      "Epoch 273/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4422e-08 - val_loss: 4.4058e-08 - learning_rate: 2.5069e-05\n",
      "Epoch 274/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4051e-08 - val_loss: 4.3942e-08 - learning_rate: 2.5069e-05\n",
      "Epoch 275/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4144e-08 - val_loss: 4.5141e-08 - learning_rate: 2.5069e-05\n",
      "Epoch 276/700\n",
      "\u001b[1m 94/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3748e-08\n",
      "Epoch 276: ReduceLROnPlateau reducing learning rate to 2.381542299190187e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3954e-08 - val_loss: 4.3791e-08 - learning_rate: 2.5069e-05\n",
      "Epoch 277/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3955e-08 - val_loss: 4.4647e-08 - learning_rate: 2.3815e-05\n",
      "Epoch 278/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5250e-08 - val_loss: 4.3819e-08 - learning_rate: 2.3815e-05\n",
      "Epoch 279/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4713e-08 - val_loss: 4.3694e-08 - learning_rate: 2.3815e-05\n",
      "Epoch 280/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4131e-08 - val_loss: 4.3736e-08 - learning_rate: 2.3815e-05\n",
      "Epoch 281/700\n",
      "\u001b[1m114/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 894us/step - loss: 4.4188e-08\n",
      "Epoch 281: ReduceLROnPlateau reducing learning rate to 2.262465141029679e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4191e-08 - val_loss: 4.3902e-08 - learning_rate: 2.3815e-05\n",
      "Epoch 282/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4389e-08 - val_loss: 4.3649e-08 - learning_rate: 2.2625e-05\n",
      "Epoch 283/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4097e-08 - val_loss: 4.4105e-08 - learning_rate: 2.2625e-05\n",
      "Epoch 284/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4101e-08 - val_loss: 4.3636e-08 - learning_rate: 2.2625e-05\n",
      "Epoch 285/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4039e-08 - val_loss: 4.3702e-08 - learning_rate: 2.2625e-05\n",
      "Epoch 286/700\n",
      "\u001b[1m102/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4174e-08\n",
      "Epoch 286: ReduceLROnPlateau reducing learning rate to 2.1493419444595927e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4185e-08 - val_loss: 4.4979e-08 - learning_rate: 2.2625e-05\n",
      "Epoch 287/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4263e-08 - val_loss: 4.3464e-08 - learning_rate: 2.1493e-05\n",
      "Epoch 288/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3553e-08 - val_loss: 4.3479e-08 - learning_rate: 2.1493e-05\n",
      "Epoch 289/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3402e-08 - val_loss: 4.3853e-08 - learning_rate: 2.1493e-05\n",
      "Epoch 290/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4045e-08 - val_loss: 4.3916e-08 - learning_rate: 2.1493e-05\n",
      "Epoch 291/700\n",
      "\u001b[1m 93/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4222e-08  \n",
      "Epoch 291: ReduceLROnPlateau reducing learning rate to 2.0418747953954153e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4171e-08 - val_loss: 4.3834e-08 - learning_rate: 2.1493e-05\n",
      "Epoch 292/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4030e-08 - val_loss: 4.3611e-08 - learning_rate: 2.0419e-05\n",
      "Epoch 293/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3896e-08 - val_loss: 4.3399e-08 - learning_rate: 2.0419e-05\n",
      "Epoch 294/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4235e-08 - val_loss: 4.4293e-08 - learning_rate: 2.0419e-05\n",
      "Epoch 295/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4259e-08 - val_loss: 4.4342e-08 - learning_rate: 2.0419e-05\n",
      "Epoch 296/700\n",
      "\u001b[1m122/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3690e-08\n",
      "Epoch 296: ReduceLROnPlateau reducing learning rate to 1.9397809865040472e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3699e-08 - val_loss: 4.3623e-08 - learning_rate: 2.0419e-05\n",
      "Epoch 297/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4083e-08 - val_loss: 4.3128e-08 - learning_rate: 1.9398e-05\n",
      "Epoch 298/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3493e-08 - val_loss: 4.3141e-08 - learning_rate: 1.9398e-05\n",
      "Epoch 299/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3371e-08 - val_loss: 4.3829e-08 - learning_rate: 1.9398e-05\n",
      "Epoch 300/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3346e-08 - val_loss: 4.3199e-08 - learning_rate: 1.9398e-05\n",
      "Epoch 301/700\n",
      "\u001b[1m110/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 4.4197e-08\n",
      "Epoch 301: ReduceLROnPlateau reducing learning rate to 1.842791980379843e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4123e-08 - val_loss: 4.3230e-08 - learning_rate: 1.9398e-05\n",
      "Epoch 302/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3604e-08 - val_loss: 4.3431e-08 - learning_rate: 1.8428e-05\n",
      "Epoch 303/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3253e-08 - val_loss: 4.3293e-08 - learning_rate: 1.8428e-05\n",
      "Epoch 304/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4201e-08 - val_loss: 4.3275e-08 - learning_rate: 1.8428e-05\n",
      "Epoch 305/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3747e-08 - val_loss: 4.3709e-08 - learning_rate: 1.8428e-05\n",
      "Epoch 306/700\n",
      "\u001b[1m114/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 891us/step - loss: 4.3822e-08\n",
      "Epoch 306: ReduceLROnPlateau reducing learning rate to 1.7506523727206512e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3802e-08 - val_loss: 4.2913e-08 - learning_rate: 1.8428e-05\n",
      "Epoch 307/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3230e-08 - val_loss: 4.5073e-08 - learning_rate: 1.7507e-05\n",
      "Epoch 308/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3824e-08 - val_loss: 4.3442e-08 - learning_rate: 1.7507e-05\n",
      "Epoch 309/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3408e-08 - val_loss: 4.3028e-08 - learning_rate: 1.7507e-05\n",
      "Epoch 310/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2780e-08 - val_loss: 4.3301e-08 - learning_rate: 1.7507e-05\n",
      "Epoch 311/700\n",
      "\u001b[1m104/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 4.3484e-08\n",
      "Epoch 311: ReduceLROnPlateau reducing learning rate to 1.66311971952382e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3463e-08 - val_loss: 4.4041e-08 - learning_rate: 1.7507e-05\n",
      "Epoch 312/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3430e-08 - val_loss: 4.3235e-08 - learning_rate: 1.6631e-05\n",
      "Epoch 313/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3379e-08 - val_loss: 4.3207e-08 - learning_rate: 1.6631e-05\n",
      "Epoch 314/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2759e-08 - val_loss: 4.2913e-08 - learning_rate: 1.6631e-05\n",
      "Epoch 315/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3569e-08 - val_loss: 4.2727e-08 - learning_rate: 1.6631e-05\n",
      "Epoch 316/700\n",
      "\u001b[1m121/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3137e-08\n",
      "Epoch 316: ReduceLROnPlateau reducing learning rate to 1.5799636730662315e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3140e-08 - val_loss: 4.3790e-08 - learning_rate: 1.6631e-05\n",
      "Epoch 317/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3635e-08 - val_loss: 4.3400e-08 - learning_rate: 1.5800e-05\n",
      "Epoch 318/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3600e-08 - val_loss: 4.3636e-08 - learning_rate: 1.5800e-05\n",
      "Epoch 319/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3085e-08 - val_loss: 4.2948e-08 - learning_rate: 1.5800e-05\n",
      "Epoch 320/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2727e-08 - val_loss: 4.3426e-08 - learning_rate: 1.5800e-05\n",
      "Epoch 321/700\n",
      "\u001b[1m126/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3177e-08\n",
      "Epoch 321: ReduceLROnPlateau reducing learning rate to 1.5009654634923207e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3177e-08 - val_loss: 4.2887e-08 - learning_rate: 1.5800e-05\n",
      "Epoch 322/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2729e-08 - val_loss: 4.2930e-08 - learning_rate: 1.5010e-05\n",
      "Epoch 323/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2933e-08 - val_loss: 4.2551e-08 - learning_rate: 1.5010e-05\n",
      "Epoch 324/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3047e-08 - val_loss: 4.3433e-08 - learning_rate: 1.5010e-05\n",
      "Epoch 325/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2935e-08 - val_loss: 4.2639e-08 - learning_rate: 1.5010e-05\n",
      "Epoch 326/700\n",
      "\u001b[1m103/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3364e-08\n",
      "Epoch 326: ReduceLROnPlateau reducing learning rate to 1.4259172075981041e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3329e-08 - val_loss: 4.2493e-08 - learning_rate: 1.5010e-05\n",
      "Epoch 327/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2092e-08 - val_loss: 4.3655e-08 - learning_rate: 1.4259e-05\n",
      "Epoch 328/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2824e-08 - val_loss: 4.2442e-08 - learning_rate: 1.4259e-05\n",
      "Epoch 329/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2834e-08 - val_loss: 4.2610e-08 - learning_rate: 1.4259e-05\n",
      "Epoch 330/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2516e-08 - val_loss: 4.2767e-08 - learning_rate: 1.4259e-05\n",
      "Epoch 331/700\n",
      "\u001b[1m124/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2711e-08\n",
      "Epoch 331: ReduceLROnPlateau reducing learning rate to 1.3546213904191972e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2720e-08 - val_loss: 4.2689e-08 - learning_rate: 1.4259e-05\n",
      "Epoch 332/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2647e-08 - val_loss: 4.3658e-08 - learning_rate: 1.3546e-05\n",
      "Epoch 333/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3170e-08 - val_loss: 4.2395e-08 - learning_rate: 1.3546e-05\n",
      "Epoch 334/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3271e-08 - val_loss: 4.2652e-08 - learning_rate: 1.3546e-05\n",
      "Epoch 335/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2678e-08 - val_loss: 4.2464e-08 - learning_rate: 1.3546e-05\n",
      "Epoch 336/700\n",
      "\u001b[1m101/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3261e-08\n",
      "Epoch 336: ReduceLROnPlateau reducing learning rate to 1.2868903468188364e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3231e-08 - val_loss: 4.2501e-08 - learning_rate: 1.3546e-05\n",
      "Epoch 337/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2642e-08 - val_loss: 4.2521e-08 - learning_rate: 1.2869e-05\n",
      "Epoch 338/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2676e-08 - val_loss: 4.2541e-08 - learning_rate: 1.2869e-05\n",
      "Epoch 339/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2942e-08 - val_loss: 4.2351e-08 - learning_rate: 1.2869e-05\n",
      "Epoch 340/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3157e-08 - val_loss: 4.2504e-08 - learning_rate: 1.2869e-05\n",
      "Epoch 341/700\n",
      "\u001b[1m121/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 4.2725e-08\n",
      "Epoch 341: ReduceLROnPlateau reducing learning rate to 1.2225458294778945e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2730e-08 - val_loss: 4.2384e-08 - learning_rate: 1.2869e-05\n",
      "Epoch 342/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3094e-08 - val_loss: 4.2351e-08 - learning_rate: 1.2225e-05\n",
      "Epoch 343/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2987e-08 - val_loss: 4.2187e-08 - learning_rate: 1.2225e-05\n",
      "Epoch 344/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2354e-08 - val_loss: 4.2265e-08 - learning_rate: 1.2225e-05\n",
      "Epoch 345/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2388e-08 - val_loss: 4.2311e-08 - learning_rate: 1.2225e-05\n",
      "Epoch 346/700\n",
      "\u001b[1m113/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 896us/step - loss: 4.2894e-08\n",
      "Epoch 346: ReduceLROnPlateau reducing learning rate to 1.1614185768848983e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2879e-08 - val_loss: 4.2288e-08 - learning_rate: 1.2225e-05\n",
      "Epoch 347/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2430e-08 - val_loss: 4.2301e-08 - learning_rate: 1.1614e-05\n",
      "Epoch 348/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2390e-08 - val_loss: 4.2928e-08 - learning_rate: 1.1614e-05\n",
      "Epoch 349/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2478e-08 - val_loss: 4.2321e-08 - learning_rate: 1.1614e-05\n",
      "Epoch 350/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2898e-08 - val_loss: 4.2235e-08 - learning_rate: 1.1614e-05\n",
      "Epoch 351/700\n",
      "\u001b[1m110/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 928us/step - loss: 4.2418e-08\n",
      "Epoch 351: ReduceLROnPlateau reducing learning rate to 1.1033476221200543e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2443e-08 - val_loss: 4.2550e-08 - learning_rate: 1.1614e-05\n",
      "Epoch 352/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2684e-08 - val_loss: 4.2391e-08 - learning_rate: 1.1033e-05\n",
      "Epoch 353/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2680e-08 - val_loss: 4.2511e-08 - learning_rate: 1.1033e-05\n",
      "Epoch 354/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2696e-08 - val_loss: 4.2628e-08 - learning_rate: 1.1033e-05\n",
      "Epoch 355/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2622e-08 - val_loss: 4.2453e-08 - learning_rate: 1.1033e-05\n",
      "Epoch 356/700\n",
      "\u001b[1m104/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 982us/step - loss: 4.2729e-08\n",
      "Epoch 356: ReduceLROnPlateau reducing learning rate to 1.048180206453253e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2685e-08 - val_loss: 4.2069e-08 - learning_rate: 1.1033e-05\n",
      "Epoch 357/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2695e-08 - val_loss: 4.2107e-08 - learning_rate: 1.0482e-05\n",
      "Epoch 358/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2187e-08 - val_loss: 4.1990e-08 - learning_rate: 1.0482e-05\n",
      "Epoch 359/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2228e-08 - val_loss: 4.2057e-08 - learning_rate: 1.0482e-05\n",
      "Epoch 360/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2208e-08 - val_loss: 4.3126e-08 - learning_rate: 1.0482e-05\n",
      "Epoch 361/700\n",
      "\u001b[1m 95/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3045e-08\n",
      "Epoch 361: ReduceLROnPlateau reducing learning rate to 9.957711745300912e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2918e-08 - val_loss: 4.2194e-08 - learning_rate: 1.0482e-05\n",
      "Epoch 362/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2342e-08 - val_loss: 4.2339e-08 - learning_rate: 9.9577e-06\n",
      "Epoch 363/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2117e-08 - val_loss: 4.1892e-08 - learning_rate: 9.9577e-06\n",
      "Epoch 364/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2044e-08 - val_loss: 4.1910e-08 - learning_rate: 9.9577e-06\n",
      "Epoch 365/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2361e-08 - val_loss: 4.1968e-08 - learning_rate: 9.9577e-06\n",
      "Epoch 366/700\n",
      "\u001b[1m106/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 962us/step - loss: 4.2056e-08\n",
      "Epoch 366: ReduceLROnPlateau reducing learning rate to 9.459826287638862e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2105e-08 - val_loss: 4.2016e-08 - learning_rate: 9.9577e-06\n",
      "Epoch 367/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2376e-08 - val_loss: 4.2000e-08 - learning_rate: 9.4598e-06\n",
      "Epoch 368/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2313e-08 - val_loss: 4.1927e-08 - learning_rate: 9.4598e-06\n",
      "Epoch 369/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2751e-08 - val_loss: 4.2135e-08 - learning_rate: 9.4598e-06\n",
      "Epoch 370/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2152e-08 - val_loss: 4.1874e-08 - learning_rate: 9.4598e-06\n",
      "Epoch 371/700\n",
      "\u001b[1m 84/129\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1702e-08  \n",
      "Epoch 371: ReduceLROnPlateau reducing learning rate to 8.986834973256918e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1977e-08 - val_loss: 4.1971e-08 - learning_rate: 9.4598e-06\n",
      "Epoch 372/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2031e-08 - val_loss: 4.1862e-08 - learning_rate: 8.9868e-06\n",
      "Epoch 373/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1926e-08 - val_loss: 4.1904e-08 - learning_rate: 8.9868e-06\n",
      "Epoch 374/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2497e-08 - val_loss: 4.2116e-08 - learning_rate: 8.9868e-06\n",
      "Epoch 375/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2467e-08 - val_loss: 4.2054e-08 - learning_rate: 8.9868e-06\n",
      "Epoch 376/700\n",
      "\u001b[1m 98/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1959e-08\n",
      "Epoch 376: ReduceLROnPlateau reducing learning rate to 8.537493613403058e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2051e-08 - val_loss: 4.1897e-08 - learning_rate: 8.9868e-06\n",
      "Epoch 377/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2365e-08 - val_loss: 4.1831e-08 - learning_rate: 8.5375e-06\n",
      "Epoch 378/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2543e-08 - val_loss: 4.1777e-08 - learning_rate: 8.5375e-06\n",
      "Epoch 379/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1702e-08 - val_loss: 4.1711e-08 - learning_rate: 8.5375e-06\n",
      "Epoch 380/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2205e-08 - val_loss: 4.1713e-08 - learning_rate: 8.5375e-06\n",
      "Epoch 381/700\n",
      "\u001b[1m 95/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2533e-08\n",
      "Epoch 381: ReduceLROnPlateau reducing learning rate to 8.110619364742887e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2411e-08 - val_loss: 4.1730e-08 - learning_rate: 8.5375e-06\n",
      "Epoch 382/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2421e-08 - val_loss: 4.2015e-08 - learning_rate: 8.1106e-06\n",
      "Epoch 383/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2195e-08 - val_loss: 4.1739e-08 - learning_rate: 8.1106e-06\n",
      "Epoch 384/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1999e-08 - val_loss: 4.1703e-08 - learning_rate: 8.1106e-06\n",
      "Epoch 385/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2269e-08 - val_loss: 4.2037e-08 - learning_rate: 8.1106e-06\n",
      "Epoch 386/700\n",
      "\u001b[1m 96/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2467e-08\n",
      "Epoch 386: ReduceLROnPlateau reducing learning rate to 7.705088137299754e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2426e-08 - val_loss: 4.1815e-08 - learning_rate: 8.1106e-06\n",
      "Epoch 387/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2103e-08 - val_loss: 4.1910e-08 - learning_rate: 7.7051e-06\n",
      "Epoch 388/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1603e-08 - val_loss: 4.2359e-08 - learning_rate: 7.7051e-06\n",
      "Epoch 389/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1833e-08 - val_loss: 4.2111e-08 - learning_rate: 7.7051e-06\n",
      "Epoch 390/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2118e-08 - val_loss: 4.1633e-08 - learning_rate: 7.7051e-06\n",
      "Epoch 391/700\n",
      "\u001b[1m118/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 4.2136e-08\n",
      "Epoch 391: ReduceLROnPlateau reducing learning rate to 7.319833730434766e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2137e-08 - val_loss: 4.1877e-08 - learning_rate: 7.7051e-06\n",
      "Epoch 392/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1838e-08 - val_loss: 4.1990e-08 - learning_rate: 7.3198e-06\n",
      "Epoch 393/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2283e-08 - val_loss: 4.1700e-08 - learning_rate: 7.3198e-06\n",
      "Epoch 394/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1942e-08 - val_loss: 4.1787e-08 - learning_rate: 7.3198e-06\n",
      "Epoch 395/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1619e-08 - val_loss: 4.1584e-08 - learning_rate: 7.3198e-06\n",
      "Epoch 396/700\n",
      "\u001b[1m112/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 913us/step - loss: 4.1902e-08\n",
      "Epoch 396: ReduceLROnPlateau reducing learning rate to 6.953842216717021e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1924e-08 - val_loss: 4.1606e-08 - learning_rate: 7.3198e-06\n",
      "Epoch 397/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2471e-08 - val_loss: 4.1610e-08 - learning_rate: 6.9538e-06\n",
      "Epoch 398/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2151e-08 - val_loss: 4.1541e-08 - learning_rate: 6.9538e-06\n",
      "Epoch 399/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1879e-08 - val_loss: 4.1515e-08 - learning_rate: 6.9538e-06\n",
      "Epoch 400/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1429e-08 - val_loss: 4.1721e-08 - learning_rate: 6.9538e-06\n",
      "Epoch 401/700\n",
      "\u001b[1m103/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 990us/step - loss: 4.1581e-08\n",
      "Epoch 401: ReduceLROnPlateau reducing learning rate to 6.6061502138836655e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1645e-08 - val_loss: 4.1533e-08 - learning_rate: 6.9538e-06\n",
      "Epoch 402/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2143e-08 - val_loss: 4.1527e-08 - learning_rate: 6.6062e-06\n",
      "Epoch 403/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1816e-08 - val_loss: 4.1582e-08 - learning_rate: 6.6062e-06\n",
      "Epoch 404/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2137e-08 - val_loss: 4.1909e-08 - learning_rate: 6.6062e-06\n",
      "Epoch 405/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1513e-08 - val_loss: 4.1545e-08 - learning_rate: 6.6062e-06\n",
      "Epoch 406/700\n",
      "\u001b[1m112/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 911us/step - loss: 4.2007e-08\n",
      "Epoch 406: ReduceLROnPlateau reducing learning rate to 6.2758427247899816e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2006e-08 - val_loss: 4.1487e-08 - learning_rate: 6.6062e-06\n",
      "Epoch 407/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1399e-08 - val_loss: 4.1493e-08 - learning_rate: 6.2758e-06\n",
      "Epoch 408/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1851e-08 - val_loss: 4.1748e-08 - learning_rate: 6.2758e-06\n",
      "Epoch 409/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1374e-08 - val_loss: 4.1467e-08 - learning_rate: 6.2758e-06\n",
      "Epoch 410/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2030e-08 - val_loss: 4.1443e-08 - learning_rate: 6.2758e-06\n",
      "Epoch 411/700\n",
      "\u001b[1m112/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 910us/step - loss: 4.1813e-08\n",
      "Epoch 411: ReduceLROnPlateau reducing learning rate to 5.962050545349484e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1836e-08 - val_loss: 4.1457e-08 - learning_rate: 6.2758e-06\n",
      "Epoch 412/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1895e-08 - val_loss: 4.1453e-08 - learning_rate: 5.9621e-06\n",
      "Epoch 413/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1813e-08 - val_loss: 4.1569e-08 - learning_rate: 5.9621e-06\n",
      "Epoch 414/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1585e-08 - val_loss: 4.1428e-08 - learning_rate: 5.9621e-06\n",
      "Epoch 415/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2372e-08 - val_loss: 4.1527e-08 - learning_rate: 5.9621e-06\n",
      "Epoch 416/700\n",
      "\u001b[1m104/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 4.1695e-08\n",
      "Epoch 416: ReduceLROnPlateau reducing learning rate to 5.663948104484007e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1725e-08 - val_loss: 4.1383e-08 - learning_rate: 5.9621e-06\n",
      "Epoch 417/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1280e-08 - val_loss: 4.1365e-08 - learning_rate: 5.6639e-06\n",
      "Epoch 418/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2342e-08 - val_loss: 4.1436e-08 - learning_rate: 5.6639e-06\n",
      "Epoch 419/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2089e-08 - val_loss: 4.1372e-08 - learning_rate: 5.6639e-06\n",
      "Epoch 420/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1900e-08 - val_loss: 4.1721e-08 - learning_rate: 5.6639e-06\n",
      "Epoch 421/700\n",
      "\u001b[1m 94/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1557e-08\n",
      "Epoch 421: ReduceLROnPlateau reducing learning rate to 5.380750872063799e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1596e-08 - val_loss: 4.1419e-08 - learning_rate: 5.6639e-06\n",
      "Epoch 422/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1551e-08 - val_loss: 4.1406e-08 - learning_rate: 5.3808e-06\n",
      "Epoch 423/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1975e-08 - val_loss: 4.1336e-08 - learning_rate: 5.3808e-06\n",
      "Epoch 424/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1629e-08 - val_loss: 4.1510e-08 - learning_rate: 5.3808e-06\n",
      "Epoch 425/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1947e-08 - val_loss: 4.1364e-08 - learning_rate: 5.3808e-06\n",
      "Epoch 426/700\n",
      "\u001b[1m105/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 4.2177e-08 \n",
      "Epoch 426: ReduceLROnPlateau reducing learning rate to 5.1117131988576144e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2119e-08 - val_loss: 4.1362e-08 - learning_rate: 5.3808e-06\n",
      "Epoch 427/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1215e-08 - val_loss: 4.1630e-08 - learning_rate: 5.1117e-06\n",
      "Epoch 428/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2108e-08 - val_loss: 4.1509e-08 - learning_rate: 5.1117e-06\n",
      "Epoch 429/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1519e-08 - val_loss: 4.1311e-08 - learning_rate: 5.1117e-06\n",
      "Epoch 430/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1414e-08 - val_loss: 4.1396e-08 - learning_rate: 5.1117e-06\n",
      "Epoch 431/700\n",
      "\u001b[1m108/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 4.1309e-08\n",
      "Epoch 431: ReduceLROnPlateau reducing learning rate to 4.856127452512737e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1380e-08 - val_loss: 4.1287e-08 - learning_rate: 5.1117e-06\n",
      "Epoch 432/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1833e-08 - val_loss: 4.1280e-08 - learning_rate: 4.8561e-06\n",
      "Epoch 433/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1306e-08 - val_loss: 4.1364e-08 - learning_rate: 4.8561e-06\n",
      "Epoch 434/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1684e-08 - val_loss: 4.1396e-08 - learning_rate: 4.8561e-06\n",
      "Epoch 435/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1789e-08 - val_loss: 4.1275e-08 - learning_rate: 4.8561e-06\n",
      "Epoch 436/700\n",
      "\u001b[1m115/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1805e-08\n",
      "Epoch 436: ReduceLROnPlateau reducing learning rate to 4.613320993485104e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1791e-08 - val_loss: 4.1340e-08 - learning_rate: 4.8561e-06\n",
      "Epoch 437/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1268e-08 - val_loss: 4.1363e-08 - learning_rate: 4.6133e-06\n",
      "Epoch 438/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1767e-08 - val_loss: 4.1444e-08 - learning_rate: 4.6133e-06\n",
      "Epoch 439/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1871e-08 - val_loss: 4.1234e-08 - learning_rate: 4.6133e-06\n",
      "Epoch 440/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1765e-08 - val_loss: 4.1359e-08 - learning_rate: 4.6133e-06\n",
      "Epoch 441/700\n",
      "\u001b[1m 98/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1529e-08\n",
      "Epoch 441: ReduceLROnPlateau reducing learning rate to 4.382654879009351e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1538e-08 - val_loss: 4.1282e-08 - learning_rate: 4.6133e-06\n",
      "Epoch 442/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2030e-08 - val_loss: 4.1236e-08 - learning_rate: 4.3827e-06\n",
      "Epoch 443/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2443e-08 - val_loss: 4.1299e-08 - learning_rate: 4.3827e-06\n",
      "Epoch 444/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1790e-08 - val_loss: 4.1387e-08 - learning_rate: 4.3827e-06\n",
      "Epoch 445/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1776e-08 - val_loss: 4.1257e-08 - learning_rate: 4.3827e-06\n",
      "Epoch 446/700\n",
      "\u001b[1m100/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1997e-08\n",
      "Epoch 446: ReduceLROnPlateau reducing learning rate to 4.163522135058884e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1926e-08 - val_loss: 4.1407e-08 - learning_rate: 4.3827e-06\n",
      "Epoch 447/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2185e-08 - val_loss: 4.1234e-08 - learning_rate: 4.1635e-06\n",
      "Epoch 448/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1271e-08 - val_loss: 4.1240e-08 - learning_rate: 4.1635e-06\n",
      "Epoch 449/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1642e-08 - val_loss: 4.1189e-08 - learning_rate: 4.1635e-06\n",
      "Epoch 450/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1921e-08 - val_loss: 4.1361e-08 - learning_rate: 4.1635e-06\n",
      "Epoch 451/700\n",
      "\u001b[1m104/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 4.1701e-08\n",
      "Epoch 451: ReduceLROnPlateau reducing learning rate to 3.9553460283059396e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1671e-08 - val_loss: 4.1162e-08 - learning_rate: 4.1635e-06\n",
      "Epoch 452/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1373e-08 - val_loss: 4.1313e-08 - learning_rate: 3.9553e-06\n",
      "Epoch 453/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1750e-08 - val_loss: 4.1148e-08 - learning_rate: 3.9553e-06\n",
      "Epoch 454/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1352e-08 - val_loss: 4.1263e-08 - learning_rate: 3.9553e-06\n",
      "Epoch 455/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1097e-08 - val_loss: 4.1170e-08 - learning_rate: 3.9553e-06\n",
      "Epoch 456/700\n",
      "\u001b[1m106/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2010e-08\n",
      "Epoch 456: ReduceLROnPlateau reducing learning rate to 3.7575787700916406e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1929e-08 - val_loss: 4.1168e-08 - learning_rate: 3.9553e-06\n",
      "Epoch 457/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1776e-08 - val_loss: 4.1255e-08 - learning_rate: 3.7576e-06\n",
      "Epoch 458/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1394e-08 - val_loss: 4.1273e-08 - learning_rate: 3.7576e-06\n",
      "Epoch 459/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1323e-08 - val_loss: 4.1154e-08 - learning_rate: 3.7576e-06\n",
      "Epoch 460/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1768e-08 - val_loss: 4.1502e-08 - learning_rate: 3.7576e-06\n",
      "Epoch 461/700\n",
      "\u001b[1m127/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1425e-08\n",
      "Epoch 461: ReduceLROnPlateau reducing learning rate to 3.5696997883860602e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1428e-08 - val_loss: 4.1288e-08 - learning_rate: 3.7576e-06\n",
      "Epoch 462/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1729e-08 - val_loss: 4.1167e-08 - learning_rate: 3.5697e-06\n",
      "Epoch 463/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1951e-08 - val_loss: 4.1169e-08 - learning_rate: 3.5697e-06\n",
      "Epoch 464/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1617e-08 - val_loss: 4.1161e-08 - learning_rate: 3.5697e-06\n",
      "Epoch 465/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1257e-08 - val_loss: 4.1197e-08 - learning_rate: 3.5697e-06\n",
      "Epoch 466/700\n",
      "\u001b[1m 89/129\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2005e-08\n",
      "Epoch 466: ReduceLROnPlateau reducing learning rate to 3.3912148637682547e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1897e-08 - val_loss: 4.1108e-08 - learning_rate: 3.5697e-06\n",
      "Epoch 467/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1703e-08 - val_loss: 4.1199e-08 - learning_rate: 3.3912e-06\n",
      "Epoch 468/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1319e-08 - val_loss: 4.1118e-08 - learning_rate: 3.3912e-06\n",
      "Epoch 469/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0918e-08 - val_loss: 4.1080e-08 - learning_rate: 3.3912e-06\n",
      "Epoch 470/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1196e-08 - val_loss: 4.1078e-08 - learning_rate: 3.3912e-06\n",
      "Epoch 471/700\n",
      "\u001b[1m 88/129\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1493e-08\n",
      "Epoch 471: ReduceLROnPlateau reducing learning rate to 3.2216541853813395e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1514e-08 - val_loss: 4.1115e-08 - learning_rate: 3.3912e-06\n",
      "Epoch 472/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1515e-08 - val_loss: 4.1212e-08 - learning_rate: 3.2217e-06\n",
      "Epoch 473/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1635e-08 - val_loss: 4.1148e-08 - learning_rate: 3.2217e-06\n",
      "Epoch 474/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1918e-08 - val_loss: 4.1133e-08 - learning_rate: 3.2217e-06\n",
      "Epoch 475/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1409e-08 - val_loss: 4.1067e-08 - learning_rate: 3.2217e-06\n",
      "Epoch 476/700\n",
      "\u001b[1m105/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1533e-08\n",
      "Epoch 476: ReduceLROnPlateau reducing learning rate to 3.0605714869125222e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1530e-08 - val_loss: 4.1143e-08 - learning_rate: 3.2217e-06\n",
      "Epoch 477/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1364e-08 - val_loss: 4.1055e-08 - learning_rate: 3.0606e-06\n",
      "Epoch 478/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2031e-08 - val_loss: 4.1077e-08 - learning_rate: 3.0606e-06\n",
      "Epoch 479/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1675e-08 - val_loss: 4.1078e-08 - learning_rate: 3.0606e-06\n",
      "Epoch 480/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1556e-08 - val_loss: 4.1150e-08 - learning_rate: 3.0606e-06\n",
      "Epoch 481/700\n",
      "\u001b[1m 93/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0960e-08\n",
      "Epoch 481: ReduceLROnPlateau reducing learning rate to 2.907542966568144e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1047e-08 - val_loss: 4.1048e-08 - learning_rate: 3.0606e-06\n",
      "Epoch 482/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2070e-08 - val_loss: 4.1105e-08 - learning_rate: 2.9075e-06\n",
      "Epoch 483/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1950e-08 - val_loss: 4.1063e-08 - learning_rate: 2.9075e-06\n",
      "Epoch 484/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1578e-08 - val_loss: 4.1084e-08 - learning_rate: 2.9075e-06\n",
      "Epoch 485/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1938e-08 - val_loss: 4.1056e-08 - learning_rate: 2.9075e-06\n",
      "Epoch 486/700\n",
      "\u001b[1m104/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1291e-08\n",
      "Epoch 486: ReduceLROnPlateau reducing learning rate to 2.7621657750387382e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1316e-08 - val_loss: 4.1104e-08 - learning_rate: 2.9075e-06\n",
      "Epoch 487/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1813e-08 - val_loss: 4.1017e-08 - learning_rate: 2.7622e-06\n",
      "Epoch 488/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1146e-08 - val_loss: 4.0998e-08 - learning_rate: 2.7622e-06\n",
      "Epoch 489/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1687e-08 - val_loss: 4.1075e-08 - learning_rate: 2.7622e-06\n",
      "Epoch 490/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1160e-08 - val_loss: 4.1003e-08 - learning_rate: 2.7622e-06\n",
      "Epoch 491/700\n",
      "\u001b[1m 94/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0927e-08  \n",
      "Epoch 491: ReduceLROnPlateau reducing learning rate to 2.6240575834890476e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1064e-08 - val_loss: 4.1007e-08 - learning_rate: 2.7622e-06\n",
      "Epoch 492/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1838e-08 - val_loss: 4.1012e-08 - learning_rate: 2.6241e-06\n",
      "Epoch 493/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1679e-08 - val_loss: 4.1070e-08 - learning_rate: 2.6241e-06\n",
      "Epoch 494/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1261e-08 - val_loss: 4.1067e-08 - learning_rate: 2.6241e-06\n",
      "Epoch 495/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1436e-08 - val_loss: 4.0978e-08 - learning_rate: 2.6241e-06\n",
      "Epoch 496/700\n",
      "\u001b[1m101/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1672e-08\n",
      "Epoch 496: ReduceLROnPlateau reducing learning rate to 2.492854639513098e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1620e-08 - val_loss: 4.1096e-08 - learning_rate: 2.6241e-06\n",
      "Epoch 497/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1481e-08 - val_loss: 4.0984e-08 - learning_rate: 2.4929e-06\n",
      "Epoch 498/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1164e-08 - val_loss: 4.1023e-08 - learning_rate: 2.4929e-06\n",
      "Epoch 499/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1269e-08 - val_loss: 4.0980e-08 - learning_rate: 2.4929e-06\n",
      "Epoch 500/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1447e-08 - val_loss: 4.1000e-08 - learning_rate: 2.4929e-06\n",
      "Epoch 501/700\n",
      "\u001b[1m104/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 4.0755e-08 \n",
      "Epoch 501: ReduceLROnPlateau reducing learning rate to 2.36821198313919e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0894e-08 - val_loss: 4.0950e-08 - learning_rate: 2.4929e-06\n",
      "Epoch 502/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1451e-08 - val_loss: 4.0965e-08 - learning_rate: 2.3682e-06\n",
      "Epoch 503/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1614e-08 - val_loss: 4.0994e-08 - learning_rate: 2.3682e-06\n",
      "Epoch 504/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1585e-08 - val_loss: 4.1090e-08 - learning_rate: 2.3682e-06\n",
      "Epoch 505/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1129e-08 - val_loss: 4.1018e-08 - learning_rate: 2.3682e-06\n",
      "Epoch 506/700\n",
      "\u001b[1m 94/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1475e-08\n",
      "Epoch 506: ReduceLROnPlateau reducing learning rate to 2.249801286779984e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1435e-08 - val_loss: 4.0969e-08 - learning_rate: 2.3682e-06\n",
      "Epoch 507/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1518e-08 - val_loss: 4.1093e-08 - learning_rate: 2.2498e-06\n",
      "Epoch 508/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1680e-08 - val_loss: 4.1007e-08 - learning_rate: 2.2498e-06\n",
      "Epoch 509/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1746e-08 - val_loss: 4.1090e-08 - learning_rate: 2.2498e-06\n",
      "Epoch 510/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1740e-08 - val_loss: 4.1134e-08 - learning_rate: 2.2498e-06\n",
      "Epoch 511/700\n",
      "\u001b[1m105/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 4.1435e-08\n",
      "Epoch 511: ReduceLROnPlateau reducing learning rate to 2.1373112872424825e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1428e-08 - val_loss: 4.1099e-08 - learning_rate: 2.2498e-06\n",
      "Epoch 512/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1733e-08 - val_loss: 4.0941e-08 - learning_rate: 2.1373e-06\n",
      "Epoch 513/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1146e-08 - val_loss: 4.0947e-08 - learning_rate: 2.1373e-06\n",
      "Epoch 514/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1126e-08 - val_loss: 4.0933e-08 - learning_rate: 2.1373e-06\n",
      "Epoch 515/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1442e-08 - val_loss: 4.1036e-08 - learning_rate: 2.1373e-06\n",
      "Epoch 516/700\n",
      "\u001b[1m 84/129\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1297e-08\n",
      "Epoch 516: ReduceLROnPlateau reducing learning rate to 2.030445625678112e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1316e-08 - val_loss: 4.1027e-08 - learning_rate: 2.1373e-06\n",
      "Epoch 517/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1215e-08 - val_loss: 4.1037e-08 - learning_rate: 2.0304e-06\n",
      "Epoch 518/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1414e-08 - val_loss: 4.1053e-08 - learning_rate: 2.0304e-06\n",
      "Epoch 519/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1483e-08 - val_loss: 4.0940e-08 - learning_rate: 2.0304e-06\n",
      "Epoch 520/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1451e-08 - val_loss: 4.0902e-08 - learning_rate: 2.0304e-06\n",
      "Epoch 521/700\n",
      "\u001b[1m 78/129\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0841e-08\n",
      "Epoch 521: ReduceLROnPlateau reducing learning rate to 1.928923279592709e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1045e-08 - val_loss: 4.0904e-08 - learning_rate: 2.0304e-06\n",
      "Epoch 522/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1703e-08 - val_loss: 4.0930e-08 - learning_rate: 1.9289e-06\n",
      "Epoch 523/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1044e-08 - val_loss: 4.0925e-08 - learning_rate: 1.9289e-06\n",
      "Epoch 524/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1022e-08 - val_loss: 4.0899e-08 - learning_rate: 1.9289e-06\n",
      "Epoch 525/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1283e-08 - val_loss: 4.1043e-08 - learning_rate: 1.9289e-06\n",
      "Epoch 526/700\n",
      "\u001b[1m112/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1406e-08\n",
      "Epoch 526: ReduceLROnPlateau reducing learning rate to 1.8324770508115761e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1399e-08 - val_loss: 4.0899e-08 - learning_rate: 1.9289e-06\n",
      "Epoch 527/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1143e-08 - val_loss: 4.0926e-08 - learning_rate: 1.8325e-06\n",
      "Epoch 528/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0995e-08 - val_loss: 4.0998e-08 - learning_rate: 1.8325e-06\n",
      "Epoch 529/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1278e-08 - val_loss: 4.0876e-08 - learning_rate: 1.8325e-06\n",
      "Epoch 530/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1227e-08 - val_loss: 4.0875e-08 - learning_rate: 1.8325e-06\n",
      "Epoch 531/700\n",
      "\u001b[1m103/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1134e-08\n",
      "Epoch 531: ReduceLROnPlateau reducing learning rate to 1.7408532414719956e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1150e-08 - val_loss: 4.0920e-08 - learning_rate: 1.8325e-06\n",
      "Epoch 532/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1231e-08 - val_loss: 4.0924e-08 - learning_rate: 1.7409e-06\n",
      "Epoch 533/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1192e-08 - val_loss: 4.0896e-08 - learning_rate: 1.7409e-06\n",
      "Epoch 534/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0911e-08 - val_loss: 4.0885e-08 - learning_rate: 1.7409e-06\n",
      "Epoch 535/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1535e-08 - val_loss: 4.0899e-08 - learning_rate: 1.7409e-06\n",
      "Epoch 536/700\n",
      "\u001b[1m108/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1169e-08\n",
      "Epoch 536: ReduceLROnPlateau reducing learning rate to 1.653810573998271e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1202e-08 - val_loss: 4.0952e-08 - learning_rate: 1.7409e-06\n",
      "Epoch 537/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1193e-08 - val_loss: 4.0880e-08 - learning_rate: 1.6538e-06\n",
      "Epoch 538/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1954e-08 - val_loss: 4.0915e-08 - learning_rate: 1.6538e-06\n",
      "Epoch 539/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1874e-08 - val_loss: 4.0896e-08 - learning_rate: 1.6538e-06\n",
      "Epoch 540/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0958e-08 - val_loss: 4.0885e-08 - learning_rate: 1.6538e-06\n",
      "Epoch 541/700\n",
      "\u001b[1m 89/129\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1048e-08\n",
      "Epoch 541: ReduceLROnPlateau reducing learning rate to 1.571120083099231e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1092e-08 - val_loss: 4.0847e-08 - learning_rate: 1.6538e-06\n",
      "Epoch 542/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1654e-08 - val_loss: 4.0914e-08 - learning_rate: 1.5711e-06\n",
      "Epoch 543/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1362e-08 - val_loss: 4.0859e-08 - learning_rate: 1.5711e-06\n",
      "Epoch 544/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1178e-08 - val_loss: 4.0980e-08 - learning_rate: 1.5711e-06\n",
      "Epoch 545/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0996e-08 - val_loss: 4.1049e-08 - learning_rate: 1.5711e-06\n",
      "Epoch 546/700\n",
      "\u001b[1m 83/129\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1142e-08\n",
      "Epoch 546: ReduceLROnPlateau reducing learning rate to 1.4925640357432712e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1187e-08 - val_loss: 4.0890e-08 - learning_rate: 1.5711e-06\n",
      "Epoch 547/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1407e-08 - val_loss: 4.0847e-08 - learning_rate: 1.4926e-06\n",
      "Epoch 548/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0960e-08 - val_loss: 4.0928e-08 - learning_rate: 1.4926e-06\n",
      "Epoch 549/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0874e-08 - val_loss: 4.0870e-08 - learning_rate: 1.4926e-06\n",
      "Epoch 550/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1446e-08 - val_loss: 4.0988e-08 - learning_rate: 1.4926e-06\n",
      "Epoch 551/700\n",
      "\u001b[1m 88/129\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0874e-08\n",
      "Epoch 551: ReduceLROnPlateau reducing learning rate to 1.417935823155858e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0973e-08 - val_loss: 4.0881e-08 - learning_rate: 1.4926e-06\n",
      "Epoch 552/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1386e-08 - val_loss: 4.0833e-08 - learning_rate: 1.4179e-06\n",
      "Epoch 553/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1306e-08 - val_loss: 4.0856e-08 - learning_rate: 1.4179e-06\n",
      "Epoch 554/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1480e-08 - val_loss: 4.0855e-08 - learning_rate: 1.4179e-06\n",
      "Epoch 555/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1520e-08 - val_loss: 4.0917e-08 - learning_rate: 1.4179e-06\n",
      "Epoch 556/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1593e-08\n",
      "Epoch 556: ReduceLROnPlateau reducing learning rate to 1.3470389887970668e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1590e-08 - val_loss: 4.0828e-08 - learning_rate: 1.4179e-06\n",
      "Epoch 557/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1246e-08 - val_loss: 4.0911e-08 - learning_rate: 1.3470e-06\n",
      "Epoch 558/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1253e-08 - val_loss: 4.0879e-08 - learning_rate: 1.3470e-06\n",
      "Epoch 559/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1242e-08 - val_loss: 4.0837e-08 - learning_rate: 1.3470e-06\n",
      "Epoch 560/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1048e-08 - val_loss: 4.0838e-08 - learning_rate: 1.3470e-06\n",
      "Epoch 561/700\n",
      "\u001b[1m124/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1572e-08\n",
      "Epoch 561: ReduceLROnPlateau reducing learning rate to 1.2796870123565895e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1557e-08 - val_loss: 4.0849e-08 - learning_rate: 1.3470e-06\n",
      "Epoch 562/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1477e-08 - val_loss: 4.0889e-08 - learning_rate: 1.2797e-06\n",
      "Epoch 563/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1932e-08 - val_loss: 4.0826e-08 - learning_rate: 1.2797e-06\n",
      "Epoch 564/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0751e-08 - val_loss: 4.0833e-08 - learning_rate: 1.2797e-06\n",
      "Epoch 565/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1221e-08 - val_loss: 4.0840e-08 - learning_rate: 1.2797e-06\n",
      "Epoch 566/700\n",
      "\u001b[1m 99/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0933e-08\n",
      "Epoch 566: ReduceLROnPlateau reducing learning rate to 1.21570266173876e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0962e-08 - val_loss: 4.0813e-08 - learning_rate: 1.2797e-06\n",
      "Epoch 567/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1361e-08 - val_loss: 4.0858e-08 - learning_rate: 1.2157e-06\n",
      "Epoch 568/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1274e-08 - val_loss: 4.0859e-08 - learning_rate: 1.2157e-06\n",
      "Epoch 569/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1249e-08 - val_loss: 4.0881e-08 - learning_rate: 1.2157e-06\n",
      "Epoch 570/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0929e-08 - val_loss: 4.0891e-08 - learning_rate: 1.2157e-06\n",
      "Epoch 571/700\n",
      "\u001b[1m109/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1539e-08\n",
      "Epoch 571: ReduceLROnPlateau reducing learning rate to 1.1549175610525707e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1503e-08 - val_loss: 4.0802e-08 - learning_rate: 1.2157e-06\n",
      "Epoch 572/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1262e-08 - val_loss: 4.0794e-08 - learning_rate: 1.1549e-06\n",
      "Epoch 573/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1347e-08 - val_loss: 4.0849e-08 - learning_rate: 1.1549e-06\n",
      "Epoch 574/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1085e-08 - val_loss: 4.0806e-08 - learning_rate: 1.1549e-06\n",
      "Epoch 575/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0788e-08 - val_loss: 4.0849e-08 - learning_rate: 1.1549e-06\n",
      "Epoch 576/700\n",
      "\u001b[1m 84/129\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1546e-08\n",
      "Epoch 576: ReduceLROnPlateau reducing learning rate to 1.0971716505991936e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1440e-08 - val_loss: 4.0888e-08 - learning_rate: 1.1549e-06\n",
      "Epoch 577/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1002e-08 - val_loss: 4.0868e-08 - learning_rate: 1.0972e-06\n",
      "Epoch 578/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1144e-08 - val_loss: 4.0813e-08 - learning_rate: 1.0972e-06\n",
      "Epoch 579/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0748e-08 - val_loss: 4.0793e-08 - learning_rate: 1.0972e-06\n",
      "Epoch 580/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1463e-08 - val_loss: 4.0820e-08 - learning_rate: 1.0972e-06\n",
      "Epoch 581/700\n",
      "\u001b[1m 98/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1274e-08\n",
      "Epoch 581: ReduceLROnPlateau reducing learning rate to 1.0423130788694833e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1277e-08 - val_loss: 4.0793e-08 - learning_rate: 1.0972e-06\n",
      "Epoch 582/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1365e-08 - val_loss: 4.0785e-08 - learning_rate: 1.0423e-06\n",
      "Epoch 583/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1146e-08 - val_loss: 4.0781e-08 - learning_rate: 1.0423e-06\n",
      "Epoch 584/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1120e-08 - val_loss: 4.0789e-08 - learning_rate: 1.0423e-06\n",
      "Epoch 585/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1311e-08 - val_loss: 4.0913e-08 - learning_rate: 1.0423e-06\n",
      "Epoch 586/700\n",
      "\u001b[1m100/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1545e-08\n",
      "Epoch 586: ReduceLROnPlateau reducing learning rate to 9.901974465265084e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1482e-08 - val_loss: 4.0782e-08 - learning_rate: 1.0423e-06\n",
      "Epoch 587/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1305e-08 - val_loss: 4.0813e-08 - learning_rate: 9.9020e-07\n",
      "Epoch 588/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1283e-08 - val_loss: 4.0811e-08 - learning_rate: 9.9020e-07\n",
      "Epoch 589/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1466e-08 - val_loss: 4.0781e-08 - learning_rate: 9.9020e-07\n",
      "Epoch 590/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0825e-08 - val_loss: 4.0823e-08 - learning_rate: 9.9020e-07\n",
      "Epoch 591/700\n",
      "\u001b[1m 85/129\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1832e-08\n",
      "Epoch 591: ReduceLROnPlateau reducing learning rate to 9.406875904005574e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1648e-08 - val_loss: 4.0791e-08 - learning_rate: 9.9020e-07\n",
      "Epoch 592/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1393e-08 - val_loss: 4.0779e-08 - learning_rate: 9.4069e-07\n",
      "Epoch 593/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1515e-08 - val_loss: 4.0831e-08 - learning_rate: 9.4069e-07\n",
      "Epoch 594/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1386e-08 - val_loss: 4.0788e-08 - learning_rate: 9.4069e-07\n",
      "Epoch 595/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1523e-08 - val_loss: 4.0771e-08 - learning_rate: 9.4069e-07\n",
      "Epoch 596/700\n",
      "\u001b[1m 85/129\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0802e-08\n",
      "Epoch 596: ReduceLROnPlateau reducing learning rate to 8.936532054804047e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0936e-08 - val_loss: 4.0781e-08 - learning_rate: 9.4069e-07\n",
      "Epoch 597/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1211e-08 - val_loss: 4.0769e-08 - learning_rate: 8.9365e-07\n",
      "Epoch 598/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0992e-08 - val_loss: 4.0776e-08 - learning_rate: 8.9365e-07\n",
      "Epoch 599/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0833e-08 - val_loss: 4.0797e-08 - learning_rate: 8.9365e-07\n",
      "Epoch 600/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1352e-08 - val_loss: 4.0765e-08 - learning_rate: 8.9365e-07\n",
      "Epoch 601/700\n",
      "\u001b[1m103/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 4.0948e-08\n",
      "Epoch 601: ReduceLROnPlateau reducing learning rate to 8.489705209058229e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1001e-08 - val_loss: 4.0774e-08 - learning_rate: 8.9365e-07\n",
      "Epoch 602/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1179e-08 - val_loss: 4.0775e-08 - learning_rate: 8.4897e-07\n",
      "Epoch 603/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0917e-08 - val_loss: 4.0819e-08 - learning_rate: 8.4897e-07\n",
      "Epoch 604/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1467e-08 - val_loss: 4.0823e-08 - learning_rate: 8.4897e-07\n",
      "Epoch 605/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0921e-08 - val_loss: 4.0768e-08 - learning_rate: 8.4897e-07\n",
      "Epoch 606/700\n",
      "\u001b[1m 98/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1262e-08\n",
      "Epoch 606: ReduceLROnPlateau reducing learning rate to 8.065219759600949e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1231e-08 - val_loss: 4.0789e-08 - learning_rate: 8.4897e-07\n",
      "Epoch 607/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0979e-08 - val_loss: 4.0764e-08 - learning_rate: 8.0652e-07\n",
      "Epoch 608/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1042e-08 - val_loss: 4.0763e-08 - learning_rate: 8.0652e-07\n",
      "Epoch 609/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0927e-08 - val_loss: 4.0752e-08 - learning_rate: 8.0652e-07\n",
      "Epoch 610/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1566e-08 - val_loss: 4.0746e-08 - learning_rate: 8.0652e-07\n",
      "Epoch 611/700\n",
      "\u001b[1m 98/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1618e-08\n",
      "Epoch 611: ReduceLROnPlateau reducing learning rate to 7.66195896062527e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1523e-08 - val_loss: 4.0762e-08 - learning_rate: 8.0652e-07\n",
      "Epoch 612/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0767e-08 - val_loss: 4.0771e-08 - learning_rate: 7.6620e-07\n",
      "Epoch 613/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1330e-08 - val_loss: 4.0748e-08 - learning_rate: 7.6620e-07\n",
      "Epoch 614/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0869e-08 - val_loss: 4.0775e-08 - learning_rate: 7.6620e-07\n",
      "Epoch 615/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1192e-08 - val_loss: 4.0806e-08 - learning_rate: 7.6620e-07\n",
      "Epoch 616/700\n",
      "\u001b[1m119/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 861us/step - loss: 4.1317e-08\n",
      "Epoch 616: ReduceLROnPlateau reducing learning rate to 7.278861147597126e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1303e-08 - val_loss: 4.0753e-08 - learning_rate: 7.6620e-07\n",
      "Epoch 617/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1140e-08 - val_loss: 4.0743e-08 - learning_rate: 7.2789e-07\n",
      "Epoch 618/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1105e-08 - val_loss: 4.0753e-08 - learning_rate: 7.2789e-07\n",
      "Epoch 619/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1572e-08 - val_loss: 4.0759e-08 - learning_rate: 7.2789e-07\n",
      "Epoch 620/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1420e-08 - val_loss: 4.0739e-08 - learning_rate: 7.2789e-07\n",
      "Epoch 621/700\n",
      "\u001b[1m104/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 4.1541e-08\n",
      "Epoch 621: ReduceLROnPlateau reducing learning rate to 6.914918117217894e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1447e-08 - val_loss: 4.0767e-08 - learning_rate: 7.2789e-07\n",
      "Epoch 622/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1291e-08 - val_loss: 4.0735e-08 - learning_rate: 6.9149e-07\n",
      "Epoch 623/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0884e-08 - val_loss: 4.0757e-08 - learning_rate: 6.9149e-07\n",
      "Epoch 624/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1165e-08 - val_loss: 4.0770e-08 - learning_rate: 6.9149e-07\n",
      "Epoch 625/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1091e-08 - val_loss: 4.0773e-08 - learning_rate: 6.9149e-07\n",
      "Epoch 626/700\n",
      "\u001b[1m 93/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1726e-08\n",
      "Epoch 626: ReduceLROnPlateau reducing learning rate to 6.569172427361991e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1528e-08 - val_loss: 4.0741e-08 - learning_rate: 6.9149e-07\n",
      "Epoch 627/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0813e-08 - val_loss: 4.0735e-08 - learning_rate: 6.5692e-07\n",
      "Epoch 628/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1007e-08 - val_loss: 4.0733e-08 - learning_rate: 6.5692e-07\n",
      "Epoch 629/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1154e-08 - val_loss: 4.0748e-08 - learning_rate: 6.5692e-07\n",
      "Epoch 630/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0752e-08 - val_loss: 4.0731e-08 - learning_rate: 6.5692e-07\n",
      "Epoch 631/700\n",
      "\u001b[1m116/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 4.1319e-08\n",
      "Epoch 631: ReduceLROnPlateau reducing learning rate to 6.240713616989524e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1302e-08 - val_loss: 4.0734e-08 - learning_rate: 6.5692e-07\n",
      "Epoch 632/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1118e-08 - val_loss: 4.0758e-08 - learning_rate: 6.2407e-07\n",
      "Epoch 633/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0965e-08 - val_loss: 4.0739e-08 - learning_rate: 6.2407e-07\n",
      "Epoch 634/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1298e-08 - val_loss: 4.0757e-08 - learning_rate: 6.2407e-07\n",
      "Epoch 635/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0990e-08 - val_loss: 4.0750e-08 - learning_rate: 6.2407e-07\n",
      "Epoch 636/700\n",
      "\u001b[1m128/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 4.0965e-08\n",
      "Epoch 636: ReduceLROnPlateau reducing learning rate to 5.928678206146287e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0968e-08 - val_loss: 4.0732e-08 - learning_rate: 6.2407e-07\n",
      "Epoch 637/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0999e-08 - val_loss: 4.0754e-08 - learning_rate: 5.9287e-07\n",
      "Epoch 638/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1087e-08 - val_loss: 4.0724e-08 - learning_rate: 5.9287e-07\n",
      "Epoch 639/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0691e-08 - val_loss: 4.0727e-08 - learning_rate: 5.9287e-07\n",
      "Epoch 640/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0842e-08 - val_loss: 4.0754e-08 - learning_rate: 5.9287e-07\n",
      "Epoch 641/700\n",
      "\u001b[1m112/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 912us/step - loss: 4.0750e-08\n",
      "Epoch 641: ReduceLROnPlateau reducing learning rate to 5.632244295838972e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0807e-08 - val_loss: 4.0737e-08 - learning_rate: 5.9287e-07\n",
      "Epoch 642/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1062e-08 - val_loss: 4.0719e-08 - learning_rate: 5.6322e-07\n",
      "Epoch 643/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0872e-08 - val_loss: 4.0721e-08 - learning_rate: 5.6322e-07\n",
      "Epoch 644/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1353e-08 - val_loss: 4.0759e-08 - learning_rate: 5.6322e-07\n",
      "Epoch 645/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1527e-08 - val_loss: 4.0716e-08 - learning_rate: 5.6322e-07\n",
      "Epoch 646/700\n",
      "\u001b[1m120/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 856us/step - loss: 4.1668e-08\n",
      "Epoch 646: ReduceLROnPlateau reducing learning rate to 5.350632108047648e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1624e-08 - val_loss: 4.0723e-08 - learning_rate: 5.6322e-07\n",
      "Epoch 647/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1296e-08 - val_loss: 4.0768e-08 - learning_rate: 5.3506e-07\n",
      "Epoch 648/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1243e-08 - val_loss: 4.0722e-08 - learning_rate: 5.3506e-07\n",
      "Epoch 649/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1442e-08 - val_loss: 4.0726e-08 - learning_rate: 5.3506e-07\n",
      "Epoch 650/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1334e-08 - val_loss: 4.0733e-08 - learning_rate: 5.3506e-07\n",
      "Epoch 651/700\n",
      "\u001b[1m118/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 4.1248e-08\n",
      "Epoch 651: ReduceLROnPlateau reducing learning rate to 5.083100745650882e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1235e-08 - val_loss: 4.0724e-08 - learning_rate: 5.3506e-07\n",
      "Epoch 652/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1450e-08 - val_loss: 4.0717e-08 - learning_rate: 5.0831e-07\n",
      "Epoch 653/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0774e-08 - val_loss: 4.0745e-08 - learning_rate: 5.0831e-07\n",
      "Epoch 654/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1518e-08 - val_loss: 4.0713e-08 - learning_rate: 5.0831e-07\n",
      "Epoch 655/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1378e-08 - val_loss: 4.0710e-08 - learning_rate: 5.0831e-07\n",
      "Epoch 656/700\n",
      "\u001b[1m 99/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0725e-08  \n",
      "Epoch 656: ReduceLROnPlateau reducing learning rate to 4.828945492363346e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0794e-08 - val_loss: 4.0707e-08 - learning_rate: 5.0831e-07\n",
      "Epoch 657/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1197e-08 - val_loss: 4.0711e-08 - learning_rate: 4.8289e-07\n",
      "Epoch 658/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1176e-08 - val_loss: 4.0715e-08 - learning_rate: 4.8289e-07\n",
      "Epoch 659/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1174e-08 - val_loss: 4.0727e-08 - learning_rate: 4.8289e-07\n",
      "Epoch 660/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1143e-08 - val_loss: 4.0709e-08 - learning_rate: 4.8289e-07\n",
      "Epoch 661/700\n",
      "\u001b[1m122/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 843us/step - loss: 4.1213e-08\n",
      "Epoch 661: ReduceLROnPlateau reducing learning rate to 4.587498352748298e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1205e-08 - val_loss: 4.0707e-08 - learning_rate: 4.8289e-07\n",
      "Epoch 662/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1035e-08 - val_loss: 4.0707e-08 - learning_rate: 4.5875e-07\n",
      "Epoch 663/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1210e-08 - val_loss: 4.0712e-08 - learning_rate: 4.5875e-07\n",
      "Epoch 664/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1236e-08 - val_loss: 4.0705e-08 - learning_rate: 4.5875e-07\n",
      "Epoch 665/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0776e-08 - val_loss: 4.0706e-08 - learning_rate: 4.5875e-07\n",
      "Epoch 666/700\n",
      "\u001b[1m117/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 4.1035e-08\n",
      "Epoch 666: ReduceLROnPlateau reducing learning rate to 4.358123462111507e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1042e-08 - val_loss: 4.0745e-08 - learning_rate: 4.5875e-07\n",
      "Epoch 667/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1130e-08 - val_loss: 4.0718e-08 - learning_rate: 4.3581e-07\n",
      "Epoch 668/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1529e-08 - val_loss: 4.0706e-08 - learning_rate: 4.3581e-07\n",
      "Epoch 669/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0988e-08 - val_loss: 4.0702e-08 - learning_rate: 4.3581e-07\n",
      "Epoch 670/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0986e-08 - val_loss: 4.0709e-08 - learning_rate: 4.3581e-07\n",
      "Epoch 671/700\n",
      "\u001b[1m 95/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1021e-08\n",
      "Epoch 671: ReduceLROnPlateau reducing learning rate to 4.1402173565074915e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1008e-08 - val_loss: 4.0717e-08 - learning_rate: 4.3581e-07\n",
      "Epoch 672/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0947e-08 - val_loss: 4.0723e-08 - learning_rate: 4.1402e-07\n",
      "Epoch 673/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0898e-08 - val_loss: 4.0706e-08 - learning_rate: 4.1402e-07\n",
      "Epoch 674/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0582e-08 - val_loss: 4.0731e-08 - learning_rate: 4.1402e-07\n",
      "Epoch 675/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0910e-08 - val_loss: 4.0717e-08 - learning_rate: 4.1402e-07\n",
      "Epoch 676/700\n",
      "\u001b[1m119/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 857us/step - loss: 4.1291e-08\n",
      "Epoch 676: ReduceLROnPlateau reducing learning rate to 3.933206542683365e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1271e-08 - val_loss: 4.0697e-08 - learning_rate: 4.1402e-07\n",
      "Epoch 677/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0858e-08 - val_loss: 4.0709e-08 - learning_rate: 3.9332e-07\n",
      "Epoch 678/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1110e-08 - val_loss: 4.0721e-08 - learning_rate: 3.9332e-07\n",
      "Epoch 679/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0784e-08 - val_loss: 4.0700e-08 - learning_rate: 3.9332e-07\n",
      "Epoch 680/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1090e-08 - val_loss: 4.0707e-08 - learning_rate: 3.9332e-07\n",
      "Epoch 681/700\n",
      "\u001b[1m111/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 914us/step - loss: 4.1293e-08\n",
      "Epoch 681: ReduceLROnPlateau reducing learning rate to 3.736546148047637e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1271e-08 - val_loss: 4.0717e-08 - learning_rate: 3.9332e-07\n",
      "Epoch 682/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1565e-08 - val_loss: 4.0701e-08 - learning_rate: 3.7365e-07\n",
      "Epoch 683/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1427e-08 - val_loss: 4.0697e-08 - learning_rate: 3.7365e-07\n",
      "Epoch 684/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1441e-08 - val_loss: 4.0700e-08 - learning_rate: 3.7365e-07\n",
      "Epoch 685/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0845e-08 - val_loss: 4.0697e-08 - learning_rate: 3.7365e-07\n",
      "Epoch 686/700\n",
      "\u001b[1m108/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 942us/step - loss: 4.1066e-08\n",
      "Epoch 686: ReduceLROnPlateau reducing learning rate to 3.549718840645255e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1098e-08 - val_loss: 4.0720e-08 - learning_rate: 3.7365e-07\n",
      "Epoch 687/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1027e-08 - val_loss: 4.0700e-08 - learning_rate: 3.5497e-07\n",
      "Epoch 688/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1108e-08 - val_loss: 4.0710e-08 - learning_rate: 3.5497e-07\n",
      "Epoch 689/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0742e-08 - val_loss: 4.0695e-08 - learning_rate: 3.5497e-07\n",
      "Epoch 690/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0651e-08 - val_loss: 4.0703e-08 - learning_rate: 3.5497e-07\n",
      "Epoch 691/700\n",
      "\u001b[1m122/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1458e-08\n",
      "Epoch 691: ReduceLROnPlateau reducing learning rate to 3.372232939113928e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1433e-08 - val_loss: 4.0742e-08 - learning_rate: 3.5497e-07\n",
      "Epoch 692/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1050e-08 - val_loss: 4.0699e-08 - learning_rate: 3.3722e-07\n",
      "Epoch 693/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1005e-08 - val_loss: 4.0690e-08 - learning_rate: 3.3722e-07\n",
      "Epoch 694/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0980e-08 - val_loss: 4.0700e-08 - learning_rate: 3.3722e-07\n",
      "Epoch 695/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1515e-08 - val_loss: 4.0704e-08 - learning_rate: 3.3722e-07\n",
      "Epoch 696/700\n",
      "\u001b[1m116/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 4.0751e-08\n",
      "Epoch 696: ReduceLROnPlateau reducing learning rate to 3.203621332659168e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0785e-08 - val_loss: 4.0700e-08 - learning_rate: 3.3722e-07\n",
      "Epoch 697/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0797e-08 - val_loss: 4.0696e-08 - learning_rate: 3.2036e-07\n",
      "Epoch 698/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0517e-08 - val_loss: 4.0701e-08 - learning_rate: 3.2036e-07\n",
      "Epoch 699/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1077e-08 - val_loss: 4.0699e-08 - learning_rate: 3.2036e-07\n",
      "Epoch 700/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1045e-08 - val_loss: 4.0697e-08 - learning_rate: 3.2036e-07\n"
     ]
    }
   ],
   "source": [
    "# Define training parameters\n",
    "maxEpochs = 700 # val_loss ~ 3.5e-8, NMSE=-33.416, n_prev = 4. [increase to 700 if didn't achieve this]\n",
    "miniBatchSize = 1024\n",
    "iterPerEpoch = len(X_train) // miniBatchSize\n",
    "validation_freq = 2 * iterPerEpoch\n",
    "\n",
    "\n",
    "# history = model.fit(X_train, (y_train.y_real, y_train.y_img), epochs = 200, batch_size = 10,\n",
    "#                     validation_data = (X_val, (y_val.y_real, y_val.y_img)))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, (y_train[:,0], y_train[:,1]),\n",
    "    epochs=maxEpochs,\n",
    "    batch_size=miniBatchSize,\n",
    "    callbacks=[checkpoint_callback,reduce_lr],\n",
    "    validation_data = (X_val, (y_val[:,0], y_val[:,1])),\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('tf_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = tf.reshape(model.predict(X_test),(131520,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helperNMSE(tf.convert_to_tensor(y_test,dtype=tf.float32),y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(pd.DataFrame(history.history['loss'])).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4110/4110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 647us/step\n"
     ]
    }
   ],
   "source": [
    "tf_output = model.predict(preprocess(y_test))\n",
    "tf_output = np.concatenate((tf_output[0],tf_output[1]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tf_output).to_csv('tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_output = y_min_max_scaler.inverse_transform(tf_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "\n",
    "for i in range(len(tf_output)):\n",
    "    arr.append(f'{tf_output[i][0]} + {tf_output[i][1]}i')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(arr).to_csv('tf_model_output_test.csv',index=False,header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "# Function to parse complex numbers from the given format\n",
    "def parse_complex_number(s):\n",
    "    real, imag = s.split(' + ')\n",
    "    real = float(real)\n",
    "    imag = float(imag.replace('i', ''))\n",
    "    return np.complex64(real + 1j * imag)\n",
    "\n",
    "# Read the CSV file\n",
    "filename = './tf_model_output_test.csv'  # Replace with your CSV filename\n",
    "with open(filename, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Parse the lines into complex numbers\n",
    "complex_numbers = np.array([parse_complex_number(line.strip()) for line in lines], dtype=np.complex64)\n",
    "\n",
    "# Save the array to a .mat file\n",
    "output_filename = 'complex_data.mat'\n",
    "scipy.io.savemat(output_filename, {'complex_data': complex_numbers})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
