{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Input, Normalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.txt')\n",
    "df_val = pd.read_csv('val.txt')\n",
    "df_test = pd.read_csv('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1_1</th>\n",
       "      <th>Var1_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001374191+0.0007656791i</td>\n",
       "      <td>0.001241228+0.0008236876i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002563628+0.0009175068i</td>\n",
       "      <td>0.002474111+0.001257619i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003343391+0.001457953i</td>\n",
       "      <td>0.003212301+0.001847259i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003577207+0.0020518i</td>\n",
       "      <td>0.003443445+0.002519653i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003223582+0.00233736i</td>\n",
       "      <td>0.002986031+0.002810816i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131515</th>\n",
       "      <td>-0.003944376+0.003175103i</td>\n",
       "      <td>-0.004349313+0.002706349i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131516</th>\n",
       "      <td>-0.003387596+0.002728857i</td>\n",
       "      <td>-0.003774833+0.002274562i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131517</th>\n",
       "      <td>-0.002925173+0.001754717i</td>\n",
       "      <td>-0.003207921+0.001297825i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131518</th>\n",
       "      <td>-0.002830184+0.0003680737i</td>\n",
       "      <td>-0.002935457-0.0001179909i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131519</th>\n",
       "      <td>-0.00333057-0.001272039i</td>\n",
       "      <td>-0.003206692-0.00180746i</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131520 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Var1_1                      Var1_2\n",
       "0        0.001374191+0.0007656791i   0.001241228+0.0008236876i\n",
       "1        0.002563628+0.0009175068i    0.002474111+0.001257619i\n",
       "2         0.003343391+0.001457953i    0.003212301+0.001847259i\n",
       "3           0.003577207+0.0020518i    0.003443445+0.002519653i\n",
       "4          0.003223582+0.00233736i    0.002986031+0.002810816i\n",
       "...                            ...                         ...\n",
       "131515   -0.003944376+0.003175103i   -0.004349313+0.002706349i\n",
       "131516   -0.003387596+0.002728857i   -0.003774833+0.002274562i\n",
       "131517   -0.002925173+0.001754717i   -0.003207921+0.001297825i\n",
       "131518  -0.002830184+0.0003680737i  -0.002935457-0.0001179909i\n",
       "131519    -0.00333057-0.001272039i    -0.003206692-0.00180746i\n",
       "\n",
       "[131520 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "df_train['X_real'] = df_train['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_train['X_img'] = df_train['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "df_train['y_real'] = df_train['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_train['y_img'] = df_train['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "\n",
    "# test\n",
    "df_test['X_real'] = df_test['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_test['X_img'] = df_test['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "df_test['y_real'] = df_test['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_test['y_img'] = df_test['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "\n",
    "# val\n",
    "df_val['X_real'] = df_val['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_val['X_img'] = df_val['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "df_val['y_real'] = df_val['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_val['y_img'] = df_val['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[['X_real','X_img']].to_numpy()\n",
    "y_train = df_train[['y_real','y_img']].to_numpy()\n",
    "\n",
    "X_test = df_test[['X_real','X_img']].to_numpy()\n",
    "y_test = df_test[['y_real','y_img']].to_numpy()\n",
    "\n",
    "X_val = df_val[['X_real','X_img']].to_numpy()\n",
    "y_val = df_val[['y_real','y_img']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "\n",
    "# x_min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# y_min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# X_train = x_min_max_scaler.fit_transform(X_train)\n",
    "# y_train = y_min_max_scaler.fit_transform(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val = x_min_max_scaler.transform(X_val)\n",
    "# y_val = y_min_max_scaler.transform(y_val)\n",
    "\n",
    "# X_test = x_min_max_scaler.transform(X_test)\n",
    "# y_test = y_min_max_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the model using Sequential API\n",
    "# model = Sequential([\n",
    "#     Dense(30),\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     Dense(30),\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     Dense(30),\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     Dense(2)\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=Adam(learning_rate=4e-4), \n",
    "#               loss='mean_squared_error', \n",
    "#               metrics=['mean_squared_error'])\n",
    "\n",
    "# # Define training parameters\n",
    "# maxEpochs = 200\n",
    "# miniBatchSize = 1024\n",
    "# iterPerEpoch = len(X_train) // miniBatchSize\n",
    "# validation_freq = 2 * iterPerEpoch\n",
    "\n",
    "# # Callbacks for learning rate adjustment, early stopping, and model checkpoint\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=5, verbose=1, mode='auto')\n",
    "# # early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     epochs=maxEpochs,\n",
    "#     batch_size=miniBatchSize,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     callbacks=[reduce_lr],\n",
    "#     shuffle=True,\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helperNMSE(y_true, y_pred):\n",
    "    \n",
    "    diff = y_pred - y_true\n",
    "    mse = tf.reduce_mean(tf.norm(diff,axis=1)**2) # NOTE THIS IS NOT A GOOD PRACICE AS TF.NORM ALREADY GETS THE SQUARED ERROR THEN SQRT IT\n",
    "    factor = tf.reduce_mean(tf.norm(y_true,axis=1)**2)\n",
    "    # nmse = 10 * tf.math.log(mse / factor) / tf.math.log(tf.constant(10,dtype=tf.float32))\n",
    "    \n",
    "    return mse\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Compute the L1 norms\n",
    "    norm_true = tf.reduce_sum(tf.abs(y_true), axis=1)\n",
    "    norm_pred = tf.reduce_sum(tf.abs(y_pred), axis=1)\n",
    "    \n",
    "    # Reshape to be compatible with the helperNMSE function\n",
    "    norm_true = tf.reshape(norm_true, (-1, 1))\n",
    "    norm_pred = tf.reshape(norm_pred, (-1, 1))\n",
    "    \n",
    "    # Compute the NMSE\n",
    "    loss = helperNMSE(norm_true, norm_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a  = tf.convert_to_tensor([[2,1],[5,6]],dtype=tf.float32)\n",
    "# b = tf.convert_to_tensor([[2,5],[2,4]],dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helperNMSE(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abood\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# defining layers\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "dense_layer_1 = Dense(units = 30, activation = LeakyReLU(alpha=0.01))(input_layer) \n",
    "dense_layer_2 = Dense(units = 24, activation = LeakyReLU(alpha=0.01))(dense_layer_1)\n",
    "dense_layer_3 = Dense(units = 19, activation = LeakyReLU(alpha=0.01))(dense_layer_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Y1 output\n",
    "y1_output = Dense(units = 1, activation = \"linear\", name = \"y1_output\")(dense_layer_2)\n",
    "\n",
    "#Y2 output\n",
    "y2_output = Dense(units = 1, activation = \"linear\", name = \"y2_output\")(dense_layer_3)\n",
    "\n",
    "#Define the model with the input layer and a list of outputs\n",
    "model = Model(inputs = input_layer, outputs = [y1_output, y2_output])\n",
    "\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=5, verbose=1, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='bestmodel.keras',\n",
    "    monitor='val_loss',       # Metric to monitor\n",
    "    save_best_only=True,      # Only save the model if it is the best\n",
    "    verbose=0                 # Verbosity mode\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#specify the optimizer and compile with the loss function for both outputs\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=4e-4)\n",
    "\n",
    "model.compile(optimizer = optimizer,\n",
    "              loss = {'y1_output':'mse','y2_output':'mse'},\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.5455e-05 - val_loss: 5.1081e-07 - learning_rate: 4.0000e-04\n",
      "Epoch 2/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5775e-07 - val_loss: 4.7679e-07 - learning_rate: 4.0000e-04\n",
      "Epoch 3/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2305e-07 - val_loss: 3.6421e-07 - learning_rate: 4.0000e-04\n",
      "Epoch 4/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1613e-07 - val_loss: 6.3336e-07 - learning_rate: 4.0000e-04\n",
      "Epoch 5/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7820e-07 - val_loss: 3.5065e-07 - learning_rate: 4.0000e-04\n",
      "Epoch 6/500\n",
      "\u001b[1m102/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8974e-07\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00037999999040039256.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9015e-07 - val_loss: 2.2468e-07 - learning_rate: 4.0000e-04\n",
      "Epoch 7/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8130e-07 - val_loss: 2.0155e-07 - learning_rate: 3.8000e-04\n",
      "Epoch 8/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3077e-07 - val_loss: 1.8149e-07 - learning_rate: 3.8000e-04\n",
      "Epoch 9/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0599e-07 - val_loss: 2.0733e-07 - learning_rate: 3.8000e-04\n",
      "Epoch 10/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1772e-07 - val_loss: 1.6814e-07 - learning_rate: 3.8000e-04\n",
      "Epoch 11/500\n",
      "\u001b[1m101/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9343e-07\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0003609999839682132.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9303e-07 - val_loss: 2.2635e-07 - learning_rate: 3.8000e-04\n",
      "Epoch 12/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5178e-07 - val_loss: 1.6119e-07 - learning_rate: 3.6100e-04\n",
      "Epoch 13/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7051e-07 - val_loss: 1.1477e-07 - learning_rate: 3.6100e-04\n",
      "Epoch 14/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6133e-07 - val_loss: 1.1844e-07 - learning_rate: 3.6100e-04\n",
      "Epoch 15/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4552e-07 - val_loss: 1.0296e-07 - learning_rate: 3.6100e-04\n",
      "Epoch 16/500\n",
      "\u001b[1m102/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4611e-07\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.00034294998476980254.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4374e-07 - val_loss: 1.0198e-07 - learning_rate: 3.6100e-04\n",
      "Epoch 17/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1040e-07 - val_loss: 1.1838e-07 - learning_rate: 3.4295e-04\n",
      "Epoch 18/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2048e-07 - val_loss: 1.9547e-07 - learning_rate: 3.4295e-04\n",
      "Epoch 19/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4449e-07 - val_loss: 9.6927e-08 - learning_rate: 3.4295e-04\n",
      "Epoch 20/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2247e-07 - val_loss: 1.3691e-07 - learning_rate: 3.4295e-04\n",
      "Epoch 21/500\n",
      "\u001b[1m112/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3851e-07\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00032580249244347216.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3844e-07 - val_loss: 1.2887e-07 - learning_rate: 3.4295e-04\n",
      "Epoch 22/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.5623e-08 - val_loss: 9.2467e-08 - learning_rate: 3.2580e-04\n",
      "Epoch 23/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1929e-07 - val_loss: 2.2346e-07 - learning_rate: 3.2580e-04\n",
      "Epoch 24/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3217e-07 - val_loss: 3.1258e-07 - learning_rate: 3.2580e-04\n",
      "Epoch 25/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4437e-07 - val_loss: 8.4570e-08 - learning_rate: 3.2580e-04\n",
      "Epoch 26/500\n",
      "\u001b[1m126/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1578e-07\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00030951235676184296.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1611e-07 - val_loss: 8.3256e-08 - learning_rate: 3.2580e-04\n",
      "Epoch 27/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.8930e-08 - val_loss: 1.0429e-07 - learning_rate: 3.0951e-04\n",
      "Epoch 28/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0467e-07 - val_loss: 9.0269e-08 - learning_rate: 3.0951e-04\n",
      "Epoch 29/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0739e-07 - val_loss: 1.0402e-07 - learning_rate: 3.0951e-04\n",
      "Epoch 30/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2940e-07 - val_loss: 1.1979e-07 - learning_rate: 3.0951e-04\n",
      "Epoch 31/500\n",
      "\u001b[1m127/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1595e-07\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00029403674998320636.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1579e-07 - val_loss: 1.0973e-07 - learning_rate: 3.0951e-04\n",
      "Epoch 32/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.6763e-08 - val_loss: 8.3716e-08 - learning_rate: 2.9404e-04\n",
      "Epoch 33/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0964e-07 - val_loss: 8.2170e-08 - learning_rate: 2.9404e-04\n",
      "Epoch 34/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0843e-07 - val_loss: 1.0553e-07 - learning_rate: 2.9404e-04\n",
      "Epoch 35/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.8877e-08 - val_loss: 2.1888e-07 - learning_rate: 2.9404e-04\n",
      "Epoch 36/500\n",
      "\u001b[1m 97/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3262e-07\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.00027933491801377384.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2725e-07 - val_loss: 1.3357e-07 - learning_rate: 2.9404e-04\n",
      "Epoch 37/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0492e-07 - val_loss: 1.4086e-07 - learning_rate: 2.7933e-04\n",
      "Epoch 38/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0074e-07 - val_loss: 8.3854e-08 - learning_rate: 2.7933e-04\n",
      "Epoch 39/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0729e-07 - val_loss: 7.7109e-08 - learning_rate: 2.7933e-04\n",
      "Epoch 40/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.4805e-08 - val_loss: 1.7393e-07 - learning_rate: 2.7933e-04\n",
      "Epoch 41/500\n",
      "\u001b[1m124/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.9373e-08\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00026536818040767685.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.9218e-08 - val_loss: 1.0214e-07 - learning_rate: 2.7933e-04\n",
      "Epoch 42/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7011e-08 - val_loss: 9.1902e-08 - learning_rate: 2.6537e-04\n",
      "Epoch 43/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.5189e-08 - val_loss: 2.0585e-07 - learning_rate: 2.6537e-04\n",
      "Epoch 44/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1788e-07 - val_loss: 1.4058e-07 - learning_rate: 2.6537e-04\n",
      "Epoch 45/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1013e-07 - val_loss: 1.0636e-07 - learning_rate: 2.6537e-04\n",
      "Epoch 46/500\n",
      "\u001b[1m128/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0018e-07\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.00025209976447513325.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0020e-07 - val_loss: 7.9744e-08 - learning_rate: 2.6537e-04\n",
      "Epoch 47/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7235e-08 - val_loss: 8.9104e-08 - learning_rate: 2.5210e-04\n",
      "Epoch 48/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1041e-08 - val_loss: 9.0692e-08 - learning_rate: 2.5210e-04\n",
      "Epoch 49/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1022e-07 - val_loss: 7.9231e-08 - learning_rate: 2.5210e-04\n",
      "Epoch 50/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1694e-08 - val_loss: 1.3478e-07 - learning_rate: 2.5210e-04\n",
      "Epoch 51/500\n",
      "\u001b[1m123/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0153e-07\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.00023949477763380854.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0105e-07 - val_loss: 9.6919e-08 - learning_rate: 2.5210e-04\n",
      "Epoch 52/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.8877e-08 - val_loss: 9.8223e-08 - learning_rate: 2.3949e-04\n",
      "Epoch 53/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.8532e-08 - val_loss: 7.6114e-08 - learning_rate: 2.3949e-04\n",
      "Epoch 54/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.2352e-08 - val_loss: 9.2247e-08 - learning_rate: 2.3949e-04\n",
      "Epoch 55/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.3953e-08 - val_loss: 7.8816e-08 - learning_rate: 2.3949e-04\n",
      "Epoch 56/500\n",
      "\u001b[1m128/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.3066e-08\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.000227520041516982.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.3054e-08 - val_loss: 1.0232e-07 - learning_rate: 2.3949e-04\n",
      "Epoch 57/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.2128e-08 - val_loss: 1.5824e-07 - learning_rate: 2.2752e-04\n",
      "Epoch 58/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0463e-07 - val_loss: 8.3263e-08 - learning_rate: 2.2752e-04\n",
      "Epoch 59/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1308e-08 - val_loss: 7.3762e-08 - learning_rate: 2.2752e-04\n",
      "Epoch 60/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.4062e-08 - val_loss: 9.3502e-08 - learning_rate: 2.2752e-04\n",
      "Epoch 61/500\n",
      "\u001b[1m121/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0128e-07\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.00021614403667626902.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0086e-07 - val_loss: 7.8961e-08 - learning_rate: 2.2752e-04\n",
      "Epoch 62/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1225e-08 - val_loss: 7.3164e-08 - learning_rate: 2.1614e-04\n",
      "Epoch 63/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7914e-08 - val_loss: 8.0834e-08 - learning_rate: 2.1614e-04\n",
      "Epoch 64/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7419e-08 - val_loss: 9.8377e-08 - learning_rate: 2.1614e-04\n",
      "Epoch 65/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3603e-08 - val_loss: 8.2750e-08 - learning_rate: 2.1614e-04\n",
      "Epoch 66/500\n",
      "\u001b[1m 98/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.6957e-08\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 0.00020533683346002363.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.6598e-08 - val_loss: 9.2239e-08 - learning_rate: 2.1614e-04\n",
      "Epoch 67/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.4329e-08 - val_loss: 7.8891e-08 - learning_rate: 2.0534e-04\n",
      "Epoch 68/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.4700e-08 - val_loss: 7.3932e-08 - learning_rate: 2.0534e-04\n",
      "Epoch 69/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.7263e-08 - val_loss: 8.5065e-08 - learning_rate: 2.0534e-04\n",
      "Epoch 70/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2510e-08 - val_loss: 9.7119e-08 - learning_rate: 2.0534e-04\n",
      "Epoch 71/500\n",
      "\u001b[1m101/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.5851e-08\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0001950699952431023.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.5659e-08 - val_loss: 1.1157e-07 - learning_rate: 2.0534e-04\n",
      "Epoch 72/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.4347e-08 - val_loss: 7.3536e-08 - learning_rate: 1.9507e-04\n",
      "Epoch 73/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0033e-08 - val_loss: 7.3728e-08 - learning_rate: 1.9507e-04\n",
      "Epoch 74/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2380e-08 - val_loss: 8.5196e-08 - learning_rate: 1.9507e-04\n",
      "Epoch 75/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.0978e-08 - val_loss: 9.6671e-08 - learning_rate: 1.9507e-04\n",
      "Epoch 76/500\n",
      "\u001b[1m119/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.4078e-08\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0001853164954809472.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.3927e-08 - val_loss: 8.0890e-08 - learning_rate: 1.9507e-04\n",
      "Epoch 77/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0658e-08 - val_loss: 7.7102e-08 - learning_rate: 1.8532e-04\n",
      "Epoch 78/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.4938e-08 - val_loss: 8.2773e-08 - learning_rate: 1.8532e-04\n",
      "Epoch 79/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2020e-08 - val_loss: 7.2291e-08 - learning_rate: 1.8532e-04\n",
      "Epoch 80/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1006e-08 - val_loss: 7.2860e-08 - learning_rate: 1.8532e-04\n",
      "Epoch 81/500\n",
      "\u001b[1m100/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0764e-08\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 0.00017605067623662762.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0823e-08 - val_loss: 7.2198e-08 - learning_rate: 1.8532e-04\n",
      "Epoch 82/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5532e-08 - val_loss: 7.2807e-08 - learning_rate: 1.7605e-04\n",
      "Epoch 83/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.5632e-08 - val_loss: 7.5952e-08 - learning_rate: 1.7605e-04\n",
      "Epoch 84/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.8538e-08 - val_loss: 7.7223e-08 - learning_rate: 1.7605e-04\n",
      "Epoch 85/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3572e-08 - val_loss: 1.0159e-07 - learning_rate: 1.7605e-04\n",
      "Epoch 86/500\n",
      "\u001b[1m 98/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6540e-08\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 0.00016724813758628443.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.5898e-08 - val_loss: 7.9075e-08 - learning_rate: 1.7605e-04\n",
      "Epoch 87/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8572e-08 - val_loss: 1.3279e-07 - learning_rate: 1.6725e-04\n",
      "Epoch 88/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6468e-08 - val_loss: 8.5871e-08 - learning_rate: 1.6725e-04\n",
      "Epoch 89/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2345e-08 - val_loss: 7.8848e-08 - learning_rate: 1.6725e-04\n",
      "Epoch 90/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0913e-08 - val_loss: 8.0583e-08 - learning_rate: 1.6725e-04\n",
      "Epoch 91/500\n",
      "\u001b[1m122/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9273e-08\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 0.00015888572379481046.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.9305e-08 - val_loss: 7.0450e-08 - learning_rate: 1.6725e-04\n",
      "Epoch 92/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2290e-08 - val_loss: 7.1025e-08 - learning_rate: 1.5889e-04\n",
      "Epoch 93/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5779e-08 - val_loss: 8.2777e-08 - learning_rate: 1.5889e-04\n",
      "Epoch 94/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0289e-08 - val_loss: 7.0915e-08 - learning_rate: 1.5889e-04\n",
      "Epoch 95/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7541e-08 - val_loss: 7.5741e-08 - learning_rate: 1.5889e-04\n",
      "Epoch 96/500\n",
      "\u001b[1m102/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7779e-08\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 0.00015094144036993383.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.8307e-08 - val_loss: 7.3726e-08 - learning_rate: 1.5889e-04\n",
      "Epoch 97/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2756e-08 - val_loss: 8.0719e-08 - learning_rate: 1.5094e-04\n",
      "Epoch 98/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2464e-08 - val_loss: 7.4499e-08 - learning_rate: 1.5094e-04\n",
      "Epoch 99/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.8239e-08 - val_loss: 7.3570e-08 - learning_rate: 1.5094e-04\n",
      "Epoch 100/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1347e-08 - val_loss: 7.0732e-08 - learning_rate: 1.5094e-04\n",
      "Epoch 101/500\n",
      "\u001b[1m100/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5011e-08\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 0.00014339437111630105.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5742e-08 - val_loss: 7.3392e-08 - learning_rate: 1.5094e-04\n",
      "Epoch 102/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1666e-08 - val_loss: 7.1713e-08 - learning_rate: 1.4339e-04\n",
      "Epoch 103/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3540e-08 - val_loss: 7.8955e-08 - learning_rate: 1.4339e-04\n",
      "Epoch 104/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0138e-08 - val_loss: 6.9990e-08 - learning_rate: 1.4339e-04\n",
      "Epoch 105/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.8293e-08 - val_loss: 7.1894e-08 - learning_rate: 1.4339e-04\n",
      "Epoch 106/500\n",
      "\u001b[1m122/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9286e-08\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 0.00013622465048683806.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.9441e-08 - val_loss: 9.3661e-08 - learning_rate: 1.4339e-04\n",
      "Epoch 107/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2808e-08 - val_loss: 8.1947e-08 - learning_rate: 1.3622e-04\n",
      "Epoch 108/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7192e-08 - val_loss: 7.3934e-08 - learning_rate: 1.3622e-04\n",
      "Epoch 109/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5315e-08 - val_loss: 7.5758e-08 - learning_rate: 1.3622e-04\n",
      "Epoch 110/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0069e-08 - val_loss: 7.7262e-08 - learning_rate: 1.3622e-04\n",
      "Epoch 111/500\n",
      "\u001b[1m101/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5694e-08\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 0.00012941342210979201.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5879e-08 - val_loss: 8.2806e-08 - learning_rate: 1.3622e-04\n",
      "Epoch 112/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5368e-08 - val_loss: 7.6004e-08 - learning_rate: 1.2941e-04\n",
      "Epoch 113/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8251e-08 - val_loss: 8.0024e-08 - learning_rate: 1.2941e-04\n",
      "Epoch 114/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.6415e-08 - val_loss: 7.5562e-08 - learning_rate: 1.2941e-04\n",
      "Epoch 115/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3843e-08 - val_loss: 7.3710e-08 - learning_rate: 1.2941e-04\n",
      "Epoch 116/500\n",
      "\u001b[1m104/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.6454e-08\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 0.00012294275584281422.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6729e-08 - val_loss: 6.9768e-08 - learning_rate: 1.2941e-04\n",
      "Epoch 117/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1798e-08 - val_loss: 8.5820e-08 - learning_rate: 1.2294e-04\n",
      "Epoch 118/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.8618e-08 - val_loss: 7.0796e-08 - learning_rate: 1.2294e-04\n",
      "Epoch 119/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5127e-08 - val_loss: 8.8371e-08 - learning_rate: 1.2294e-04\n",
      "Epoch 120/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.6159e-08 - val_loss: 6.9810e-08 - learning_rate: 1.2294e-04\n",
      "Epoch 121/500\n",
      "\u001b[1m 99/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7997e-08\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 0.00011679562012432142.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.7811e-08 - val_loss: 8.0826e-08 - learning_rate: 1.2294e-04\n",
      "Epoch 122/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3443e-08 - val_loss: 7.8426e-08 - learning_rate: 1.1680e-04\n",
      "Epoch 123/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2232e-08 - val_loss: 8.1719e-08 - learning_rate: 1.1680e-04\n",
      "Epoch 124/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.6227e-08 - val_loss: 7.2952e-08 - learning_rate: 1.1680e-04\n",
      "Epoch 125/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4300e-08 - val_loss: 7.0419e-08 - learning_rate: 1.1680e-04\n",
      "Epoch 126/500\n",
      "\u001b[1m101/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7699e-08\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 0.0001109558405005373.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.7359e-08 - val_loss: 6.9958e-08 - learning_rate: 1.1680e-04\n",
      "Epoch 127/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.4127e-08 - val_loss: 7.8297e-08 - learning_rate: 1.1096e-04\n",
      "Epoch 128/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2874e-08 - val_loss: 8.9168e-08 - learning_rate: 1.1096e-04\n",
      "Epoch 129/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.6805e-08 - val_loss: 8.0739e-08 - learning_rate: 1.1096e-04\n",
      "Epoch 130/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.4668e-08 - val_loss: 7.8884e-08 - learning_rate: 1.1096e-04\n",
      "Epoch 131/500\n",
      "\u001b[1m101/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7579e-08\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 0.00010540805124037433.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.7271e-08 - val_loss: 8.1996e-08 - learning_rate: 1.1096e-04\n",
      "Epoch 132/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6001e-08 - val_loss: 7.7251e-08 - learning_rate: 1.0541e-04\n",
      "Epoch 133/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3694e-08 - val_loss: 7.2370e-08 - learning_rate: 1.0541e-04\n",
      "Epoch 134/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3631e-08 - val_loss: 7.0445e-08 - learning_rate: 1.0541e-04\n",
      "Epoch 135/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3272e-08 - val_loss: 7.7270e-08 - learning_rate: 1.0541e-04\n",
      "Epoch 136/500\n",
      "\u001b[1m 96/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2697e-08\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 0.00010013764695031567.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3035e-08 - val_loss: 8.2236e-08 - learning_rate: 1.0541e-04\n",
      "Epoch 137/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2364e-08 - val_loss: 7.2028e-08 - learning_rate: 1.0014e-04\n",
      "Epoch 138/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3544e-08 - val_loss: 7.9568e-08 - learning_rate: 1.0014e-04\n",
      "Epoch 139/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2754e-08 - val_loss: 8.3580e-08 - learning_rate: 1.0014e-04\n",
      "Epoch 140/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3344e-08 - val_loss: 8.1981e-08 - learning_rate: 1.0014e-04\n",
      "Epoch 141/500\n",
      "\u001b[1m102/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3251e-08\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 9.5130761837936e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3337e-08 - val_loss: 6.8828e-08 - learning_rate: 1.0014e-04\n",
      "Epoch 142/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2390e-08 - val_loss: 7.0671e-08 - learning_rate: 9.5131e-05\n",
      "Epoch 143/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2764e-08 - val_loss: 8.1497e-08 - learning_rate: 9.5131e-05\n",
      "Epoch 144/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4122e-08 - val_loss: 7.4102e-08 - learning_rate: 9.5131e-05\n",
      "Epoch 145/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1865e-08 - val_loss: 7.3753e-08 - learning_rate: 9.5131e-05\n",
      "Epoch 146/500\n",
      "\u001b[1m103/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2864e-08\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 9.037422132678329e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2928e-08 - val_loss: 7.2434e-08 - learning_rate: 9.5131e-05\n",
      "Epoch 147/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1645e-08 - val_loss: 7.6008e-08 - learning_rate: 9.0374e-05\n",
      "Epoch 148/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2166e-08 - val_loss: 7.1030e-08 - learning_rate: 9.0374e-05\n",
      "Epoch 149/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4233e-08 - val_loss: 7.3834e-08 - learning_rate: 9.0374e-05\n",
      "Epoch 150/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3318e-08 - val_loss: 7.6566e-08 - learning_rate: 9.0374e-05\n",
      "Epoch 151/500\n",
      "\u001b[1m114/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.4653e-08\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 8.585550749558024e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4518e-08 - val_loss: 7.2104e-08 - learning_rate: 9.0374e-05\n",
      "Epoch 152/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1080e-08 - val_loss: 6.9580e-08 - learning_rate: 8.5856e-05\n",
      "Epoch 153/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1574e-08 - val_loss: 6.8578e-08 - learning_rate: 8.5856e-05\n",
      "Epoch 154/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1901e-08 - val_loss: 6.9196e-08 - learning_rate: 8.5856e-05\n",
      "Epoch 155/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3638e-08 - val_loss: 6.9734e-08 - learning_rate: 8.5856e-05\n",
      "Epoch 156/500\n",
      "\u001b[1m121/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1574e-08\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 8.156273142958525e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1571e-08 - val_loss: 8.3612e-08 - learning_rate: 8.5856e-05\n",
      "Epoch 157/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2288e-08 - val_loss: 7.1022e-08 - learning_rate: 8.1563e-05\n",
      "Epoch 158/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0727e-08 - val_loss: 7.5570e-08 - learning_rate: 8.1563e-05\n",
      "Epoch 159/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3541e-08 - val_loss: 6.9217e-08 - learning_rate: 8.1563e-05\n",
      "Epoch 160/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0945e-08 - val_loss: 7.3053e-08 - learning_rate: 8.1563e-05\n",
      "Epoch 161/500\n",
      "\u001b[1m123/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2704e-08\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 7.748459174763411e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2746e-08 - val_loss: 8.4818e-08 - learning_rate: 8.1563e-05\n",
      "Epoch 162/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5172e-08 - val_loss: 7.1856e-08 - learning_rate: 7.7485e-05\n",
      "Epoch 163/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0515e-08 - val_loss: 7.2377e-08 - learning_rate: 7.7485e-05\n",
      "Epoch 164/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0561e-08 - val_loss: 7.0082e-08 - learning_rate: 7.7485e-05\n",
      "Epoch 165/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1281e-08 - val_loss: 7.1085e-08 - learning_rate: 7.7485e-05\n",
      "Epoch 166/500\n",
      "\u001b[1m125/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1107e-08\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 7.361036077782046e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1090e-08 - val_loss: 7.2006e-08 - learning_rate: 7.7485e-05\n",
      "Epoch 167/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9516e-08 - val_loss: 8.1086e-08 - learning_rate: 7.3610e-05\n",
      "Epoch 168/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1378e-08 - val_loss: 6.8345e-08 - learning_rate: 7.3610e-05\n",
      "Epoch 169/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0490e-08 - val_loss: 7.1786e-08 - learning_rate: 7.3610e-05\n",
      "Epoch 170/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2144e-08 - val_loss: 8.0302e-08 - learning_rate: 7.3610e-05\n",
      "Epoch 171/500\n",
      "\u001b[1m128/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0669e-08\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 6.992984308453742e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0674e-08 - val_loss: 6.9486e-08 - learning_rate: 7.3610e-05\n",
      "Epoch 172/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0679e-08 - val_loss: 7.0527e-08 - learning_rate: 6.9930e-05\n",
      "Epoch 173/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1623e-08 - val_loss: 6.8610e-08 - learning_rate: 6.9930e-05\n",
      "Epoch 174/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1007e-08 - val_loss: 6.9608e-08 - learning_rate: 6.9930e-05\n",
      "Epoch 175/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0020e-08 - val_loss: 6.9837e-08 - learning_rate: 6.9930e-05\n",
      "Epoch 176/500\n",
      "\u001b[1m125/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0304e-08\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 6.643334781983866e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0314e-08 - val_loss: 6.8229e-08 - learning_rate: 6.9930e-05\n",
      "Epoch 177/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0401e-08 - val_loss: 7.2075e-08 - learning_rate: 6.6433e-05\n",
      "Epoch 178/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0712e-08 - val_loss: 6.8107e-08 - learning_rate: 6.6433e-05\n",
      "Epoch 179/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1652e-08 - val_loss: 6.8787e-08 - learning_rate: 6.6433e-05\n",
      "Epoch 180/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0893e-08 - val_loss: 7.0420e-08 - learning_rate: 6.6433e-05\n",
      "Epoch 181/500\n",
      "\u001b[1m111/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0438e-08\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 6.311168181127869e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0735e-08 - val_loss: 8.3725e-08 - learning_rate: 6.6433e-05\n",
      "Epoch 182/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2703e-08 - val_loss: 7.2789e-08 - learning_rate: 6.3112e-05\n",
      "Epoch 183/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1057e-08 - val_loss: 6.8523e-08 - learning_rate: 6.3112e-05\n",
      "Epoch 184/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0220e-08 - val_loss: 7.2294e-08 - learning_rate: 6.3112e-05\n",
      "Epoch 185/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0521e-08 - val_loss: 6.8756e-08 - learning_rate: 6.3112e-05\n",
      "Epoch 186/500\n",
      "\u001b[1m117/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9167e-08\n",
      "Epoch 186: ReduceLROnPlateau reducing learning rate to 5.995610117679462e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9264e-08 - val_loss: 7.0770e-08 - learning_rate: 6.3112e-05\n",
      "Epoch 187/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0535e-08 - val_loss: 7.5228e-08 - learning_rate: 5.9956e-05\n",
      "Epoch 188/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1745e-08 - val_loss: 7.2683e-08 - learning_rate: 5.9956e-05\n",
      "Epoch 189/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0683e-08 - val_loss: 6.9032e-08 - learning_rate: 5.9956e-05\n",
      "Epoch 190/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9883e-08 - val_loss: 7.3476e-08 - learning_rate: 5.9956e-05\n",
      "Epoch 191/500\n",
      "\u001b[1m121/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1179e-08\n",
      "Epoch 191: ReduceLROnPlateau reducing learning rate to 5.695829750038683e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1116e-08 - val_loss: 7.2839e-08 - learning_rate: 5.9956e-05\n",
      "Epoch 192/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0742e-08 - val_loss: 7.0562e-08 - learning_rate: 5.6958e-05\n",
      "Epoch 193/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9271e-08 - val_loss: 6.8047e-08 - learning_rate: 5.6958e-05\n",
      "Epoch 194/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9851e-08 - val_loss: 6.9463e-08 - learning_rate: 5.6958e-05\n",
      "Epoch 195/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9528e-08 - val_loss: 7.1538e-08 - learning_rate: 5.6958e-05\n",
      "Epoch 196/500\n",
      "\u001b[1m125/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0670e-08\n",
      "Epoch 196: ReduceLROnPlateau reducing learning rate to 5.411038400779944e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0623e-08 - val_loss: 7.0108e-08 - learning_rate: 5.6958e-05\n",
      "Epoch 197/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9365e-08 - val_loss: 6.8286e-08 - learning_rate: 5.4110e-05\n",
      "Epoch 198/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1166e-08 - val_loss: 6.9332e-08 - learning_rate: 5.4110e-05\n",
      "Epoch 199/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9436e-08 - val_loss: 9.4368e-08 - learning_rate: 5.4110e-05\n",
      "Epoch 200/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2501e-08 - val_loss: 6.8200e-08 - learning_rate: 5.4110e-05\n",
      "Epoch 201/500\n",
      "\u001b[1m128/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9893e-08\n",
      "Epoch 201: ReduceLROnPlateau reducing learning rate to 5.1404864461801476e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9906e-08 - val_loss: 7.4092e-08 - learning_rate: 5.4110e-05\n",
      "Epoch 202/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1505e-08 - val_loss: 6.8628e-08 - learning_rate: 5.1405e-05\n",
      "Epoch 203/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9140e-08 - val_loss: 6.8411e-08 - learning_rate: 5.1405e-05\n",
      "Epoch 204/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9315e-08 - val_loss: 6.9434e-08 - learning_rate: 5.1405e-05\n",
      "Epoch 205/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0527e-08 - val_loss: 6.7816e-08 - learning_rate: 5.1405e-05\n",
      "Epoch 206/500\n",
      "\u001b[1m125/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9560e-08\n",
      "Epoch 206: ReduceLROnPlateau reducing learning rate to 4.883462279394734e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9550e-08 - val_loss: 6.8020e-08 - learning_rate: 5.1405e-05\n",
      "Epoch 207/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9209e-08 - val_loss: 6.8576e-08 - learning_rate: 4.8835e-05\n",
      "Epoch 208/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9915e-08 - val_loss: 6.8932e-08 - learning_rate: 4.8835e-05\n",
      "Epoch 209/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9235e-08 - val_loss: 6.8684e-08 - learning_rate: 4.8835e-05\n",
      "Epoch 210/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8987e-08 - val_loss: 6.8299e-08 - learning_rate: 4.8835e-05\n",
      "Epoch 211/500\n",
      "\u001b[1m123/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9586e-08\n",
      "Epoch 211: ReduceLROnPlateau reducing learning rate to 4.639289199985796e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9582e-08 - val_loss: 7.2774e-08 - learning_rate: 4.8835e-05\n",
      "Epoch 212/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0507e-08 - val_loss: 6.8610e-08 - learning_rate: 4.6393e-05\n",
      "Epoch 213/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9256e-08 - val_loss: 6.7644e-08 - learning_rate: 4.6393e-05\n",
      "Epoch 214/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8677e-08 - val_loss: 6.7825e-08 - learning_rate: 4.6393e-05\n",
      "Epoch 215/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8731e-08 - val_loss: 6.8044e-08 - learning_rate: 4.6393e-05\n",
      "Epoch 216/500\n",
      "\u001b[1m115/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9199e-08\n",
      "Epoch 216: ReduceLROnPlateau reducing learning rate to 4.407324722706107e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9229e-08 - val_loss: 6.9798e-08 - learning_rate: 4.6393e-05\n",
      "Epoch 217/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0402e-08 - val_loss: 6.8398e-08 - learning_rate: 4.4073e-05\n",
      "Epoch 218/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9192e-08 - val_loss: 7.0448e-08 - learning_rate: 4.4073e-05\n",
      "Epoch 219/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9173e-08 - val_loss: 7.2231e-08 - learning_rate: 4.4073e-05\n",
      "Epoch 220/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0411e-08 - val_loss: 6.9230e-08 - learning_rate: 4.4073e-05\n",
      "Epoch 221/500\n",
      "\u001b[1m100/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9805e-08\n",
      "Epoch 221: ReduceLROnPlateau reducing learning rate to 4.1869585038512015e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9683e-08 - val_loss: 6.7946e-08 - learning_rate: 4.4073e-05\n",
      "Epoch 222/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8152e-08 - val_loss: 6.9591e-08 - learning_rate: 4.1870e-05\n",
      "Epoch 223/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9689e-08 - val_loss: 6.8162e-08 - learning_rate: 4.1870e-05\n",
      "Epoch 224/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8555e-08 - val_loss: 6.9866e-08 - learning_rate: 4.1870e-05\n",
      "Epoch 225/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9130e-08 - val_loss: 7.0332e-08 - learning_rate: 4.1870e-05\n",
      "Epoch 226/500\n",
      "\u001b[1m101/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8889e-08\n",
      "Epoch 226: ReduceLROnPlateau reducing learning rate to 3.9776106132194396e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8865e-08 - val_loss: 6.7792e-08 - learning_rate: 4.1870e-05\n",
      "Epoch 227/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8220e-08 - val_loss: 6.7827e-08 - learning_rate: 3.9776e-05\n",
      "Epoch 228/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9186e-08 - val_loss: 7.0976e-08 - learning_rate: 3.9776e-05\n",
      "Epoch 229/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9047e-08 - val_loss: 6.8669e-08 - learning_rate: 3.9776e-05\n",
      "Epoch 230/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9020e-08 - val_loss: 6.7711e-08 - learning_rate: 3.9776e-05\n",
      "Epoch 231/500\n",
      "\u001b[1m101/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9668e-08\n",
      "Epoch 231: ReduceLROnPlateau reducing learning rate to 3.778730151680065e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9609e-08 - val_loss: 6.9579e-08 - learning_rate: 3.9776e-05\n",
      "Epoch 232/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9318e-08 - val_loss: 6.8689e-08 - learning_rate: 3.7787e-05\n",
      "Epoch 233/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8116e-08 - val_loss: 7.0802e-08 - learning_rate: 3.7787e-05\n",
      "Epoch 234/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8759e-08 - val_loss: 7.4085e-08 - learning_rate: 3.7787e-05\n",
      "Epoch 235/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9523e-08 - val_loss: 6.9497e-08 - learning_rate: 3.7787e-05\n",
      "Epoch 236/500\n",
      "\u001b[1m 98/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9077e-08\n",
      "Epoch 236: ReduceLROnPlateau reducing learning rate to 3.589793523133266e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9067e-08 - val_loss: 7.1241e-08 - learning_rate: 3.7787e-05\n",
      "Epoch 237/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9401e-08 - val_loss: 6.9295e-08 - learning_rate: 3.5898e-05\n",
      "Epoch 238/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0076e-08 - val_loss: 6.8119e-08 - learning_rate: 3.5898e-05\n",
      "Epoch 239/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8650e-08 - val_loss: 6.8476e-08 - learning_rate: 3.5898e-05\n",
      "Epoch 240/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8761e-08 - val_loss: 6.8065e-08 - learning_rate: 3.5898e-05\n",
      "Epoch 241/500\n",
      "\u001b[1m100/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8860e-08\n",
      "Epoch 241: ReduceLROnPlateau reducing learning rate to 3.410303743294207e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9022e-08 - val_loss: 6.7940e-08 - learning_rate: 3.5898e-05\n",
      "Epoch 242/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8550e-08 - val_loss: 6.7794e-08 - learning_rate: 3.4103e-05\n",
      "Epoch 243/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8238e-08 - val_loss: 6.9279e-08 - learning_rate: 3.4103e-05\n",
      "Epoch 244/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9481e-08 - val_loss: 6.9456e-08 - learning_rate: 3.4103e-05\n",
      "Epoch 245/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9613e-08 - val_loss: 6.8702e-08 - learning_rate: 3.4103e-05\n",
      "Epoch 246/500\n",
      "\u001b[1m100/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8754e-08\n",
      "Epoch 246: ReduceLROnPlateau reducing learning rate to 3.239788711653091e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8846e-08 - val_loss: 6.8514e-08 - learning_rate: 3.4103e-05\n",
      "Epoch 247/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9065e-08 - val_loss: 6.8095e-08 - learning_rate: 3.2398e-05\n",
      "Epoch 248/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8478e-08 - val_loss: 6.7620e-08 - learning_rate: 3.2398e-05\n",
      "Epoch 249/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8012e-08 - val_loss: 6.7999e-08 - learning_rate: 3.2398e-05\n",
      "Epoch 250/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8524e-08 - val_loss: 6.7935e-08 - learning_rate: 3.2398e-05\n",
      "Epoch 251/500\n",
      "\u001b[1m127/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8529e-08\n",
      "Epoch 251: ReduceLROnPlateau reducing learning rate to 3.0777991378272417e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8524e-08 - val_loss: 6.7529e-08 - learning_rate: 3.2398e-05\n",
      "Epoch 252/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7950e-08 - val_loss: 6.8454e-08 - learning_rate: 3.0778e-05\n",
      "Epoch 253/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8966e-08 - val_loss: 7.4600e-08 - learning_rate: 3.0778e-05\n",
      "Epoch 254/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9373e-08 - val_loss: 6.8634e-08 - learning_rate: 3.0778e-05\n",
      "Epoch 255/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8094e-08 - val_loss: 7.1234e-08 - learning_rate: 3.0778e-05\n",
      "Epoch 256/500\n",
      "\u001b[1m125/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8417e-08\n",
      "Epoch 256: ReduceLROnPlateau reducing learning rate to 2.9239092327770777e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8419e-08 - val_loss: 6.7763e-08 - learning_rate: 3.0778e-05\n",
      "Epoch 257/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8630e-08 - val_loss: 6.7379e-08 - learning_rate: 2.9239e-05\n",
      "Epoch 258/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7995e-08 - val_loss: 6.8465e-08 - learning_rate: 2.9239e-05\n",
      "Epoch 259/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9245e-08 - val_loss: 6.8952e-08 - learning_rate: 2.9239e-05\n",
      "Epoch 260/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8334e-08 - val_loss: 6.7720e-08 - learning_rate: 2.9239e-05\n",
      "Epoch 261/500\n",
      "\u001b[1m 97/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8217e-08\n",
      "Epoch 261: ReduceLROnPlateau reducing learning rate to 2.7777137711382237e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8230e-08 - val_loss: 6.7588e-08 - learning_rate: 2.9239e-05\n",
      "Epoch 262/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7845e-08 - val_loss: 7.2488e-08 - learning_rate: 2.7777e-05\n",
      "Epoch 263/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8967e-08 - val_loss: 6.7610e-08 - learning_rate: 2.7777e-05\n",
      "Epoch 264/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8675e-08 - val_loss: 6.8967e-08 - learning_rate: 2.7777e-05\n",
      "Epoch 265/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8627e-08 - val_loss: 6.7492e-08 - learning_rate: 2.7777e-05\n",
      "Epoch 266/500\n",
      "\u001b[1m128/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7808e-08\n",
      "Epoch 266: ReduceLROnPlateau reducing learning rate to 2.6388280912215122e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7813e-08 - val_loss: 6.7948e-08 - learning_rate: 2.7777e-05\n",
      "Epoch 267/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8437e-08 - val_loss: 6.8553e-08 - learning_rate: 2.6388e-05\n",
      "Epoch 268/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8224e-08 - val_loss: 6.8255e-08 - learning_rate: 2.6388e-05\n",
      "Epoch 269/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8234e-08 - val_loss: 6.7831e-08 - learning_rate: 2.6388e-05\n",
      "Epoch 270/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8386e-08 - val_loss: 6.8349e-08 - learning_rate: 2.6388e-05\n",
      "Epoch 271/500\n",
      "\u001b[1m124/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8014e-08\n",
      "Epoch 271: ReduceLROnPlateau reducing learning rate to 2.5068867125810355e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8024e-08 - val_loss: 6.8056e-08 - learning_rate: 2.6388e-05\n",
      "Epoch 272/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8821e-08 - val_loss: 6.8611e-08 - learning_rate: 2.5069e-05\n",
      "Epoch 273/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8296e-08 - val_loss: 6.8901e-08 - learning_rate: 2.5069e-05\n",
      "Epoch 274/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8067e-08 - val_loss: 6.8530e-08 - learning_rate: 2.5069e-05\n",
      "Epoch 275/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7935e-08 - val_loss: 6.9686e-08 - learning_rate: 2.5069e-05\n",
      "Epoch 276/500\n",
      "\u001b[1m 96/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8837e-08\n",
      "Epoch 276: ReduceLROnPlateau reducing learning rate to 2.381542299190187e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8662e-08 - val_loss: 6.7697e-08 - learning_rate: 2.5069e-05\n",
      "Epoch 277/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7691e-08 - val_loss: 6.8199e-08 - learning_rate: 2.3815e-05\n",
      "Epoch 278/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7934e-08 - val_loss: 6.8171e-08 - learning_rate: 2.3815e-05\n",
      "Epoch 279/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7841e-08 - val_loss: 6.9438e-08 - learning_rate: 2.3815e-05\n",
      "Epoch 280/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8744e-08 - val_loss: 6.8341e-08 - learning_rate: 2.3815e-05\n",
      "Epoch 281/500\n",
      "\u001b[1m125/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7970e-08\n",
      "Epoch 281: ReduceLROnPlateau reducing learning rate to 2.262465141029679e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7983e-08 - val_loss: 7.2190e-08 - learning_rate: 2.3815e-05\n",
      "Epoch 282/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8332e-08 - val_loss: 6.7904e-08 - learning_rate: 2.2625e-05\n",
      "Epoch 283/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8259e-08 - val_loss: 6.7840e-08 - learning_rate: 2.2625e-05\n",
      "Epoch 284/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8004e-08 - val_loss: 6.9121e-08 - learning_rate: 2.2625e-05\n",
      "Epoch 285/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8406e-08 - val_loss: 6.8586e-08 - learning_rate: 2.2625e-05\n",
      "Epoch 286/500\n",
      "\u001b[1m 98/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7825e-08\n",
      "Epoch 286: ReduceLROnPlateau reducing learning rate to 2.1493419444595927e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7880e-08 - val_loss: 6.8461e-08 - learning_rate: 2.2625e-05\n",
      "Epoch 287/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8093e-08 - val_loss: 6.9198e-08 - learning_rate: 2.1493e-05\n",
      "Epoch 288/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8522e-08 - val_loss: 6.8629e-08 - learning_rate: 2.1493e-05\n",
      "Epoch 289/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8318e-08 - val_loss: 6.8388e-08 - learning_rate: 2.1493e-05\n",
      "Epoch 290/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8079e-08 - val_loss: 6.7585e-08 - learning_rate: 2.1493e-05\n",
      "Epoch 291/500\n",
      "\u001b[1m120/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7532e-08\n",
      "Epoch 291: ReduceLROnPlateau reducing learning rate to 2.0418747953954153e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7582e-08 - val_loss: 6.8240e-08 - learning_rate: 2.1493e-05\n",
      "Epoch 292/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8043e-08 - val_loss: 7.0932e-08 - learning_rate: 2.0419e-05\n",
      "Epoch 293/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8811e-08 - val_loss: 6.7816e-08 - learning_rate: 2.0419e-05\n",
      "Epoch 294/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8246e-08 - val_loss: 6.8076e-08 - learning_rate: 2.0419e-05\n",
      "Epoch 295/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7817e-08 - val_loss: 6.7366e-08 - learning_rate: 2.0419e-05\n",
      "Epoch 296/500\n",
      "\u001b[1m122/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8129e-08\n",
      "Epoch 296: ReduceLROnPlateau reducing learning rate to 1.9397809865040472e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8118e-08 - val_loss: 6.8041e-08 - learning_rate: 2.0419e-05\n",
      "Epoch 297/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8602e-08 - val_loss: 6.7849e-08 - learning_rate: 1.9398e-05\n",
      "Epoch 298/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7724e-08 - val_loss: 6.7754e-08 - learning_rate: 1.9398e-05\n",
      "Epoch 299/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8438e-08 - val_loss: 6.7464e-08 - learning_rate: 1.9398e-05\n",
      "Epoch 300/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8222e-08 - val_loss: 6.7842e-08 - learning_rate: 1.9398e-05\n",
      "Epoch 301/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8163e-08\n",
      "Epoch 301: ReduceLROnPlateau reducing learning rate to 1.842791980379843e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8163e-08 - val_loss: 6.7340e-08 - learning_rate: 1.9398e-05\n",
      "Epoch 302/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7806e-08 - val_loss: 6.9813e-08 - learning_rate: 1.8428e-05\n",
      "Epoch 303/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8071e-08 - val_loss: 6.8298e-08 - learning_rate: 1.8428e-05\n",
      "Epoch 304/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8032e-08 - val_loss: 6.9618e-08 - learning_rate: 1.8428e-05\n",
      "Epoch 305/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7903e-08 - val_loss: 6.9380e-08 - learning_rate: 1.8428e-05\n",
      "Epoch 306/500\n",
      "\u001b[1m122/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8244e-08\n",
      "Epoch 306: ReduceLROnPlateau reducing learning rate to 1.7506523727206512e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8232e-08 - val_loss: 6.8405e-08 - learning_rate: 1.8428e-05\n",
      "Epoch 307/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8278e-08 - val_loss: 6.7406e-08 - learning_rate: 1.7507e-05\n",
      "Epoch 308/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8156e-08 - val_loss: 6.9063e-08 - learning_rate: 1.7507e-05\n",
      "Epoch 309/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8069e-08 - val_loss: 6.7106e-08 - learning_rate: 1.7507e-05\n",
      "Epoch 310/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7943e-08 - val_loss: 6.9608e-08 - learning_rate: 1.7507e-05\n",
      "Epoch 311/500\n",
      "\u001b[1m113/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8041e-08\n",
      "Epoch 311: ReduceLROnPlateau reducing learning rate to 1.66311971952382e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8037e-08 - val_loss: 6.7686e-08 - learning_rate: 1.7507e-05\n",
      "Epoch 312/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7612e-08 - val_loss: 6.9053e-08 - learning_rate: 1.6631e-05\n",
      "Epoch 313/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8045e-08 - val_loss: 6.7536e-08 - learning_rate: 1.6631e-05\n",
      "Epoch 314/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7339e-08 - val_loss: 6.7161e-08 - learning_rate: 1.6631e-05\n",
      "Epoch 315/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7553e-08 - val_loss: 6.8081e-08 - learning_rate: 1.6631e-05\n",
      "Epoch 316/500\n",
      "\u001b[1m120/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7395e-08\n",
      "Epoch 316: ReduceLROnPlateau reducing learning rate to 1.5799636730662315e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7425e-08 - val_loss: 6.7517e-08 - learning_rate: 1.6631e-05\n",
      "Epoch 317/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7498e-08 - val_loss: 6.9599e-08 - learning_rate: 1.5800e-05\n",
      "Epoch 318/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8080e-08 - val_loss: 6.7299e-08 - learning_rate: 1.5800e-05\n",
      "Epoch 319/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7653e-08 - val_loss: 6.8073e-08 - learning_rate: 1.5800e-05\n",
      "Epoch 320/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7618e-08 - val_loss: 6.8646e-08 - learning_rate: 1.5800e-05\n",
      "Epoch 321/500\n",
      "\u001b[1m125/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8171e-08\n",
      "Epoch 321: ReduceLROnPlateau reducing learning rate to 1.5009654634923207e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8159e-08 - val_loss: 6.9822e-08 - learning_rate: 1.5800e-05\n",
      "Epoch 322/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7749e-08 - val_loss: 6.7310e-08 - learning_rate: 1.5010e-05\n",
      "Epoch 323/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8193e-08 - val_loss: 6.7150e-08 - learning_rate: 1.5010e-05\n",
      "Epoch 324/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7761e-08 - val_loss: 6.9200e-08 - learning_rate: 1.5010e-05\n",
      "Epoch 325/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7877e-08 - val_loss: 6.7496e-08 - learning_rate: 1.5010e-05\n",
      "Epoch 326/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7526e-08\n",
      "Epoch 326: ReduceLROnPlateau reducing learning rate to 1.4259172075981041e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7528e-08 - val_loss: 7.0159e-08 - learning_rate: 1.5010e-05\n",
      "Epoch 327/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7901e-08 - val_loss: 6.7780e-08 - learning_rate: 1.4259e-05\n",
      "Epoch 328/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7846e-08 - val_loss: 6.7398e-08 - learning_rate: 1.4259e-05\n",
      "Epoch 329/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7520e-08 - val_loss: 6.7976e-08 - learning_rate: 1.4259e-05\n",
      "Epoch 330/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8270e-08 - val_loss: 6.7915e-08 - learning_rate: 1.4259e-05\n",
      "Epoch 331/500\n",
      "\u001b[1m 99/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7325e-08\n",
      "Epoch 331: ReduceLROnPlateau reducing learning rate to 1.3546213904191972e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7410e-08 - val_loss: 6.8037e-08 - learning_rate: 1.4259e-05\n",
      "Epoch 332/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7561e-08 - val_loss: 6.7162e-08 - learning_rate: 1.3546e-05\n",
      "Epoch 333/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8016e-08 - val_loss: 6.7487e-08 - learning_rate: 1.3546e-05\n",
      "Epoch 334/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7782e-08 - val_loss: 6.7278e-08 - learning_rate: 1.3546e-05\n",
      "Epoch 335/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7635e-08 - val_loss: 6.7702e-08 - learning_rate: 1.3546e-05\n",
      "Epoch 336/500\n",
      "\u001b[1m 99/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7288e-08\n",
      "Epoch 336: ReduceLROnPlateau reducing learning rate to 1.2868903468188364e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7405e-08 - val_loss: 6.7368e-08 - learning_rate: 1.3546e-05\n",
      "Epoch 337/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7564e-08 - val_loss: 6.7269e-08 - learning_rate: 1.2869e-05\n",
      "Epoch 338/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7295e-08 - val_loss: 6.8077e-08 - learning_rate: 1.2869e-05\n",
      "Epoch 339/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7618e-08 - val_loss: 6.7234e-08 - learning_rate: 1.2869e-05\n",
      "Epoch 340/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7601e-08 - val_loss: 6.7132e-08 - learning_rate: 1.2869e-05\n",
      "Epoch 341/500\n",
      "\u001b[1m 97/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7627e-08\n",
      "Epoch 341: ReduceLROnPlateau reducing learning rate to 1.2225458294778945e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7630e-08 - val_loss: 6.8470e-08 - learning_rate: 1.2869e-05\n",
      "Epoch 342/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8091e-08 - val_loss: 6.7515e-08 - learning_rate: 1.2225e-05\n",
      "Epoch 343/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8034e-08 - val_loss: 6.8064e-08 - learning_rate: 1.2225e-05\n",
      "Epoch 344/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7837e-08 - val_loss: 6.8154e-08 - learning_rate: 1.2225e-05\n",
      "Epoch 345/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7592e-08 - val_loss: 6.7050e-08 - learning_rate: 1.2225e-05\n",
      "Epoch 346/500\n",
      "\u001b[1m123/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8154e-08\n",
      "Epoch 346: ReduceLROnPlateau reducing learning rate to 1.1614185768848983e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8136e-08 - val_loss: 6.7079e-08 - learning_rate: 1.2225e-05\n",
      "Epoch 347/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7095e-08 - val_loss: 6.7053e-08 - learning_rate: 1.1614e-05\n",
      "Epoch 348/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7410e-08 - val_loss: 6.8769e-08 - learning_rate: 1.1614e-05\n",
      "Epoch 349/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7824e-08 - val_loss: 6.7808e-08 - learning_rate: 1.1614e-05\n",
      "Epoch 350/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8208e-08 - val_loss: 6.9536e-08 - learning_rate: 1.1614e-05\n",
      "Epoch 351/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7828e-08\n",
      "Epoch 351: ReduceLROnPlateau reducing learning rate to 1.1033476221200543e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7827e-08 - val_loss: 6.8402e-08 - learning_rate: 1.1614e-05\n",
      "Epoch 352/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7681e-08 - val_loss: 6.7608e-08 - learning_rate: 1.1033e-05\n",
      "Epoch 353/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7430e-08 - val_loss: 6.7226e-08 - learning_rate: 1.1033e-05\n",
      "Epoch 354/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7348e-08 - val_loss: 6.7170e-08 - learning_rate: 1.1033e-05\n",
      "Epoch 355/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7646e-08 - val_loss: 6.7367e-08 - learning_rate: 1.1033e-05\n",
      "Epoch 356/500\n",
      "\u001b[1m124/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7326e-08\n",
      "Epoch 356: ReduceLROnPlateau reducing learning rate to 1.048180206453253e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7330e-08 - val_loss: 6.7380e-08 - learning_rate: 1.1033e-05\n",
      "Epoch 357/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7203e-08 - val_loss: 6.7033e-08 - learning_rate: 1.0482e-05\n",
      "Epoch 358/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7637e-08 - val_loss: 6.7055e-08 - learning_rate: 1.0482e-05\n",
      "Epoch 359/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7725e-08 - val_loss: 6.8908e-08 - learning_rate: 1.0482e-05\n",
      "Epoch 360/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8015e-08 - val_loss: 6.7438e-08 - learning_rate: 1.0482e-05\n",
      "Epoch 361/500\n",
      "\u001b[1m104/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.7081e-08\n",
      "Epoch 361: ReduceLROnPlateau reducing learning rate to 9.957711745300912e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7132e-08 - val_loss: 6.7163e-08 - learning_rate: 1.0482e-05\n",
      "Epoch 362/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7636e-08 - val_loss: 6.7546e-08 - learning_rate: 9.9577e-06\n",
      "Epoch 363/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7262e-08 - val_loss: 6.7036e-08 - learning_rate: 9.9577e-06\n",
      "Epoch 364/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7580e-08 - val_loss: 6.7444e-08 - learning_rate: 9.9577e-06\n",
      "Epoch 365/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7106e-08 - val_loss: 6.8423e-08 - learning_rate: 9.9577e-06\n",
      "Epoch 366/500\n",
      "\u001b[1m 98/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7148e-08\n",
      "Epoch 366: ReduceLROnPlateau reducing learning rate to 9.459826287638862e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7260e-08 - val_loss: 6.7103e-08 - learning_rate: 9.9577e-06\n",
      "Epoch 367/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7382e-08 - val_loss: 6.7901e-08 - learning_rate: 9.4598e-06\n",
      "Epoch 368/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7485e-08 - val_loss: 6.8563e-08 - learning_rate: 9.4598e-06\n",
      "Epoch 369/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8066e-08 - val_loss: 6.8127e-08 - learning_rate: 9.4598e-06\n",
      "Epoch 370/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7509e-08 - val_loss: 6.7096e-08 - learning_rate: 9.4598e-06\n",
      "Epoch 371/500\n",
      "\u001b[1m 95/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7794e-08\n",
      "Epoch 371: ReduceLROnPlateau reducing learning rate to 8.986834973256918e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7711e-08 - val_loss: 6.7413e-08 - learning_rate: 9.4598e-06\n",
      "Epoch 372/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7283e-08 - val_loss: 6.6999e-08 - learning_rate: 8.9868e-06\n",
      "Epoch 373/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7262e-08 - val_loss: 6.7271e-08 - learning_rate: 8.9868e-06\n",
      "Epoch 374/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7651e-08 - val_loss: 6.7989e-08 - learning_rate: 8.9868e-06\n",
      "Epoch 375/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7661e-08 - val_loss: 6.7219e-08 - learning_rate: 8.9868e-06\n",
      "Epoch 376/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7242e-08\n",
      "Epoch 376: ReduceLROnPlateau reducing learning rate to 8.537493613403058e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7243e-08 - val_loss: 6.7315e-08 - learning_rate: 8.9868e-06\n",
      "Epoch 377/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7551e-08 - val_loss: 6.7522e-08 - learning_rate: 8.5375e-06\n",
      "Epoch 378/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7568e-08 - val_loss: 6.7041e-08 - learning_rate: 8.5375e-06\n",
      "Epoch 379/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7323e-08 - val_loss: 6.7135e-08 - learning_rate: 8.5375e-06\n",
      "Epoch 380/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7073e-08 - val_loss: 6.7673e-08 - learning_rate: 8.5375e-06\n",
      "Epoch 381/500\n",
      "\u001b[1m121/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7280e-08\n",
      "Epoch 381: ReduceLROnPlateau reducing learning rate to 8.110619364742887e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7287e-08 - val_loss: 6.7239e-08 - learning_rate: 8.5375e-06\n",
      "Epoch 382/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7581e-08 - val_loss: 6.7138e-08 - learning_rate: 8.1106e-06\n",
      "Epoch 383/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7432e-08 - val_loss: 6.7335e-08 - learning_rate: 8.1106e-06\n",
      "Epoch 384/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7599e-08 - val_loss: 6.7191e-08 - learning_rate: 8.1106e-06\n",
      "Epoch 385/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7693e-08 - val_loss: 6.7274e-08 - learning_rate: 8.1106e-06\n",
      "Epoch 386/500\n",
      "\u001b[1m 99/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7023e-08\n",
      "Epoch 386: ReduceLROnPlateau reducing learning rate to 7.705088137299754e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7118e-08 - val_loss: 6.7326e-08 - learning_rate: 8.1106e-06\n",
      "Epoch 387/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7612e-08 - val_loss: 6.7033e-08 - learning_rate: 7.7051e-06\n",
      "Epoch 388/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6985e-08 - val_loss: 6.7613e-08 - learning_rate: 7.7051e-06\n",
      "Epoch 389/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7068e-08 - val_loss: 6.7075e-08 - learning_rate: 7.7051e-06\n",
      "Epoch 390/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7605e-08 - val_loss: 6.7313e-08 - learning_rate: 7.7051e-06\n",
      "Epoch 391/500\n",
      "\u001b[1m127/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6998e-08\n",
      "Epoch 391: ReduceLROnPlateau reducing learning rate to 7.319833730434766e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7006e-08 - val_loss: 6.7392e-08 - learning_rate: 7.7051e-06\n",
      "Epoch 392/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7326e-08 - val_loss: 6.7389e-08 - learning_rate: 7.3198e-06\n",
      "Epoch 393/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7486e-08 - val_loss: 6.7060e-08 - learning_rate: 7.3198e-06\n",
      "Epoch 394/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6916e-08 - val_loss: 6.7395e-08 - learning_rate: 7.3198e-06\n",
      "Epoch 395/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7330e-08 - val_loss: 6.7167e-08 - learning_rate: 7.3198e-06\n",
      "Epoch 396/500\n",
      "\u001b[1m101/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7454e-08\n",
      "Epoch 396: ReduceLROnPlateau reducing learning rate to 6.953842216717021e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7453e-08 - val_loss: 6.7070e-08 - learning_rate: 7.3198e-06\n",
      "Epoch 397/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7400e-08 - val_loss: 6.7150e-08 - learning_rate: 6.9538e-06\n",
      "Epoch 398/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7346e-08 - val_loss: 6.7039e-08 - learning_rate: 6.9538e-06\n",
      "Epoch 399/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7190e-08 - val_loss: 6.7281e-08 - learning_rate: 6.9538e-06\n",
      "Epoch 400/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7398e-08 - val_loss: 6.7732e-08 - learning_rate: 6.9538e-06\n",
      "Epoch 401/500\n",
      "\u001b[1m125/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7497e-08\n",
      "Epoch 401: ReduceLROnPlateau reducing learning rate to 6.6061502138836655e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7495e-08 - val_loss: 6.7037e-08 - learning_rate: 6.9538e-06\n",
      "Epoch 402/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7041e-08 - val_loss: 6.7043e-08 - learning_rate: 6.6062e-06\n",
      "Epoch 403/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7504e-08 - val_loss: 6.7135e-08 - learning_rate: 6.6062e-06\n",
      "Epoch 404/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7123e-08 - val_loss: 6.7222e-08 - learning_rate: 6.6062e-06\n",
      "Epoch 405/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7328e-08 - val_loss: 6.6973e-08 - learning_rate: 6.6062e-06\n",
      "Epoch 406/500\n",
      "\u001b[1m104/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.7320e-08\n",
      "Epoch 406: ReduceLROnPlateau reducing learning rate to 6.2758427247899816e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7320e-08 - val_loss: 6.6962e-08 - learning_rate: 6.6062e-06\n",
      "Epoch 407/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7357e-08 - val_loss: 6.7210e-08 - learning_rate: 6.2758e-06\n",
      "Epoch 408/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7835e-08 - val_loss: 6.7198e-08 - learning_rate: 6.2758e-06\n",
      "Epoch 409/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7340e-08 - val_loss: 6.7156e-08 - learning_rate: 6.2758e-06\n",
      "Epoch 410/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6977e-08 - val_loss: 6.7107e-08 - learning_rate: 6.2758e-06\n",
      "Epoch 411/500\n",
      "\u001b[1m127/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7432e-08\n",
      "Epoch 411: ReduceLROnPlateau reducing learning rate to 5.962050545349484e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7428e-08 - val_loss: 6.7353e-08 - learning_rate: 6.2758e-06\n",
      "Epoch 412/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7290e-08 - val_loss: 6.7019e-08 - learning_rate: 5.9621e-06\n",
      "Epoch 413/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7132e-08 - val_loss: 6.7370e-08 - learning_rate: 5.9621e-06\n",
      "Epoch 414/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7173e-08 - val_loss: 6.7399e-08 - learning_rate: 5.9621e-06\n",
      "Epoch 415/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6757e-08 - val_loss: 6.7043e-08 - learning_rate: 5.9621e-06\n",
      "Epoch 416/500\n",
      "\u001b[1m103/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.7288e-08\n",
      "Epoch 416: ReduceLROnPlateau reducing learning rate to 5.663948104484007e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7287e-08 - val_loss: 6.7089e-08 - learning_rate: 5.9621e-06\n",
      "Epoch 417/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7358e-08 - val_loss: 6.7178e-08 - learning_rate: 5.6639e-06\n",
      "Epoch 418/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7383e-08 - val_loss: 6.7094e-08 - learning_rate: 5.6639e-06\n",
      "Epoch 419/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6937e-08 - val_loss: 6.7080e-08 - learning_rate: 5.6639e-06\n",
      "Epoch 420/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7518e-08 - val_loss: 6.7069e-08 - learning_rate: 5.6639e-06\n",
      "Epoch 421/500\n",
      "\u001b[1m 97/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7148e-08\n",
      "Epoch 421: ReduceLROnPlateau reducing learning rate to 5.380750872063799e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7165e-08 - val_loss: 6.7461e-08 - learning_rate: 5.6639e-06\n",
      "Epoch 422/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7283e-08 - val_loss: 6.7295e-08 - learning_rate: 5.3808e-06\n",
      "Epoch 423/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7256e-08 - val_loss: 6.7203e-08 - learning_rate: 5.3808e-06\n",
      "Epoch 424/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7705e-08 - val_loss: 6.7449e-08 - learning_rate: 5.3808e-06\n",
      "Epoch 425/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7680e-08 - val_loss: 6.7038e-08 - learning_rate: 5.3808e-06\n",
      "Epoch 426/500\n",
      "\u001b[1m101/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7398e-08\n",
      "Epoch 426: ReduceLROnPlateau reducing learning rate to 5.1117131988576144e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7380e-08 - val_loss: 6.7304e-08 - learning_rate: 5.3808e-06\n",
      "Epoch 427/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7398e-08 - val_loss: 6.6942e-08 - learning_rate: 5.1117e-06\n",
      "Epoch 428/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6993e-08 - val_loss: 6.6988e-08 - learning_rate: 5.1117e-06\n",
      "Epoch 429/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7264e-08 - val_loss: 6.7024e-08 - learning_rate: 5.1117e-06\n",
      "Epoch 430/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6902e-08 - val_loss: 6.7005e-08 - learning_rate: 5.1117e-06\n",
      "Epoch 431/500\n",
      "\u001b[1m119/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7240e-08\n",
      "Epoch 431: ReduceLROnPlateau reducing learning rate to 4.856127452512737e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7237e-08 - val_loss: 6.7654e-08 - learning_rate: 5.1117e-06\n",
      "Epoch 432/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7310e-08 - val_loss: 6.6916e-08 - learning_rate: 4.8561e-06\n",
      "Epoch 433/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7065e-08 - val_loss: 6.7248e-08 - learning_rate: 4.8561e-06\n",
      "Epoch 434/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7296e-08 - val_loss: 6.7479e-08 - learning_rate: 4.8561e-06\n",
      "Epoch 435/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7089e-08 - val_loss: 6.7586e-08 - learning_rate: 4.8561e-06\n",
      "Epoch 436/500\n",
      "\u001b[1m105/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.7239e-08\n",
      "Epoch 436: ReduceLROnPlateau reducing learning rate to 4.613320993485104e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7231e-08 - val_loss: 6.7230e-08 - learning_rate: 4.8561e-06\n",
      "Epoch 437/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7434e-08 - val_loss: 6.7108e-08 - learning_rate: 4.6133e-06\n",
      "Epoch 438/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7153e-08 - val_loss: 6.7052e-08 - learning_rate: 4.6133e-06\n",
      "Epoch 439/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7279e-08 - val_loss: 6.6957e-08 - learning_rate: 4.6133e-06\n",
      "Epoch 440/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6849e-08 - val_loss: 6.6968e-08 - learning_rate: 4.6133e-06\n",
      "Epoch 441/500\n",
      "\u001b[1m100/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7298e-08\n",
      "Epoch 441: ReduceLROnPlateau reducing learning rate to 4.382654879009351e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7269e-08 - val_loss: 6.7584e-08 - learning_rate: 4.6133e-06\n",
      "Epoch 442/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7512e-08 - val_loss: 6.6916e-08 - learning_rate: 4.3827e-06\n",
      "Epoch 443/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7126e-08 - val_loss: 6.7121e-08 - learning_rate: 4.3827e-06\n",
      "Epoch 444/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7080e-08 - val_loss: 6.7279e-08 - learning_rate: 4.3827e-06\n",
      "Epoch 445/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7216e-08 - val_loss: 6.6927e-08 - learning_rate: 4.3827e-06\n",
      "Epoch 446/500\n",
      "\u001b[1m114/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7351e-08\n",
      "Epoch 446: ReduceLROnPlateau reducing learning rate to 4.163522135058884e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7330e-08 - val_loss: 6.7550e-08 - learning_rate: 4.3827e-06\n",
      "Epoch 447/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7423e-08 - val_loss: 6.6977e-08 - learning_rate: 4.1635e-06\n",
      "Epoch 448/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6977e-08 - val_loss: 6.7266e-08 - learning_rate: 4.1635e-06\n",
      "Epoch 449/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7052e-08 - val_loss: 6.7312e-08 - learning_rate: 4.1635e-06\n",
      "Epoch 450/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7305e-08 - val_loss: 6.7045e-08 - learning_rate: 4.1635e-06\n",
      "Epoch 451/500\n",
      "\u001b[1m104/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.7175e-08\n",
      "Epoch 451: ReduceLROnPlateau reducing learning rate to 3.9553460283059396e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7196e-08 - val_loss: 6.7376e-08 - learning_rate: 4.1635e-06\n",
      "Epoch 452/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7330e-08 - val_loss: 6.6953e-08 - learning_rate: 3.9553e-06\n",
      "Epoch 453/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7348e-08 - val_loss: 6.6994e-08 - learning_rate: 3.9553e-06\n",
      "Epoch 454/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7446e-08 - val_loss: 6.7115e-08 - learning_rate: 3.9553e-06\n",
      "Epoch 455/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7176e-08 - val_loss: 6.7001e-08 - learning_rate: 3.9553e-06\n",
      "Epoch 456/500\n",
      "\u001b[1m106/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.7171e-08\n",
      "Epoch 456: ReduceLROnPlateau reducing learning rate to 3.7575787700916406e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7172e-08 - val_loss: 6.7006e-08 - learning_rate: 3.9553e-06\n",
      "Epoch 457/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7330e-08 - val_loss: 6.7349e-08 - learning_rate: 3.7576e-06\n",
      "Epoch 458/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7170e-08 - val_loss: 6.6944e-08 - learning_rate: 3.7576e-06\n",
      "Epoch 459/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7189e-08 - val_loss: 6.7412e-08 - learning_rate: 3.7576e-06\n",
      "Epoch 460/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7061e-08 - val_loss: 6.7044e-08 - learning_rate: 3.7576e-06\n",
      "Epoch 461/500\n",
      "\u001b[1m119/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7176e-08\n",
      "Epoch 461: ReduceLROnPlateau reducing learning rate to 3.5696997883860602e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7172e-08 - val_loss: 6.6913e-08 - learning_rate: 3.7576e-06\n",
      "Epoch 462/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6992e-08 - val_loss: 6.7126e-08 - learning_rate: 3.5697e-06\n",
      "Epoch 463/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7380e-08 - val_loss: 6.6922e-08 - learning_rate: 3.5697e-06\n",
      "Epoch 464/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7200e-08 - val_loss: 6.6917e-08 - learning_rate: 3.5697e-06\n",
      "Epoch 465/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6917e-08 - val_loss: 6.7144e-08 - learning_rate: 3.5697e-06\n",
      "Epoch 466/500\n",
      "\u001b[1m123/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7233e-08\n",
      "Epoch 466: ReduceLROnPlateau reducing learning rate to 3.3912148637682547e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7228e-08 - val_loss: 6.7428e-08 - learning_rate: 3.5697e-06\n",
      "Epoch 467/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7124e-08 - val_loss: 6.7033e-08 - learning_rate: 3.3912e-06\n",
      "Epoch 468/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7133e-08 - val_loss: 6.6916e-08 - learning_rate: 3.3912e-06\n",
      "Epoch 469/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7284e-08 - val_loss: 6.6887e-08 - learning_rate: 3.3912e-06\n",
      "Epoch 470/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7543e-08 - val_loss: 6.6994e-08 - learning_rate: 3.3912e-06\n",
      "Epoch 471/500\n",
      "\u001b[1m101/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7299e-08\n",
      "Epoch 471: ReduceLROnPlateau reducing learning rate to 3.2216541853813395e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7281e-08 - val_loss: 6.6918e-08 - learning_rate: 3.3912e-06\n",
      "Epoch 472/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7010e-08 - val_loss: 6.7136e-08 - learning_rate: 3.2217e-06\n",
      "Epoch 473/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7064e-08 - val_loss: 6.7038e-08 - learning_rate: 3.2217e-06\n",
      "Epoch 474/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7128e-08 - val_loss: 6.6987e-08 - learning_rate: 3.2217e-06\n",
      "Epoch 475/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7243e-08 - val_loss: 6.7080e-08 - learning_rate: 3.2217e-06\n",
      "Epoch 476/500\n",
      "\u001b[1m115/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7210e-08\n",
      "Epoch 476: ReduceLROnPlateau reducing learning rate to 3.0605714869125222e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7203e-08 - val_loss: 6.7049e-08 - learning_rate: 3.2217e-06\n",
      "Epoch 477/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7184e-08 - val_loss: 6.7411e-08 - learning_rate: 3.0606e-06\n",
      "Epoch 478/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7102e-08 - val_loss: 6.6900e-08 - learning_rate: 3.0606e-06\n",
      "Epoch 479/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7078e-08 - val_loss: 6.7257e-08 - learning_rate: 3.0606e-06\n",
      "Epoch 480/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7050e-08 - val_loss: 6.7096e-08 - learning_rate: 3.0606e-06\n",
      "Epoch 481/500\n",
      "\u001b[1m100/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6764e-08\n",
      "Epoch 481: ReduceLROnPlateau reducing learning rate to 2.907542966568144e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6865e-08 - val_loss: 6.6889e-08 - learning_rate: 3.0606e-06\n",
      "Epoch 482/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7299e-08 - val_loss: 6.6973e-08 - learning_rate: 2.9075e-06\n",
      "Epoch 483/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7581e-08 - val_loss: 6.6949e-08 - learning_rate: 2.9075e-06\n",
      "Epoch 484/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6995e-08 - val_loss: 6.7198e-08 - learning_rate: 2.9075e-06\n",
      "Epoch 485/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6866e-08 - val_loss: 6.6930e-08 - learning_rate: 2.9075e-06\n",
      "Epoch 486/500\n",
      "\u001b[1m102/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.6809e-08\n",
      "Epoch 486: ReduceLROnPlateau reducing learning rate to 2.7621657750387382e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6893e-08 - val_loss: 6.6873e-08 - learning_rate: 2.9075e-06\n",
      "Epoch 487/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7435e-08 - val_loss: 6.7022e-08 - learning_rate: 2.7622e-06\n",
      "Epoch 488/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7177e-08 - val_loss: 6.6968e-08 - learning_rate: 2.7622e-06\n",
      "Epoch 489/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7023e-08 - val_loss: 6.6922e-08 - learning_rate: 2.7622e-06\n",
      "Epoch 490/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7300e-08 - val_loss: 6.6936e-08 - learning_rate: 2.7622e-06\n",
      "Epoch 491/500\n",
      "\u001b[1m104/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.7294e-08\n",
      "Epoch 491: ReduceLROnPlateau reducing learning rate to 2.6240575834890476e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7260e-08 - val_loss: 6.6901e-08 - learning_rate: 2.7622e-06\n",
      "Epoch 492/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7115e-08 - val_loss: 6.7328e-08 - learning_rate: 2.6241e-06\n",
      "Epoch 493/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7020e-08 - val_loss: 6.7052e-08 - learning_rate: 2.6241e-06\n",
      "Epoch 494/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6784e-08 - val_loss: 6.7019e-08 - learning_rate: 2.6241e-06\n",
      "Epoch 495/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7146e-08 - val_loss: 6.7007e-08 - learning_rate: 2.6241e-06\n",
      "Epoch 496/500\n",
      "\u001b[1m100/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6842e-08\n",
      "Epoch 496: ReduceLROnPlateau reducing learning rate to 2.492854639513098e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6915e-08 - val_loss: 6.6893e-08 - learning_rate: 2.6241e-06\n",
      "Epoch 497/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7301e-08 - val_loss: 6.6879e-08 - learning_rate: 2.4929e-06\n",
      "Epoch 498/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7244e-08 - val_loss: 6.6922e-08 - learning_rate: 2.4929e-06\n",
      "Epoch 499/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6887e-08 - val_loss: 6.6913e-08 - learning_rate: 2.4929e-06\n",
      "Epoch 500/500\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7106e-08 - val_loss: 6.6872e-08 - learning_rate: 2.4929e-06\n"
     ]
    }
   ],
   "source": [
    "# Define training parameters\n",
    "maxEpochs = 200\n",
    "miniBatchSize = 1024\n",
    "iterPerEpoch = len(X_train) // miniBatchSize\n",
    "validation_freq = 2 * iterPerEpoch\n",
    "\n",
    "\n",
    "# history = model.fit(X_train, (y_train.y_real, y_train.y_img), epochs = 200, batch_size = 10,\n",
    "#                     validation_data = (X_val, (y_val.y_real, y_val.y_img)))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, (y_train[:,0], y_train[:,1]),\n",
    "    epochs=500,\n",
    "    batch_size=miniBatchSize,\n",
    "    callbacks=[checkpoint_callback,reduce_lr],\n",
    "    validation_data = (X_val, (y_val[:,0], y_val[:,1])),\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = tf.reshape(model.predict(X_test),(131520,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helperNMSE(tf.convert_to_tensor(y_test,dtype=tf.float32),y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGsCAYAAACB/u5dAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqOElEQVR4nO3df3BUVZ738c/t7nQnIb/4lV8SEMYfiAgiCBNZZ1SiDEM5ozXr+FhsDcVOOasLW7q4U5qx1h/PszNhy9J1ZpZCx12Hmn1WcdwdHB9HUQcF1AWBKAqiCIoShSQgkp+kk3Sf54+kO7QETIfunOTk/arq6uT27Xu/fRLNh3POPdczxhgBAACkgM92AQAAwB0ECwAAkDIECwAAkDIECwAAkDIECwAAkDIECwAAkDIECwAAkDIECwAAkDIECwAAkDIECwAAkDLWgsWmTZt07bXXqrS0VJ7n6Zlnnkn7OT///HP91V/9lUaPHq2srCxddNFF2r59e9rPCwDAcGEtWLS0tGj69OlauXLlgJzvyy+/1Ny5c5WRkaEXXnhBu3fv1oMPPqiRI0cOyPkBABgOvMFwEzLP87R27Vpdd9118W3hcFh33323nnzySR07dkxTp07VP//zP+uKK67o1znuuusuvfHGG3rttddSUzQAADjJoJ1jsWzZMm3evFlr1qzRu+++qxtuuEHf+c53tHfv3n4d79lnn9WsWbN0ww03qLCwUDNmzNBjjz2W4qoBABjeBmWPxYEDBzRp0iQdOHBApaWl8f0qKio0e/Zs/eIXv0j6HJmZmZKk5cuX64YbbtC2bdt022236ZFHHtHixYtT8jkAABjuArYL6M3OnTsViUR03nnnJWwPh8MaPXq0JOmDDz7QBRdccNrj3HnnnVqxYoUkKRqNatasWfFQMmPGDO3atYtgAQBACg3KYNHc3Cy/36/q6mr5/f6E13JyciRJkyZN0vvvv3/a48RCiCSVlJRoypQpCa9fcMEF+u///u8UVQ0AAAZlsJgxY4YikYjq6+t1+eWX97pPMBjU5MmT+3zMuXPnas+ePQnbPvzwQ02YMOGMagUAAD2sBYvm5mbt27cv/v3+/fu1Y8cOjRo1Suedd54WLVqkH/3oR3rwwQc1Y8YMHT58WOvXr9e0adO0cOHCpM/393//97rsssv0i1/8Qj/84Q+1detW/eY3v9FvfvObVH4sAACGNWuTNzds2KArr7zypO2LFy/W6tWr1dHRoX/6p3/S7373O33++ecaM2aMvvnNb+r+++/XRRdd1K9zPvfcc6qsrNTevXs1ceJELV++XDfffPOZfhQAANBtUFwVAgAA3DBo17EAAABDD8ECAACkzIBP3oxGozp48KByc3Pled5Anx4AAPSDMUZNTU0qLS2Vz3fqfokBDxYHDx5UWVnZQJ8WAACkQE1NjcaNG3fK1wc8WOTm5krqKiwvL2+gTw8AAPqhsbFRZWVl8b/jpzLgwSI2/JGXl0ewAABgiPm6aQxM3gQAAClDsAAAAClDsAAAACkzKG9CBgDAYGOMUWdnpyKRiO1S0sLv9ysQCJzxUhAECwAAvkZ7e7sOHTqk1tZW26WkVXZ2tkpKShQMBvt9DIIFAACnEY1GtX//fvn9fpWWlioYDDq3wKMxRu3t7Tp8+LD279+vc88997SLYJ0OwQIAgNNob29XNBpVWVmZsrOzbZeTNllZWcrIyNCnn36q9vZ2ZWZm9us4TN4EAKAP+vsv+KEkFZ/R/VYCAAADhmABAABShmABAABShmABAIDDVq5cqbPPPluZmZmaM2eOtm7dmtbzORMsHnxpj+579j3VNrTZLgUAgEHhqaee0vLly3Xvvffqrbfe0vTp0zV//nzV19en7ZzOXG66ZluNDjeF9cNZZSrO798lMgAAfB1jjI532Fl9MyvDn9QaGg899JBuvvlmLVmyRJL0yCOP6E9/+pMef/xx3XXXXWmp0Zlg4dZSJQCAwep4R0RT7nnRyrl3/+/5yg727U93e3u7qqurVVlZGd/m8/lUUVGhzZs3p6tEd4ZCYoyM7RIAALDuyJEjikQiKioqStheVFSk2tratJ3XnR6L7i4LQ64AAKRRVoZfu//3fGvnHuzcCRYMhgAABoDneX0ejrBpzJgx8vv9qqurS9heV1en4uLitJ03qaGQ++67T57nJTwmT56crtqS4tj9YAAAOCPBYFAzZ87U+vXr49ui0ajWr1+v8vLytJ036ch14YUX6s9//nPPAQKDK7UxFAIAQJfly5dr8eLFmjVrlmbPnq2HH35YLS0t8atE0iHpVBAIBNLahdJfsQ4LJm8CANDlxhtv1OHDh3XPPfeotrZWF198sdatW3fShM5USjpY7N27V6WlpcrMzFR5ebmqqqo0fvz4U+4fDocVDofj3zc2Nvav0q8Ru66XHgsAAHosW7ZMy5YtG7DzJTXHYs6cOVq9erXWrVunVatWaf/+/br88svV1NR0yvdUVVUpPz8//igrKzvjogEAwOCUVLBYsGCBbrjhBk2bNk3z58/X888/r2PHjun3v//9Kd9TWVmphoaG+KOmpuaMiz4dOiwAALDnjGZeFhQU6LzzztO+fftOuU8oFFIoFDqT0/RJzzoWRAsAAGw5o5U3m5ub9dFHH6mkpCRV9fRbPFjYLQMAgGEtqWDxD//wD9q4caM++eQT/c///I+uv/56+f1+3XTTTemqr89YIAsAkE7DoUc8FZ8xqaGQzz77TDfddJO++OILjR07Vn/xF3+hLVu2aOzYsWdcSKoMg587AGAAZWRkSJJaW1uVlZVluZr0am1tldTzmfsjqWCxZs2afp8o3XpW3iRZAABSx+/3q6CgQPX19ZKk7OzspG5dPhQYY9Ta2qr6+noVFBTI7+//PUkG17KZZyC+QBa5AgCQYrGFIWPhwlUFBQVnvAimO8HCsfQIABg8PM9TSUmJCgsL1dHRYbuctMjIyDijnooYZ4JFDB0WAIB08fv9Kfnj67Izutx0MGEoBAAA+5wJFmKBLAAArHMmWDDDAgAA+5wJFjH0VwAAYI8zwYLbpgMAYJ87wcJ2AQAAwJ1gEWMYDAEAwBpngkV8fSxyBQAA1rgTLLoHQ8gVAADY406wYJIFAADWORMsYrgqBAAAe9wLFgyGAABgjTPBgnUsAACwz51gYbsAAADgTrCIocMCAAB7nAkWHnc3BQDAOveChd0yAAAY1twJFsyyAADAOmeCRRxdFgAAWONMsOgZCiFZAABgizvBovuZuZsAANjjTLDgZiEAANjnTrDoRo8FAAD2OBMs4kMhVqsAAGB4cydYsEAWAADWuRMsbBcAAADcCRYx9FcAAGCPM8GC26YDAGCfO8Ei/hXJAgAAW9wJFkyyAADAOmeCRQxDIQAA2ONMsIjd3ZRcAQCAPc4EC8XXsbBbBgAAw5kzwYIpFgAA2OdMsIjhtukAANjjTLDwGAoBAMA6d4IFkzcBALDOmWABAADscyZYcHdTAADscy5YAAAAe9wJFlxwCgCAdc4EixhGQgAAsMeZYBGfY8F1IQAAWONMsIihxwIAAHucCRYeszcBALDOmWARQ48FAAD2OBMsYv0V5AoAAOxxJ1iwQBYAANa5EyxsFwAAANwJFjH0VwAAYI8zwcLrWcgCAABY4k6w6H5mgSwAAOw5o2CxYsUKeZ6n22+/PUXl9B/LWAAAYF+/g8W2bdv06KOPatq0aams54xxUQgAAPb0K1g0Nzdr0aJFeuyxxzRy5MhU19RPXV0W5AoAAOzpV7BYunSpFi5cqIqKiq/dNxwOq7GxMeGRDj3rWKTl8AAAoA8Cyb5hzZo1euutt7Rt27Y+7V9VVaX7778/6cKSxRQLAADsS6rHoqamRrfddpv+8z//U5mZmX16T2VlpRoaGuKPmpqafhXaV1wVAgCAPUn1WFRXV6u+vl6XXHJJfFskEtGmTZv0r//6rwqHw/L7/QnvCYVCCoVCqan2NBgKAQDAvqSCxbx587Rz586EbUuWLNHkyZN15513nhQqBpLH5E0AAKxLKljk5uZq6tSpCdtGjBih0aNHn7R9oLGOBQAA9jmz8mYcYyEAAFiT9FUhX7Vhw4YUlHHmuFUIAAD2OdNjEZ9jQbIAAMAaZ4IFC1kAAGCfO8Gim6HLAgAAa5wJFj23TQcAALa4Eyw85lgAAGCbM8ECAADY50ywYCgEAAD73AkW8XuFEC0AALDFnWBhuwAAAOBOsAAAAPY5Eyy4KgQAAPvcCRbdz4bpmwAAWONMsGCSBQAA9rkTLLoxFAIAgD3OBIv43U0t1wEAwHDmTrCIr2Nhtw4AAIYzd4KF7QIAAIA7wSKGq0IAALDHmWDBUAgAAPa5EywYDAEAwDp3ggW5AgAA65wJFjHc3RQAAHucCRbMsQAAwD5ngoVYIAsAAOucCRbMsQAAwD5ngkUMQyEAANjjTLDgtukAANjnTrBg8iYAANa5EyxYIAsAAOucCRYxdFgAAGCPM8EiflUIYyEAAFjjTrDofiZWAABgjzvBgoUsAACwzplgEcNICAAA9rgXLBgMAQDAGmeCBetYAABgnzPBAgAA2OdMsPC4uykAANa5EywYCgEAwDp3gkX3M5M3AQCwx5lgAQAA7HMmWHgsvQkAgHUOBQsmbwIAYJs7wcJ2AQAAwJ1gEWO4LAQAAGvcCRZcbgoAgHXOBAsWyAIAwD53ggWTLAAAsM6ZYBHDUAgAAPY4EyxYeRMAAPvcCRZM3gQAwDp3ggUrWQAAYJ0zwQIAANjnTLDoGQphLAQAAFvcCRbdz8QKAADscSZYsJAFAAD2JRUsVq1apWnTpikvL095eXkqLy/XCy+8kK7a+oWREAAA7EkqWIwbN04rVqxQdXW1tm/frquuukrf//739d5776Wrvj5jHQsAAOwLJLPztddem/D9z3/+c61atUpbtmzRhRdemNLCksU6FgAA2JdUsDhRJBLR008/rZaWFpWXl59yv3A4rHA4HP++sbGxv6c8LdaxAADAvqQnb+7cuVM5OTkKhUK65ZZbtHbtWk2ZMuWU+1dVVSk/Pz/+KCsrO6OCvw4dFgAA2JN0sDj//PO1Y8cOvfnmm7r11lu1ePFi7d69+5T7V1ZWqqGhIf6oqak5o4JPhaEQAADsS3ooJBgM6pxzzpEkzZw5U9u2bdMvf/lLPfroo73uHwqFFAqFzqzKPugZCCFZAABgyxmvYxGNRhPmUNjCMhYAANiXVI9FZWWlFixYoPHjx6upqUlPPPGENmzYoBdffDFd9SWNoRAAAOxJKljU19frRz/6kQ4dOqT8/HxNmzZNL774oq6++up01ddnXneXBcECAAB7kgoW//7v/56uOlKGBbIAALDHmXuFMMcCAAD7nAkWMQyFAABgjzPBIrbyJrkCAAB73AkWLJAFAIB1zgQLAABgnzPBgtumAwBgnzvBoidZAAAAS9wJFtw2HQAA65wJFjF0WAAAYI8zwaLnqhCiBQAAtjgTLGKIFQAA2ONMsPBY0xsAAOucCRYxjIQAAGCPM8GCq00BALDPnWDB5E0AAKxzJ1jYLgAAALgTLGLorwAAwB5ngkX8qhCSBQAA1jgULLqeuQkZAAD2uBMsbBcAAADcCRYxXBQCAIA97gSL7rEQggUAAPY4Eyx6FsgiWQAAYIs7wYJJFgAAWOdMsIhhKAQAAHucCRZe92AIuQIAAHvcCRbxe4XYrQMAgOHMnWBhuwAAAOBOsOhBlwUAALY4EywYCgEAwD53ggWTNwEAsM6ZYMEkCwAA7HMnWHQzjIUAAGCNM8GiZ0lvAABgizvBgpuQAQBgnTPBAgAA2OdMsGAoBAAA+9wJFvF1LIgWAADY4lywAAAA9jgTLAAAgH3OBIv4ypuMhAAAYI07wSI2x4LpmwAAWONMsAAAAPY5FywYCgEAwB5nggUrbwIAYJ87waL7mTkWAADY406wYB0LAACscyZYxDAUAgCAPc4Ei/g6FpbrAABgOHMnWHAXMgAArHMnWNguAAAAuBMsYrgqBAAAe5wJFj23TbdbBwAAw5kzwUJM3gQAwDpnggXrWAAAYF9SwaKqqkqXXnqpcnNzVVhYqOuuu0579uxJV239YhgLAQDAmqSCxcaNG7V06VJt2bJFL7/8sjo6OnTNNdeopaUlXfX1GVebAgBgXyCZndetW5fw/erVq1VYWKjq6mp961vfSmlhyeImZAAA2JdUsPiqhoYGSdKoUaNOuU84HFY4HI5/39jYeCanPCWmWAAAYF+/J29Go1Hdfvvtmjt3rqZOnXrK/aqqqpSfnx9/lJWV9feUfUKHBQAA9vQ7WCxdulS7du3SmjVrTrtfZWWlGhoa4o+ampr+nvK0epb0JloAAGBLv4ZCli1bpueee06bNm3SuHHjTrtvKBRSKBTqV3HJiC+QlfYzAQCAU0kqWBhj9Hd/93dau3atNmzYoIkTJ6arrqR5zLIAAMC6pILF0qVL9cQTT+iPf/yjcnNzVVtbK0nKz89XVlZWWgpMFiMhAADYk9Qci1WrVqmhoUFXXHGFSkpK4o+nnnoqXfX1XXwohGQBAIAtSQ+FDFbM3QQAwD5n7hUCAADscyZYsPImAAD2uRMsup/JFQAA2ONOsIhN3qTLAgAAa5wJFgAAwD5nggULZAEAYJ87wSI+FGK3DgAAhjN3gkX3MwtkAQBgjzPBAgAA2OdOsGAoBAAA65wJFrHJm+QKAADscSdYcFEIAADWORMsYlggCwAAe5wJFizpDQCAfe4Ei/hCFnbrAABgOHMoWNiuAAAAOBMsYuiwAADAHmeCRXyOBZM3AQCwxp1gwRQLAACscyZYiLubAgBgnUPBogsjIQAA2ONMsOgZCiFZAABgizvBovuZHgsAAOxxJ1iwkAUAANY5Eyxi6LEAAMAeZ4IF/RUAANjnTrCITd6kywIAAGvcCRb0WQAAYJ0zwSKG/goAAOxxJlj0DIXYrQMAgOHMmWARwwJZAADY41ywAAAA9jgTLBgKAQDAPneCRfdVIeQKAADscSdY0GMBAIB1zgQLAABgnzPBouceZHRZAABgizvBIjbHglwBAIA17gSL2BwLu2UAADCsORMsAACAfc4Ei9gUC+5uCgCAPe4EC4ZCAACwzplgIW6bDgCAdQ4Fiy6MhAAAYI8zwaJn5U2SBQAAtrgTLLqfiRUAANjjTrDwmGMBAIBtzgSLOLosAACwxplgwVAIAAD2uRMsmLwJAIB17gQL1rEAAMA6Z4JFDP0VAADY40yw6BkKsVsHAADDmTPBIsbQZwEAgDXOBAuWsQAAwD5ngkUMQyEAANiTdLDYtGmTrr32WpWWlsrzPD3zzDNpKCt5sZU3yRUAANiTdLBoaWnR9OnTtXLlynTU02/xkRCSBQAA1gSSfcOCBQu0YMGCdNRyRphjAQCAfUkHi2SFw2GFw+H4942NjWk9H1eFAABgT9onb1ZVVSk/Pz/+KCsrS8t5YitvMnkTAAB70h4sKisr1dDQEH/U1NSk5TzxBbLScnQAANAXaR8KCYVCCoVC6T4NAAAYBJxZxyJ+23TGQgAAsCbpHovm5mbt27cv/v3+/fu1Y8cOjRo1SuPHj09pcUlhKAQAAOuSDhbbt2/XlVdeGf9++fLlkqTFixdr9erVKSssWUzeBADAvqSDxRVXXMFwAwAA6JU7cyxYIAsAAOvcCRYnfE2PCgAAdrgTLE7osiBXAABghzPBAgAA2OdMsEgYCrFWBQAAw5s7weKEZMEcCwAA7HAnWJzQZ0GsAADADmeCBQAAsM+dYJEwFGKvDAAAhjNngkXCHAsGQwAAsMKdYGG7AAAA4E6wOBFDIQAA2OFMsPC4WQgAANa5EyxO+JoeCwAA7HAnWNBhAQCAdc4EixNxVQgAAHY4EywSVt4kVwAAYIU7wSJhHQsAAGCDM8ECAADY52Sw4O6mAADY4UywYCgEAAD73AkWTN4EAMA6d4IF61gAAGCdM8EiAT0WAABY4UywSFjSm2QBAIAV7gQLjzkWAADY5k6wsF0AAABwJ1iciA4LAADscCZYJKxjwVgIAABWOBQsepJFlFwBAIAVzgQLSSrKC0mSdh9qtFwJAADDk1PB4qrJRZKkP++us1wJAADDk1PB4uophZKkNdsOqPrTo5arAQBg+HEqWFz2jTH6xtgR6ogY3fp/31KUyRYAAAwop4JFZoZff7h1roIBn+qbwnrns2O2SwIAYFhxKlhIUn52hq6+oGuuxasf1FuuBgCA4cW5YCFJV07ummuxnmABAMCAcjJYXHH+WHme9N7BRtU1ttkuBwCAYcPJYDEmJ6Rp4wokMRwCAMBAcjJYSNJV53cNhzy59YA6IlHL1QAAMDw4Gyx+eOk45WYG9M5nDXrstY9tlwMAwLDgbLAoyc/SvddeKEl6bNPHam3vtFwRAADuczZYSNJ1F5dqwuhsfdnaoae21dguBwAA5zkdLAJ+n37yrUmSpJWvfqS2jojligAAcJvTwUKSfnDJOI3JCelIc1hzfrFeH9Ry51MAANLF+WCRmeHXz6+fqswMnxqOd+iW/6hmvgUAAGnifLCQpPkXFuuNO69ScV6mPvmiVS++V6v2zqiqP/1SX7a02y4PAABneMaYAb0FaGNjo/Lz89XQ0KC8vLyBPLUeemmPfvXKPklSTiig5nCnLjorX88umyvP8wa0FgAAhpK+/v0eFj0WMQunlca/bg53DYfs/LxBW/cf1Ssf1OmnT7+jdz87pl2fN9gqEQCAIW1Y9VgYY3Tz77br48Mt+j/XTdX/e+eg1pziMtSKCwo1aWyOpo8r0GdftmreBYU6pzA3YZ/Pjx1XuCOiSWNzBqJ8AACs6evf72EVLGKMMfI8Tx/WNemaf9nUp/fkZgb0N9+apJd312lEKKBrphTpX/68Vw3HOzRn4ihdMmGk/nLmOJXmZ6k9EtXHh5t1cVmBPM9TzdFWNbV16vziXPl9XUMu9Y1tGpsbYggGADAkECz66Gdrd+qJNw/owtI8PXzjxXrgxT16aXdd/PWsDL+O93P9i4UXlWh6Wb4efOlDhTujyg0FdPWUIjWFO/Xy7jp9Y+wILbvqHF1cNlKbPjys9R/U66yCTH1v+lnKz8pQuDOiP79fp/GjsvW96Wdp44f1ihqpOD9TY0aEVJQf0msfHtHMCSM1ckQw4dz1jW3KDPqVl5lxRu0DAIBEsOiz1vZOPfHmAX33ohKVFmQpGjWa//Amffblcf3Hj2frvOJc7a1r0t/8x1sKBXz6y5njJEmPbvpIbR1RBQM+TS7OVXbQry0fHz3leTL8njoiqWtqv8/TyOygjjSHlZnh0+TiPJ1bmKP3axtV29CmI83t8fN+Y2yOppTmaeLoEYoaqbbxuDzP05GmsPKyMnS8I6K8zAyFAj6NyQmqM2o0uThXbR1RNRzvUGaGT3mZGTrY0KYvmsOaMDpbGX6fSguy9MmRFh042qq8rAx9Y2yORo3I0BfN7Ro5IqiS/Ewda+1QZ9QoFPApM8OvMTlBfVDbpI8PN6swL1NTS/M1Jieo4x0RtbZHFO6Myu958vmkUdlBHW4OKzPgV0ckqqygX1kZfvl9ntojUXny5HmSp67F0L4q9qsdNT1tBgDoH4LFGWho7dDxjoiK8zNPuU99Y5sa2zoS5l3UN7Ypw+9TW2dE+4+06MGXPtThprD+cuY4/c23J2nb/i/1+Bv7lR30639dOl7bPz2qtW9/rtqGNp1VkKWrpxTp3c8a9Pmx42pq69CXrR2aXJyrffXN6owO6I9pQIUCPoU7+34H2mDAp0jUyBiTEBpCAZ+CAZ/8nqfjHREd74jI73mKGCNjJM+TMnw++XySJ0/5WRnKDvrVGTXqiET1RXO7MvyeCrKDygr61dzWqdzMgIyklnCnggGfMvw+RY1RZ8Qo4PMUDPgU8HuKRqWoMd2Prq/9nqcMv08Zfk8Bv08+T+qIdNUtz5OvOxR58a89dUajihgp5PfpeEdERkYBn08Bn6eAv+t4kajR8Y6IckIBBf0+dUZN/NyS5PM8+TxPfl/XcWNft7ZHFDVGGX5f93mlrgoUD2iKb+/hKXGDUVd7xn4jY7V7Xtd7I9GuNvB7XTX7+jDc19su7Z1RdUSiyszwK3BCKDxx+NA76YvEeo26Cu3tvx7vlN8kHuPk9jgzp2qOr7Zzn95zyrf0rcrY+3v72cf+MvT1/zynO+PpfgX687m/7nw29ef/1Cd+llQNj99xzXnKTXGPNcFiiDPGKNzZ9T/VLR9/odf3HtGSuWfrSHO7PE/6oLZJreFOXXF+oQ4cbdV7Bxv0YV2TZk4YpbMKspQTCmjde4dUkBVUSUGmPv2iVXvrmnS4OayzR49QKOBX2agsbf/kS4UCPuVmBhTw+3S4Kaymtg59erRVxXmZyg4G1HC8XR0Ro7MKspSbGdAnX7Toy5YOHTjaqksmFGjC6BGqOdqqoy3tOtTQplEjgjreHlFtY5tGZmcoarquwumMRBU1Ukl+ps4pzNHBY8f18ZGW+P/APK/rj5FR1x/mWBg48RkA8PW23j1Phbmn/sdxf6Q1WKxcuVIPPPCAamtrNX36dP3617/W7NmzU1oYhr7YJNmYWLgoyA4mbKttaFNxfqaC/q5/mZvu937Z2qGC7AwZ0zWkE+6M6nh7RE1tnfL7PWX4Pfm7/zUe7oyqvTOqcGdEkWjX3JisoF+RqFHA7ynQPXzSGTHdvR3SsePtauuIxl8fmR1UJGr0ZWu7Wtsj8bVOpK7Jux2RqNo7jfy+rnN2RqJdx4yarl6BWM9D93Oku2ejIxKN91TEei5i/+LvClBd9USN5Pd19TB0Ro0yM3zyPK+75q5jRKJGnte1omxzW6ciUSOfz4u/T93HjESlaNQoYkx3D4KJDyN1Rky810FSd3srYVv8Z3jyD/WE3hYvsffCGBl11eF5XZ+nMxL92mOe6n9BwYBPAZ9PbR2ReG9MrISeY/W+PXaeWM9Mb/8qNl+p5Gs/ey/7JOOr50v2eL21U2/v7+shT9WOsddObLev+0f06T5Hb5+7b+/r5wn7IZmjxdqmN71uTqLxTO+b++WWK76hnFDgzA7yFX39+530WZ966iktX75cjzzyiObMmaOHH35Y8+fP1549e1RYWHhGRcMtX+3S6+2XPCcU0DmFvV2u62lsbihhS2aGX5kZ/pMmqvbXeGX3uv1sjUjJ8QFgOEp6gayHHnpIN998s5YsWaIpU6bokUceUXZ2th5//PF01AcAAIaQpIJFe3u7qqurVVFR0XMAn08VFRXavHlzr+8Jh8NqbGxMeAAAADclFSyOHDmiSCSioqKihO1FRUWqra3t9T1VVVXKz8+PP8rKyvpfLQAAGNTSfq+QyspKNTQ0xB81Nb0voQ0AAIa+pCZvjhkzRn6/X3V1dQnb6+rqVFxc3Ot7QqGQQqFQr68BAAC3JNVjEQwGNXPmTK1fvz6+LRqNav369SovL095cQAAYGhJ+nLT5cuXa/HixZo1a5Zmz56thx9+WC0tLVqyZEk66gMAAENI0sHixhtv1OHDh3XPPfeotrZWF198sdatW3fShE4AADD8sKQ3AAD4Wn39+532q0IAAMDwQbAAAAApQ7AAAAApQ7AAAAApk9p7qvZBbK4o9wwBAGDoiP3d/rprPgY8WDQ1NUkS9wwBAGAIampqUn5+/ilfH/DLTaPRqA4ePKjc3Fx5npey4zY2NqqsrEw1NTVcxppmtPXAoJ0HBu08cGjrgZGudjbGqKmpSaWlpfL5Tj2TYsB7LHw+n8aNG5e24+fl5fELO0Bo64FBOw8M2nng0NYDIx3tfLqeihgmbwIAgJQhWAAAgJRxJliEQiHde++93KJ9ANDWA4N2Hhi088ChrQeG7XYe8MmbAADAXc70WAAAAPsIFgAAIGUIFgAAIGUIFgAAIGWcCRYrV67U2WefrczMTM2ZM0dbt261XdKQsmnTJl177bUqLS2V53l65plnEl43xuiee+5RSUmJsrKyVFFRob179ybsc/ToUS1atEh5eXkqKCjQj3/8YzU3Nw/gpxj8qqqqdOmllyo3N1eFhYW67rrrtGfPnoR92tratHTpUo0ePVo5OTn6wQ9+oLq6uoR9Dhw4oIULFyo7O1uFhYX66U9/qs7OzoH8KIPaqlWrNG3atPgCQeXl5XrhhRfir9PG6bFixQp5nqfbb789vo22To377rtPnuclPCZPnhx/fVC1s3HAmjVrTDAYNI8//rh57733zM0332wKCgpMXV2d7dKGjOeff97cfffd5g9/+IORZNauXZvw+ooVK0x+fr555plnzDvvvGO+973vmYkTJ5rjx4/H9/nOd75jpk+fbrZs2WJee+01c84555ibbrppgD/J4DZ//nzz29/+1uzatcvs2LHDfPe73zXjx483zc3N8X1uueUWU1ZWZtavX2+2b99uvvnNb5rLLrss/npnZ6eZOnWqqaioMG+//bZ5/vnnzZgxY0xlZaWNjzQoPfvss+ZPf/qT+fDDD82ePXvMz372M5ORkWF27dpljKGN02Hr1q3m7LPPNtOmTTO33XZbfDttnRr33nuvufDCC82hQ4fij8OHD8dfH0zt7ESwmD17tlm6dGn8+0gkYkpLS01VVZXFqoaurwaLaDRqiouLzQMPPBDfduzYMRMKhcyTTz5pjDFm9+7dRpLZtm1bfJ8XXnjBeJ5nPv/88wGrfaipr683kszGjRuNMV3tmpGRYZ5++un4Pu+//76RZDZv3myM6QqBPp/P1NbWxvdZtWqVycvLM+FweGA/wBAycuRI82//9m+0cRo0NTWZc88917z88svm29/+djxY0Napc++995rp06f3+tpga+chPxTS3t6u6upqVVRUxLf5fD5VVFRo8+bNFitzx/79+1VbW5vQxvn5+ZozZ068jTdv3qyCggLNmjUrvk9FRYV8Pp/efPPNAa95qGhoaJAkjRo1SpJUXV2tjo6OhLaePHmyxo8fn9DWF110kYqKiuL7zJ8/X42NjXrvvfcGsPqhIRKJaM2aNWppaVF5eTltnAZLly7VwoULE9pU4vc51fbu3avS0lJNmjRJixYt0oEDByQNvnYe8JuQpdqRI0cUiUQSGkuSioqK9MEHH1iqyi21tbWS1Gsbx16rra1VYWFhwuuBQECjRo2K74NE0WhUt99+u+bOnaupU6dK6mrHYDCogoKChH2/2ta9/Sxir6HLzp07VV5erra2NuXk5Gjt2rWaMmWKduzYQRun0Jo1a/TWW29p27ZtJ73G73PqzJkzR6tXr9b555+vQ4cO6f7779fll1+uXbt2Dbp2HvLBAhiqli5dql27dun111+3XYqTzj//fO3YsUMNDQ36r//6Ly1evFgbN260XZZTampqdNttt+nll19WZmam7XKctmDBgvjX06ZN05w5czRhwgT9/ve/V1ZWlsXKTjbkh0LGjBkjv99/0uzXuro6FRcXW6rKLbF2PF0bFxcXq76+PuH1zs5OHT16lJ9DL5YtW6bnnntOr776qsaNGxffXlxcrPb2dh07dixh/6+2dW8/i9hr6BIMBnXOOedo5syZqqqq0vTp0/XLX/6SNk6h6upq1dfX65JLLlEgEFAgENDGjRv1q1/9SoFAQEVFRbR1mhQUFOi8887Tvn37Bt3v9JAPFsFgUDNnztT69evj26LRqNavX6/y8nKLlblj4sSJKi4uTmjjxsZGvfnmm/E2Li8v17Fjx1RdXR3f55VXXlE0GtWcOXMGvObByhijZcuWae3atXrllVc0ceLEhNdnzpypjIyMhLbes2ePDhw4kNDWO3fuTAhyL7/8svLy8jRlypSB+SBDUDQaVTgcpo1TaN68edq5c6d27NgRf8yaNUuLFi2Kf01bp0dzc7M++ugjlZSUDL7f6ZROBbVkzZo1JhQKmdWrV5vdu3ebn/zkJ6agoCBh9itOr6mpybz99tvm7bffNpLMQw89ZN5++23z6aefGmO6LjctKCgwf/zjH827775rvv/97/d6uemMGTPMm2++aV5//XVz7rnncrnpV9x6660mPz/fbNiwIeGysdbW1vg+t9xyixk/frx55ZVXzPbt2015ebkpLy+Pvx67bOyaa64xO3bsMOvWrTNjx47l8rwT3HXXXWbjxo1m//795t133zV33XWX8TzPvPTSS8YY2jidTrwqxBjaOlXuuOMOs2HDBrN//37zxhtvmIqKCjNmzBhTX19vjBlc7exEsDDGmF//+tdm/PjxJhgMmtmzZ5stW7bYLmlIefXVV42kkx6LFy82xnRdcvqP//iPpqioyIRCITNv3jyzZ8+ehGN88cUX5qabbjI5OTkmLy/PLFmyxDQ1NVn4NINXb20syfz2t7+N73P8+HHzt3/7t2bkyJEmOzvbXH/99ebQoUMJx/nkk0/MggULTFZWlhkzZoy54447TEdHxwB/msHrr//6r82ECRNMMBg0Y8eONfPmzYuHCmNo43T6arCgrVPjxhtvNCUlJSYYDJqzzjrL3HjjjWbfvn3x1wdTO3PbdAAAkDJDfo4FAAAYPAgWAAAgZQgWAAAgZQgWAAAgZQgWAAAgZQgWAAAgZQgWAAAgZQgWAAAgZQgWAAAgZQgWAAAgZQgWAAAgZQgWAAAgZf4/3IoYa/iy9tgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(pd.DataFrame(history.history['loss'])).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4110/4110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 529us/step\n"
     ]
    }
   ],
   "source": [
    "tf_output = model.predict(y_test)\n",
    "tf_output = np.concatenate((tf_output[0],tf_output[1]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_output = y_min_max_scaler.inverse_transform(tf_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.000329404603689909 + 0.003961558453738689i',\n",
       " '0.000554943922907114 + 0.004037679638713598i',\n",
       " '0.0005850124871358275 + 0.003906011814251542i',\n",
       " '0.0004935110919177532 + 0.0036496904212981462i',\n",
       " '0.00038422015495598316 + 0.003354865126311779i',\n",
       " '0.0003689576406031847 + 0.0030878004617989063i',\n",
       " '0.0005446297582238913 + 0.0028793285600841045i',\n",
       " '0.0009725304553285241 + 0.0027208770625293255i',\n",
       " '0.0016629532910883427 + 0.0025729485787451267i',\n",
       " '0.002567345043644309 + 0.002383622108027339i',\n",
       " '0.0035793606657534838 + 0.0021117343567311764i',\n",
       " '0.00454502459615469 + 0.0017478673253208399i',\n",
       " '0.005281135439872742 + 0.0013266312889754772i',\n",
       " '0.005599916912615299 + 0.0009257171768695116i',\n",
       " '0.005336867645382881 + 0.0006506959907710552i',\n",
       " '0.004378233104944229 + 0.0006083599291741848i',\n",
       " '0.0026840746868401766 + 0.0008748213294893503i',\n",
       " '0.0003032917156815529 + 0.001466552261263132i',\n",
       " '-0.0026224080938845873 + 0.0023224514443427324i',\n",
       " '-0.0058670733124017715 + 0.0031799213029444218i',\n",
       " '-0.009700639173388481 + 0.0032444610260427i',\n",
       " '-0.013508740812540054 + 0.0023804923985153437i',\n",
       " '-0.01606968604028225 + 0.0017650106456130743i',\n",
       " '-0.01747307926416397 + 0.0009121175389736891i',\n",
       " '-0.01767771504819393 + -0.0004323234024923295i',\n",
       " '-0.016601888462901115 + -0.002081307116895914i',\n",
       " '-0.014306994155049324 + -0.0037018239963799715i',\n",
       " '-0.011094172485172749 + -0.004549145232886076i',\n",
       " '-0.007701762020587921 + -0.005012684967368841i',\n",
       " '-0.004208412952721119 + -0.005580310709774494i',\n",
       " '-0.0006722519174218178 + -0.005740438122302294i',\n",
       " '0.0025560057256370783 + -0.005080741830170155i',\n",
       " '0.005123584531247616 + -0.00349710276350379i',\n",
       " '0.006816321983933449 + -0.0011745830997824669i',\n",
       " '0.007481306791305542 + 0.0015512038953602314i',\n",
       " '0.007156461477279663 + 0.004224065225571394i',\n",
       " '0.005805345252156258 + 0.006545387674123049i',\n",
       " '0.003914091736078262 + 0.00810933206230402i',\n",
       " '0.0017724904464557767 + 0.008685899898409843i',\n",
       " '-0.0004714444512501359 + 0.008318348787724972i',\n",
       " '-0.0026640493888407946 + 0.007223597262054682i',\n",
       " '-0.004755554720759392 + 0.005710265599191189i',\n",
       " '-0.006449471227824688 + 0.003911906853318214i',\n",
       " '-0.0077121686190366745 + 0.002024149289354682i',\n",
       " '-0.00839010626077652 + 0.00045146726188249886i',\n",
       " '-0.008484067395329475 + -0.0006345082074403763i',\n",
       " '-0.007994497194886208 + -0.0010291736107319593i',\n",
       " '-0.006945326924324036 + -0.0009564652573317289i',\n",
       " '-0.005391702055931091 + -0.0006312639452517033i',\n",
       " '-0.0033490473870187998 + -0.0003161399799864739i',\n",
       " '-0.0009588079992681742 + -8.171211811713874e-05i',\n",
       " '0.0016255616210401058 + -8.482762495987117e-05i',\n",
       " '0.00421278178691864 + -0.0004267964686732739i',\n",
       " '0.0065780626609921455 + -0.0010613284539431334i',\n",
       " '0.008590200915932655 + -0.00162047753110528i',\n",
       " '0.010194424539804459 + -0.0021655363962054253i',\n",
       " '0.010740629397332668 + -0.003122766502201557i',\n",
       " '0.010150170885026455 + -0.004401635844260454i',\n",
       " '0.008245048113167286 + -0.00571398762986064i',\n",
       " '0.00510686170309782 + -0.0064051165245473385i',\n",
       " '0.0015670370776206255 + -0.00638485187664628i',\n",
       " '-0.0022416699212044477 + -0.005581813398748636i',\n",
       " '-0.005883685313165188 + -0.00437292130663991i',\n",
       " '-0.009128169156610966 + -0.003366573480889201i',\n",
       " '-0.011949220672249794 + -0.0024165755603462458i',\n",
       " '-0.013872692361474037 + -0.0008933802600950003i',\n",
       " '-0.014377358369529247 + 0.001104225404560566i',\n",
       " '-0.013261836022138596 + 0.003367591882124543i',\n",
       " '-0.010501408949494362 + 0.005444515962153673i',\n",
       " '-0.006438263691961765 + 0.0067544314078986645i',\n",
       " '-0.0020624774042516947 + 0.006873976904898882i',\n",
       " '0.0016420003958046436 + 0.005916933994740248i',\n",
       " '0.004727044142782688 + 0.0043217637576162815i',\n",
       " '0.006823414005339146 + 0.0023466621059924364i',\n",
       " '0.007713287137448788 + 0.0004360750608611852i',\n",
       " '0.007215468212962151 + -0.001546449726447463i',\n",
       " '0.005403885617852211 + -0.0030598824378103018i',\n",
       " '0.0025590085424482822 + -0.0038091030437499285i',\n",
       " '-0.0009280738886445761 + -0.003779336577281356i',\n",
       " '-0.004580814391374588 + -0.0031008804216980934i',\n",
       " '-0.007893553003668785 + -0.002280387794598937i',\n",
       " '-0.01066318154335022 + -0.0021499760914593935i',\n",
       " '-0.012610318139195442 + -0.0019592707976698875i',\n",
       " '-0.013262579217553139 + -0.0017401955556124449i',\n",
       " '-0.01262027770280838 + -0.0017552368808537722i',\n",
       " '-0.010874735191464424 + -0.0021745332051068544i',\n",
       " '-0.008590059354901314 + -0.003201806452125311i',\n",
       " '-0.006257449276745319 + -0.005061445292085409i',\n",
       " '-0.003876875853165984 + -0.007355703506618738i',\n",
       " '-0.0018569501116871834 + -0.009145194664597511i',\n",
       " '-0.00047442037612199783 + -0.009911037050187588i',\n",
       " '-0.00032605393789708614 + -0.009209658950567245i',\n",
       " '-0.0010706698521971703 + -0.0070163351483643055i',\n",
       " '-0.0017309179529547691 + -0.003847189014777541i',\n",
       " '-0.0023676427081227303 + 0.00023634874378331006i',\n",
       " '-0.0027970613446086645 + 0.004527375567704439i',\n",
       " '-0.003260150318965316 + 0.008236836642026901i',\n",
       " '-0.0037543734069913626 + 0.010999638587236404i',\n",
       " '-0.0028734311927109957 + 0.012447180226445198i',\n",
       " '-0.0004170865286141634 + 0.011901495046913624i',\n",
       " '0.0032768864184617996 + 0.009299651719629765i',\n",
       " '0.006715531460940838 + 0.0048757330514490604i',\n",
       " '0.009561369195580482 + 0.0005779420025646687i',\n",
       " '0.013132463209331036 + -0.0032475690823048353i',\n",
       " '0.01590915210545063 + -0.007282696198672056i',\n",
       " '0.017008524388074875 + -0.010281027294695377i',\n",
       " '0.01690734550356865 + -0.011841102503240108i',\n",
       " '0.015820350497961044 + -0.011921560391783714i',\n",
       " '0.013868180103600025 + -0.010572992265224457i',\n",
       " '0.01112019270658493 + -0.008405192755162716i',\n",
       " '0.007054399698972702 + -0.005992550868541002i',\n",
       " '0.004526464268565178 + -0.003219052916392684i',\n",
       " '0.0027928054332733154 + -0.0005885008722543716i',\n",
       " '0.0014580285642296076 + 0.0009954695124179125i',\n",
       " '0.0005286864470690489 + 0.0014060067478567362i',\n",
       " '-5.6845834478735924e-05 + 0.0007626311853528023i',\n",
       " '-0.00039843854028731585 + -0.0006146556697785854i',\n",
       " '-0.0005984616000205278 + -0.0022892362903803587i',\n",
       " '-0.0007289601489901543 + -0.0038099903613328934i',\n",
       " '-0.0008096629753708839 + -0.004809127654880285i',\n",
       " '-0.0008023760747164488 + -0.005074931774288416i',\n",
       " '-0.0006234489846974611 + -0.0045851306058466434i',\n",
       " '-0.00017180945724248886 + -0.003495678538456559i',\n",
       " '0.0006338960956782103 + -0.002089898567646742i',\n",
       " '0.0018179671606048942 + -0.0007015692535787821i',\n",
       " '0.0033198012970387936 + 0.0003693558101076633i',\n",
       " '0.004983642604202032 + 0.000928608002141118i',\n",
       " '0.006570594385266304 + 0.0009307651780545712i',\n",
       " '0.007755841128528118 + 0.0007660125847905874i',\n",
       " '0.008324764668941498 + 0.0003002686717081815i',\n",
       " '0.008057902567088604 + -0.00042024461436085403i',\n",
       " '0.006773985922336578 + -0.0009249600116163492i',\n",
       " '0.004545344039797783 + -0.0007141018286347389i',\n",
       " '0.0015829025069251657 + 0.00016571118612773716i',\n",
       " '-0.001762069296091795 + 0.0015218609478324652i',\n",
       " '-0.00504427682608366 + 0.0030405705329030752i',\n",
       " '-0.008164250291883945 + 0.004010592121630907i',\n",
       " '-0.01066880114376545 + 0.004308746661990881i',\n",
       " '-0.011499474756419659 + 0.0043732840567827225i',\n",
       " '-0.010518976487219334 + 0.004099097102880478i',\n",
       " '-0.007896731607615948 + 0.003455026540905237i',\n",
       " '-0.005091340281069279 + 0.001765926368534565i',\n",
       " '-0.002106571337208152 + -0.0006884827744215727i',\n",
       " '0.0006967508234083652 + -0.0031459631863981485i',\n",
       " '0.002832014812156558 + -0.00518988911062479i',\n",
       " '0.0040413253009319305 + -0.006412862334400415i',\n",
       " '0.004062161780893803 + -0.0068013896234333515i',\n",
       " '0.0027818004600703716 + -0.0064996154978871346i',\n",
       " '0.0007731989026069641 + -0.00542225269600749i',\n",
       " '-0.0015246476978063583 + -0.003895421279594302i',\n",
       " '-0.0036399441305547953 + -0.0023151098284870386i',\n",
       " '-0.00508411880582571 + -0.0010152712929993868i',\n",
       " '-0.0055049508810043335 + -0.0002437224320601672i',\n",
       " '-0.00476762093603611 + -9.772295015864074e-05i',\n",
       " '-0.002974939765408635 + -0.0005323938094079494i',\n",
       " '-0.00046508561354130507 + -0.0013444169890135527i',\n",
       " '0.0022582539822906256 + -0.002231424441561103i',\n",
       " '0.004622930195182562 + -0.002854833612218499i',\n",
       " '0.006103341467678547 + -0.002821654547005892i',\n",
       " '0.00632911641150713 + -0.002125735394656658i',\n",
       " '0.005165628157556057 + -0.0007093865424394608i',\n",
       " '0.0027496202383190393 + 0.001469308976083994i',\n",
       " '-0.0005279881879687309 + 0.004038881044834852i',\n",
       " '-0.004232127219438553 + 0.006426307838410139i',\n",
       " '-0.008698712103068829 + 0.007667379919439554i',\n",
       " '-0.01189965195953846 + 0.008168314583599567i',\n",
       " '-0.01277818251401186 + 0.008095824159681797i',\n",
       " '-0.011293450370430946 + 0.007240456063300371i',\n",
       " '-0.00736627634614706 + 0.005865257699042559i',\n",
       " '-0.003011647379025817 + 0.0029931282624602318i',\n",
       " '0.0014327079989016056 + -0.0010537842754274607i',\n",
       " '0.005967209115624428 + -0.004943895619362593i',\n",
       " '0.01188216358423233 + -0.007031945511698723i',\n",
       " '0.015256344340741634 + -0.008931350894272327i',\n",
       " '0.016553370282053947 + -0.009980461560189724i',\n",
       " '0.01603037118911743 + -0.009892166592180729i',\n",
       " '0.01380965020507574 + -0.008646697737276554i',\n",
       " '0.009513963013887405 + -0.006898981984704733i',\n",
       " '0.0046031116507947445 + -0.003989772405475378i',\n",
       " '0.0010560741648077965 + 0.00027690178831107914i',\n",
       " '-0.0021236015018075705 + 0.004301146138459444i',\n",
       " '-0.005087509751319885 + 0.007176953833550215i',\n",
       " '-0.008061905391514301 + 0.00861644558608532i',\n",
       " '-0.00965098850429058 + 0.008702723309397697i',\n",
       " '-0.009965929202735424 + 0.00753880525007844i',\n",
       " '-0.009675612673163414 + 0.005293481517583132i',\n",
       " '-0.009164167568087578 + 0.002150033600628376i',\n",
       " '-0.009331382811069489 + -0.0020402048248797655i',\n",
       " '-0.009308228269219398 + -0.006778959650546312i',\n",
       " '-0.008945704437792301 + -0.010892914608120918i',\n",
       " '-0.008608798496425152 + -0.013319971971213818i',\n",
       " '-0.008106283843517303 + -0.014200962148606777i',\n",
       " '-0.00739324651658535 + -0.013516389764845371i',\n",
       " '-0.006605960428714752 + -0.011029409244656563i',\n",
       " '-0.0053645893931388855 + -0.006906780879944563i',\n",
       " '-0.0033562195021659136 + -0.0031876119319349527i',\n",
       " '-0.0008659684099256992 + 0.00019208973390050232i',\n",
       " '0.0012587872333824635 + 0.003160596825182438i',\n",
       " '0.0026780832558870316 + 0.005356969777494669i',\n",
       " '0.0030368193984031677 + 0.006641666404902935i',\n",
       " '0.0025755385868251324 + 0.006797590758651495i',\n",
       " '0.0014180513098835945 + 0.005974261090159416i',\n",
       " '-0.0003412252990528941 + 0.0044869049452245235i',\n",
       " '-0.002131745219230652 + 0.002547888085246086i',\n",
       " '-0.0034859890583902597 + 0.00042766614933498204i',\n",
       " '-0.0040159570053219795 + -0.0016537017654627562i',\n",
       " '-0.0035023449454456568 + -0.0035568370949476957i',\n",
       " '-0.0019517827313393354 + -0.005228900350630283i',\n",
       " '0.00046285358257591724 + -0.006627026479691267i',\n",
       " '0.0035118076484650373 + -0.007793116848915815i',\n",
       " '0.006862563081085682 + -0.008422753773629665i',\n",
       " '0.009408149868249893 + -0.009031174704432487i',\n",
       " '0.01025107130408287 + -0.009678136557340622i',\n",
       " '0.009272241033613682 + -0.010367308743298054i',\n",
       " '0.00648160558193922 + -0.011014267802238464i',\n",
       " '0.002397606847807765 + -0.01118172612041235i',\n",
       " '-0.0023553178180009127 + -0.010311662219464779i',\n",
       " '-0.006781762465834618 + -0.009086460806429386i',\n",
       " '-0.010520689189434052 + -0.007671176455914974i',\n",
       " '-0.013355683535337448 + -0.005730120465159416i',\n",
       " '-0.014476967975497246 + -0.003294862573966384i',\n",
       " '-0.0138552850112319 + -0.0009763210546225309i',\n",
       " '-0.011135071516036987 + 0.0011323621729388833i',\n",
       " '-0.007289831526577473 + 0.0020716642029583454i',\n",
       " '-0.0036493928637355566 + 0.0006525813369080424i',\n",
       " '-0.00037894851993769407 + -0.00215604854747653i',\n",
       " '0.0019474835135042667 + -0.005699757486581802i',\n",
       " '0.003888952312991023 + -0.009084636345505714i',\n",
       " '0.005215478129684925 + -0.012551507912576199i',\n",
       " '0.004437301307916641 + -0.015144617296755314i',\n",
       " '0.0022013485431671143 + -0.01623910292983055i',\n",
       " '-0.0007418233435600996 + -0.015689874067902565i',\n",
       " '-0.003764837747439742 + -0.013088789768517017i',\n",
       " '-0.006303003989160061 + -0.008187186904251575i',\n",
       " '-0.006912051700055599 + -0.002335956087335944i',\n",
       " '-0.0055200569331645966 + 0.00287438090890646i',\n",
       " '-0.0031231350731104612 + 0.007363614160567522i',\n",
       " '-0.0012322794646024704 + 0.011182654649019241i',\n",
       " '0.0011678696610033512 + 0.013962418772280216i',\n",
       " '0.004259008448570967 + 0.015125756151974201i',\n",
       " '0.006722559221088886 + 0.01436264906078577i',\n",
       " '0.00811721757054329 + 0.01210020761936903i',\n",
       " '0.00841438490897417 + 0.00846210028976202i',\n",
       " '0.007366574369370937 + 0.004481545649468899i',\n",
       " '0.004820199217647314 + 0.002102707512676716i',\n",
       " '0.0016878095921128988 + 0.001129357609897852i',\n",
       " '-0.0013063380029052496 + 0.0013021512422710657i',\n",
       " '-0.0036626567598432302 + 0.0025071415584534407i',\n",
       " '-0.005017274059355259 + 0.004328907001763582i',\n",
       " '-0.005513304844498634 + 0.006318292114883661i',\n",
       " '-0.005034330300986767 + 0.008220338262617588i',\n",
       " '-0.003589509753510356 + 0.00977026391774416i',\n",
       " '-0.0013545788824558258 + 0.010386697016656399i',\n",
       " '0.001286008395254612 + 0.010037668980658054i',\n",
       " '0.003677218221127987 + 0.008372937329113483i',\n",
       " '0.0052816132083535194 + 0.005846972577273846i',\n",
       " '0.005718923173844814 + 0.0030944410245865583i',\n",
       " '0.005052770487964153 + 0.0005523753352463245i',\n",
       " '0.00350496475584805 + -0.0014345874078571796i',\n",
       " '0.001378216315060854 + -0.002602529479190707i',\n",
       " '-0.0009778214152902365 + -0.0027855143416672945i',\n",
       " '-0.003195395926013589 + -0.0019675050862133503i',\n",
       " '-0.004949918016791344 + -0.00027092170785181224i',\n",
       " '-0.006009381264448166 + 0.0019821119494736195i',\n",
       " '-0.006333882920444012 + 0.004549461882561445i',\n",
       " '-0.006383124738931656 + 0.0069985282607376575i',\n",
       " '-0.0061053140088915825 + 0.009487244300544262i',\n",
       " '-0.005276963114738464 + 0.01164590660482645i',\n",
       " '-0.00412424560636282 + 0.01346875075250864i',\n",
       " '-0.002657042583450675 + 0.01446347963064909i',\n",
       " '-0.0013293882366269827 + 0.014367434196174145i',\n",
       " '-0.0001617905218154192 + 0.013250837102532387i',\n",
       " '0.0009664538665674627 + 0.011277539655566216i',\n",
       " '0.001729336567223072 + 0.008577265776693821i',\n",
       " '0.0013097025221213698 + 0.005727940704673529i',\n",
       " '0.00015485216863453388 + 0.0029762403573840857i',\n",
       " '-0.0011287180241197348 + 0.00039645322249270976i',\n",
       " '-0.0023649358190596104 + -0.0017470400780439377i',\n",
       " '-0.0034064871724694967 + -0.003224171232432127i',\n",
       " '-0.004144183360040188 + -0.003891082713380456i',\n",
       " '-0.0045455340296030045 + -0.0036646805237978697i',\n",
       " '-0.004634666256606579 + -0.002579502062872052i',\n",
       " '-0.004448579624295235 + -0.0008557478431612253i',\n",
       " '-0.004067788831889629 + 0.0012083041947335005i',\n",
       " '-0.003590970067307353 + 0.0032418486662209034i',\n",
       " '-0.0030998324509710073 + 0.004882383160293102i',\n",
       " '-0.0026504199486225843 + 0.005827697459608316i',\n",
       " '-0.00226474367082119 + 0.005859616212546825i',\n",
       " '-0.0019304296001791954 + 0.004905563313513994i',\n",
       " '-0.0016080245841294527 + 0.003004593774676323i',\n",
       " '-0.0012442751321941614 + 0.00041967668221332133i',\n",
       " '-0.0007883904036134481 + -0.002471425337716937i',\n",
       " '-0.0002076758537441492 + -0.005248211789876223i',\n",
       " '0.0006265572737902403 + -0.007404261734336615i',\n",
       " '0.0019014617428183556 + -0.008807490579783916i',\n",
       " '0.0029809882398694754 + -0.00914455484598875i',\n",
       " '0.003617636626586318 + -0.008511073887348175i',\n",
       " '0.0038560470566153526 + -0.00714311096817255i',\n",
       " '0.0040523093193769455 + -0.005275909323245287i',\n",
       " '0.0042670853435993195 + -0.003016713773831725i',\n",
       " '0.00424499437212944 + -0.0008800087962299585i',\n",
       " '0.004034297540783882 + 0.0008486872538924217i',\n",
       " '0.0037192224990576506 + 0.001996840350329876i',\n",
       " '0.0034033753909170628 + 0.0025273507926613092i',\n",
       " '0.003187713446095586 + 0.0025319536216557026i',\n",
       " '0.0031476698350161314 + 0.0021982877515256405i',\n",
       " '0.0033143970649689436 + 0.0017592203803360462i',\n",
       " '0.003664578078314662 + 0.0014361515641212463i',\n",
       " '0.004121439065784216 + 0.0013891230337321758i',\n",
       " '0.004567454569041729 + 0.001684227492660284i',\n",
       " '0.004866523668169975 + 0.0022846919018775225i',\n",
       " '0.004891321994364262 + 0.0030662899371236563i',\n",
       " '0.004544330760836601 + 0.0038586065638810396i',\n",
       " '0.003807410830631852 + 0.004458436276763678i',\n",
       " '0.00269329734146595 + 0.004738540854305029i',\n",
       " '0.0012998190941289067 + 0.004617475438863039i',\n",
       " '-0.0002359852660447359 + 0.004107131157070398i',\n",
       " '-0.0017586259637027979 + 0.0033007333986461163i',\n",
       " '-0.0031249651219695807 + 0.002348486566916108i',\n",
       " '-0.004232578910887241 + 0.0014211137313395739i',\n",
       " '-0.005039434880018234 + 0.0006704942788928747i',\n",
       " '-0.005570253357291222 + 0.00019064408843405545i',\n",
       " '-0.0059003671631217 + -5.538080586120486e-06i',\n",
       " '-0.0061584347859025 + 6.598446634598076e-05i',\n",
       " '-0.006465433165431023 + 0.00030156507273204625i',\n",
       " '-0.006911782547831535 + 0.0005607298808172345i',\n",
       " '-0.0075387172400951385 + 0.0007019496988505125i',\n",
       " '-0.008309513330459595 + 0.0006650967989116907i',\n",
       " '-0.009182562120258808 + 0.0002813542087096721i',\n",
       " '-0.009890015237033367 + -0.00024673432926647365i',\n",
       " '-0.010199629701673985 + -0.0006724181585013866i',\n",
       " '-0.009905898943543434 + -0.0008153142407536507i',\n",
       " '-0.008883125148713589 + -0.0005484507419168949i',\n",
       " '-0.007231450639665127 + -0.00017106576706282794i',\n",
       " '-0.004939536564052105 + 9.137651068158448e-05i',\n",
       " '-0.0020919593516737223 + 0.00021323564578779042i',\n",
       " '0.001030578976497054 + 0.00031844995100982487i',\n",
       " '0.004129303619265556 + 0.0003272441972512752i',\n",
       " '0.006896252743899822 + 0.00024250292335636914i',\n",
       " '0.009029481559991837 + 0.0006759369280189276i',\n",
       " '0.010543957352638245 + 0.0009358085226267576i',\n",
       " '0.011137913912534714 + 0.0009415005333721638i',\n",
       " '0.010701342485845089 + 0.0007427246309816837i',\n",
       " '0.009427390992641449 + 0.0006457255221903324i',\n",
       " '0.007724447175860405 + 0.0009463028982281685i',\n",
       " '0.005842072889208794 + 0.0021368591114878654i',\n",
       " '0.003905470250174403 + 0.004022377077490091i',\n",
       " '0.0021667613182216883 + 0.006115564610809088i',\n",
       " '0.0005673323175869882 + 0.008160698227584362i',\n",
       " '-0.0010066444519907236 + 0.009618178941309452i',\n",
       " '-0.0018250423017889261 + 0.010304350405931473i',\n",
       " '-0.0017316483426839113 + 0.009948368184268475i',\n",
       " '-0.0008616081904619932 + 0.008517568930983543i',\n",
       " '-6.727594882249832e-05 + 0.0060737053863704205i',\n",
       " '0.0003171485150232911 + 0.0029156841337680817i',\n",
       " '0.000591450254432857 + -0.0005047109443694353i',\n",
       " '0.0006657361518591642 + -0.003661776427179575i',\n",
       " '0.0005184838082641363 + -0.006014599930495024i',\n",
       " '0.00016859848983585835 + -0.0071240863762795925i',\n",
       " '-0.0004951402079313993 + -0.00687006302177906i',\n",
       " '-0.0013806733768433332 + -0.005221053492277861i',\n",
       " '-0.002216630382463336 + -0.0022579492069780827i',\n",
       " '-0.002972017740830779 + 0.0015194169245660305i',\n",
       " '-0.0035151171032339334 + 0.005468163173645735i',\n",
       " '-0.004728940315544605 + 0.008903145790100098i',\n",
       " '-0.005854243412613869 + 0.01158058363944292i',\n",
       " '-0.0057183727622032166 + 0.013316753320395947i',\n",
       " '-0.004399387165904045 + 0.013647067360579967i',\n",
       " '-0.0019208253361284733 + 0.012417533434927464i',\n",
       " '0.001538068987429142 + 0.009881188161671162i',\n",
       " '0.004531944170594215 + 0.006221594754606485i',\n",
       " '0.006869005039334297 + 0.0025305557064712048i',\n",
       " '0.008869650773704052 + -0.00027590731042437255i',\n",
       " '0.010803288780152798 + -0.002232487080618739i',\n",
       " '0.011874904856085777 + -0.003634513122960925i',\n",
       " '0.011778423562645912 + -0.004551413003355265i',\n",
       " '0.010306159034371376 + -0.005041233263909817i',\n",
       " '0.007721717469394207 + -0.005136564373970032i',\n",
       " '0.004912415519356728 + -0.0047742691822350025i',\n",
       " '0.002307952381670475 + -0.004014333710074425i',\n",
       " '-0.00013356213457882404 + -0.0035365985240787268i',\n",
       " '-0.0020809199195355177 + -0.003536070929840207i',\n",
       " '-0.00327673158608377 + -0.004007540177553892i',\n",
       " '-0.0035754244308918715 + -0.004813857842236757i',\n",
       " '-0.0029725509230047464 + -0.005662708543241024i',\n",
       " '-0.001625909935683012 + -0.00623710872605443i',\n",
       " '0.00016290298663079739 + -0.006313996855169535i',\n",
       " '0.0020246298518031836 + -0.005670652259141207i',\n",
       " '0.003693337319418788 + -0.004129690118134022i',\n",
       " '0.004797239322215319 + -0.0017846915870904922i',\n",
       " '0.005112018436193466 + 0.0011688692029565573i',\n",
       " '0.004519948735833168 + 0.00445295637473464i',\n",
       " '0.002824246184900403 + 0.007788700517266989i',\n",
       " '-0.00021027715411037207 + 0.010655437596142292i',\n",
       " '-0.0036490277852863073 + 0.012449714355170727i',\n",
       " '-0.006755027920007706 + 0.013043572194874287i',\n",
       " '-0.00917726755142212 + 0.012619187124073505i',\n",
       " '-0.010763978585600853 + 0.011377512477338314i',\n",
       " '-0.011448088102042675 + 0.00962076522409916i',\n",
       " '-0.011162731796503067 + 0.007727211806923151i',\n",
       " '-0.010284453630447388 + 0.006017087027430534i',\n",
       " '-0.009157480672001839 + 0.004605185706168413i',\n",
       " '-0.00791715458035469 + 0.003569971537217498i',\n",
       " '-0.007046220824122429 + 0.0028322190046310425i',\n",
       " '-0.006507813930511475 + 0.00239800033159554i',\n",
       " '-0.006156030111014843 + 0.0022561766672879457i',\n",
       " '-0.0059451693668961525 + 0.002282860688865185i',\n",
       " '-0.005759757943451405 + 0.002295716432854533i',\n",
       " '-0.005444912239909172 + 0.002136751776561141i',\n",
       " '-0.004849126562476158 + 0.001696968451142311i',\n",
       " '-0.0038671803195029497 + 0.000910577829927206i',\n",
       " '-0.002473629079759121 + -0.00016157582285813987i',\n",
       " '-0.000739165348932147 + -0.001363381277769804i',\n",
       " '0.001173890195786953 + -0.0025159732904285192i',\n",
       " '0.003036587731912732 + -0.0034371349029242992i',\n",
       " '0.004594011232256889 + -0.0039827460423111916i',\n",
       " '0.005613476969301701 + -0.003991061821579933i',\n",
       " '0.00593055784702301 + -0.003643294796347618i',\n",
       " '0.005483660846948624 + -0.003062400035560131i',\n",
       " '0.004330027848482132 + -0.0023234353866428137i',\n",
       " '0.002639878075569868 + -0.0016333360690623522i',\n",
       " '0.0006696733180433512 + -0.0012497545685619116i',\n",
       " '-0.0012804812286049128 + -0.0013391557149589062i',\n",
       " '-0.0029170892667025328 + -0.0019816295243799686i',\n",
       " '-0.004001794382929802 + -0.0031506100203841925i',\n",
       " '-0.004353463649749756 + -0.004767923150211573i',\n",
       " '-0.003799513215199113 + -0.0065896762534976006i',\n",
       " '-0.0025471970438957214 + -0.008323176763951778i',\n",
       " '-0.0007052968721836805 + -0.00963160116225481i',\n",
       " '0.0012983549386262894 + -0.010215134359896183i',\n",
       " '0.0030218155588954687 + -0.009944339282810688i',\n",
       " '0.004213389009237289 + -0.008910875767469406i',\n",
       " '0.004969220608472824 + -0.0075145140290260315i',\n",
       " '0.00515311025083065 + -0.005804427433758974i',\n",
       " '0.005264737643301487 + -0.00387586816214025i',\n",
       " '0.005102050490677357 + -0.001895152498036623i',\n",
       " '0.004694757517427206 + -0.00013722453149966896i',\n",
       " '0.004177287686616182 + 0.001195806311443448i',\n",
       " '0.0036649524699896574 + 0.002051079412922263i',\n",
       " '0.0032305014319717884 + 0.002477136440575123i',\n",
       " '0.0028927919920533895 + 0.00260641286149621i',\n",
       " '0.0026192530058324337 + 0.0026208970230072737i',\n",
       " '0.0023408939596265554 + 0.002708273474127054i',\n",
       " '0.001975731225684285 + 0.0030177708249539137i',\n",
       " '0.0014550714986398816 + 0.003624916775152087i',\n",
       " '0.000746391830034554 + 0.004512586165219545i',\n",
       " '-0.0001324416371062398 + 0.005572219844907522i',\n",
       " '-0.0011108934413641691 + 0.006592657882720232i',\n",
       " '-0.002187902806326747 + 0.007343845907598734i',\n",
       " '-0.0031808402854949236 + 0.007680131588131189i',\n",
       " '-0.003708738135173917 + 0.0074941013008356094i',\n",
       " '-0.0037586919497698545 + 0.006774723995476961i',\n",
       " '-0.0033557212445884943 + 0.0055227638222277164i',\n",
       " '-0.002718136413022876 + 0.003818984841927886i',\n",
       " '-0.0017788931727409363 + 0.0018490536604076624i',\n",
       " '-0.0006778673268854618 + -7.33036722522229e-05i',\n",
       " '0.00041845347732305527 + -0.001691501121968031i',\n",
       " '0.0013466374948620796 + -0.0028136400505900383i',\n",
       " '0.001975682331249118 + -0.0033388747833669186i',\n",
       " '0.0022302467841655016 + -0.0032746142242103815i',\n",
       " '0.0021033729426562786 + -0.0027329400181770325i',\n",
       " '0.0016562629025429487 + -0.0019071102142333984i',\n",
       " '0.0010053501464426517 + -0.001032722182571888i',\n",
       " '0.00029951101168990135 + -0.00034115739981643856i',\n",
       " '-0.0003080002497881651 + -1.4309276593849063e-05i',\n",
       " '-0.0006872190861031413 + -0.00014933288912288845i',\n",
       " '-0.0007533647585660219 + -0.00074002705514431i',\n",
       " '-0.00047887698747217655 + -0.0016784518957138062i',\n",
       " '0.0001063555246219039 + -0.0027762551326304674i',\n",
       " '0.000926018925383687 + -0.0038016662001609802i',\n",
       " '0.001876784022897482 + -0.004525084514170885i',\n",
       " '0.0028515998274087906 + -0.004764661658555269i',\n",
       " '0.0037616065237671137 + -0.004423338919878006i',\n",
       " '0.004551978781819344 + -0.003510324517264962i',\n",
       " '0.005208338610827923 + -0.0021430172491818666i',\n",
       " '0.005752706900238991 + -0.000528672942891717i',\n",
       " '0.006230193190276623 + 0.0010708742775022984i',\n",
       " '0.0066356416791677475 + 0.0024452279321849346i',\n",
       " '0.007057640701532364 + 0.0033522562589496374i',\n",
       " '0.007517564110457897 + 0.0036571368109434843i',\n",
       " '0.007978818379342556 + 0.003266574814915657i',\n",
       " '0.0083619961515069 + 0.0022067208774387836i',\n",
       " '0.008558157831430435 + 0.0006578504107892513i',\n",
       " '0.008505703881382942 + -0.0010840396862477064i',\n",
       " '0.00796488020569086 + -0.0026632798835635185i',\n",
       " '0.0068429578095674515 + -0.0036792301107198i',\n",
       " '0.00531206838786602 + -0.003981318324804306i',\n",
       " '0.0034617490600794554 + -0.003267669351771474i',\n",
       " '0.001398867228999734 + -0.0015959572046995163i',\n",
       " '-0.0006867749616503716 + 0.0008286747615784407i',\n",
       " '-0.002589581301435828 + 0.0037378950510174036i',\n",
       " '-0.00429757684469223 + 0.006592261604964733i',\n",
       " '-0.006551777012646198 + 0.008996156975626945i',\n",
       " '-0.008161121048033237 + 0.010990955866873264i',\n",
       " '-0.008536692708730698 + 0.012331325560808182i',\n",
       " '-0.00793963298201561 + 0.012805557809770107i',\n",
       " '-0.006630191579461098 + 0.012260259129106998i',\n",
       " '-0.004630004987120628 + 0.010886414907872677i',\n",
       " '-0.0024783210828900337 + 0.008814467117190361i',\n",
       " '-0.0010862324852496386 + 0.006313696037977934i',\n",
       " '-0.000779620953835547 + 0.0037324039731174707i',\n",
       " '-0.0009500009473413229 + 0.001393664162606001i',\n",
       " '-0.0015938340220600367 + -0.000481207825941965i',\n",
       " '-0.0026398557238280773 + -0.0018068000208586454i',\n",
       " '-0.003964009694755077 + -0.00261393073014915i',\n",
       " '-0.00538256298750639 + -0.003084008116275072i',\n",
       " '-0.006720953620970249 + -0.0034211024176329374i',\n",
       " '-0.007828924804925919 + -0.004013684578239918i',\n",
       " '-0.00863984227180481 + -0.0047536310739815235i',\n",
       " '-0.009021292440593243 + -0.0056452821008861065i',\n",
       " '-0.008963342756032944 + -0.006610405631363392i',\n",
       " '-0.008471413515508175 + -0.007430016528815031i',\n",
       " '-0.0076789213344454765 + -0.007901320233941078i',\n",
       " '-0.006713344715535641 + -0.007796466816216707i',\n",
       " '-0.005712275393307209 + -0.006909448653459549i',\n",
       " '-0.004759247414767742 + -0.005465496331453323i',\n",
       " '-0.0035893593449145555 + -0.0034059113822877407i',\n",
       " '-0.0024267795961350203 + -0.0008269750978797674i',\n",
       " '-0.0014958290848881006 + 0.0021852366626262665i',\n",
       " '-0.0008722578641027212 + 0.005299137439578772i',\n",
       " '-0.0008630177471786737 + 0.008124400861561298i',\n",
       " '-0.0019436385482549667 + 0.01036567147821188i',\n",
       " '-0.00304571189917624 + 0.011777381412684917i',\n",
       " '-0.003864289028570056 + 0.011981072835624218i',\n",
       " '-0.004232947714626789 + 0.01112916599959135i',\n",
       " '-0.0041728634387254715 + 0.009562584571540356i',\n",
       " '-0.0036499162670224905 + 0.007421315647661686i',\n",
       " '-0.0034363002050668 + 0.005097476299852133i',\n",
       " '-0.0030225685331970453 + 0.002673602197319269i',\n",
       " '-0.0021196906454861164 + 0.00038584883441217244i',\n",
       " '-0.0007655064109712839 + -0.0015700545627623796i',\n",
       " '0.0009336939547210932 + -0.003178648417815566i',\n",
       " '0.002817915752530098 + -0.00451786071062088i',\n",
       " '0.004736103117465973 + -0.005616154056042433i',\n",
       " '0.006989753805100918 + -0.006357437465339899i',\n",
       " '0.009184951893985271 + -0.0072257500141859055i',\n",
       " '0.010872011072933674 + -0.008056201972067356i',\n",
       " '0.011771395802497864 + -0.009051242843270302i',\n",
       " '0.012078160420060158 + -0.010032843798398972i',\n",
       " '0.012023655697703362 + -0.010662208311259747i',\n",
       " '0.011714404448866844 + -0.010585278272628784i',\n",
       " '0.011231187731027603 + -0.009658223018050194i',\n",
       " '0.010597220622003078 + -0.007978050038218498i',\n",
       " '0.009713943116366863 + -0.005655856337398291i',\n",
       " '0.009375974535942078 + -0.0027846505399793386i',\n",
       " '0.009400460869073868 + 0.00020793781732209027i',\n",
       " '0.009471573866903782 + 0.002959521720185876i',\n",
       " '0.009354522451758385 + 0.0048341345973312855i',\n",
       " '0.00893997773528099 + 0.005131119396537542i',\n",
       " '0.008139941841363907 + 0.0036856497172266245i',\n",
       " '0.00673968531191349 + 0.0011299578472971916i',\n",
       " '0.004692606162279844 + -0.001634107669815421i',\n",
       " '0.002296790014952421 + -0.004610721487551928i',\n",
       " '-8.842302486300468e-05 + -0.0071150450967252254i',\n",
       " '-0.0018047189805656672 + -0.00912527833133936i',\n",
       " '-0.0034216807689517736 + -0.00968081783503294i',\n",
       " '-0.004861969500780106 + -0.008418958634138107i',\n",
       " '-0.005852167494595051 + -0.005335046909749508i',\n",
       " '-0.005690491758286953 + -0.0012784791179001331i',\n",
       " '-0.004671263508498669 + 0.0031020150054246187i',\n",
       " '-0.0033213419374078512 + 0.007264616433531046i',\n",
       " '-0.002894609933719039 + 0.01096237450838089i',\n",
       " '-0.00229757372289896 + 0.013857250101864338i',\n",
       " '-0.0009555539581924677 + 0.0151116494089365i',\n",
       " '0.0004982249811291695 + 0.014580020681023598i',\n",
       " '0.002081554848700762 + 0.01257710624486208i',\n",
       " '0.0035332764964550734 + 0.009563895873725414i',\n",
       " '0.004114270675927401 + 0.0063694133423268795i',\n",
       " '0.0037577131297439337 + 0.004018840845674276i',\n",
       " '0.002988455817103386 + 0.0027345444541424513i',\n",
       " '0.0021025286987423897 + 0.002643324201926589i',\n",
       " '0.001200028695166111 + 0.003732366254553199i',\n",
       " '0.00035434908932074904 + 0.005735848564654589i',\n",
       " '-0.0006595799932256341 + 0.008190230466425419i',\n",
       " '-0.0023476947098970413 + 0.010511723347008228i',\n",
       " '-0.00373437418602407 + 0.012255742214620113i',\n",
       " '-0.004346130415797234 + 0.01286137755960226i',\n",
       " '-0.0039647240191698074 + 0.012129406444728374i',\n",
       " '-0.0026249983347952366 + 0.010153128765523434i',\n",
       " '-0.0008006321731954813 + 0.007234826218336821i',\n",
       " '0.00024830340407788754 + 0.0037068293895572424i',\n",
       " '0.001636370550841093 + 9.61132172960788e-05i',\n",
       " '0.0033464732114225626 + -0.0030577953439205885i',\n",
       " '0.005312255583703518 + -0.005198121536523104i',\n",
       " '0.00788469985127449 + -0.005907597951591015i',\n",
       " '0.010158810764551163 + -0.005450774449855089i',\n",
       " '0.011622382327914238 + -0.004147620871663094i',\n",
       " '0.012156197801232338 + -0.0025646984577178955i',\n",
       " '0.011930853128433228 + -0.0012040953151881695i',\n",
       " '0.010785379447042942 + -0.0005401736125349998i',\n",
       " '0.008818265981972218 + -0.0008257168810814619i',\n",
       " '0.006335596553981304 + -0.001638034824281931i',\n",
       " '0.003388927085325122 + -0.0027695561293512583i',\n",
       " '0.0001554042100906372 + -0.004442657344043255i',\n",
       " '-0.002924887230619788 + -0.006451758556067944i',\n",
       " '-0.00510331429541111 + -0.00916401855647564i',\n",
       " '-0.006808442994952202 + -0.011789347045123577i',\n",
       " '-0.008057807572185993 + -0.013143797405064106i',\n",
       " '-0.008821569383144379 + -0.01327578630298376i',\n",
       " '-0.008863605558872223 + -0.012418288737535477i',\n",
       " '-0.008311833254992962 + -0.010526446625590324i',\n",
       " '-0.007207294926047325 + -0.0075747366063296795i',\n",
       " '-0.005542980507016182 + -0.004734414163976908i',\n",
       " '-0.0031309088226407766 + -0.0029585028532892466i',\n",
       " '-0.0005414881743490696 + -0.0019015276338905096i',\n",
       " '0.0018644335214048624 + -0.0015771319158375263i',\n",
       " '0.0038714830297976732 + -0.0020311702974140644i',\n",
       " '0.005304915830492973 + -0.0031544493976980448i',\n",
       " '0.006103724241256714 + -0.004617480095475912i',\n",
       " '0.006472589448094368 + -0.006258631590753794i',\n",
       " '0.00628346111625433 + -0.007970066741108894i',\n",
       " '0.005175942555069923 + -0.009530575014650822i',\n",
       " '0.0036949526984244585 + -0.010928709991276264i',\n",
       " '0.001548907719552517 + -0.011446761898696423i',\n",
       " '-0.001003628596663475 + -0.011130156926810741i',\n",
       " '-0.003622194519266486 + -0.009785132482647896i',\n",
       " '-0.0057229334488511086 + -0.007896778173744678i',\n",
       " '-0.007361152209341526 + -0.005392043851315975i',\n",
       " '-0.008201216347515583 + -0.0026462709065526724i',\n",
       " '-0.008258409798145294 + 5.167376366443932e-05i',\n",
       " '-0.00773334875702858 + 0.002375074429437518i',\n",
       " '-0.006725683808326721 + 0.0042508975602686405i',\n",
       " '-0.0053620971739292145 + 0.005472149699926376i',\n",
       " '-0.0037987257819622755 + 0.00596444820985198i',\n",
       " '-0.002348239766433835 + 0.00569117208942771i',\n",
       " '-0.0011408415157347918 + 0.00471721775829792i',\n",
       " '-0.0002723026555031538 + 0.0031353847589343786i',\n",
       " '0.0001887116814032197 + 0.0011745099909603596i',\n",
       " '0.00022165593691170216 + -0.0009516340214759111i',\n",
       " '-0.00015282048843801022 + -0.0030165747739374638i',\n",
       " '-0.0008832996245473623 + -0.004803275689482689i',\n",
       " '-0.0018562627956271172 + -0.0060899145901203156i',\n",
       " '-0.002883289707824588 + -0.006908135954290628i',\n",
       " '-0.0040517086163163185 + -0.007111287675797939i',\n",
       " '-0.005391140468418598 + -0.006503078620880842i',\n",
       " '-0.006760762073099613 + -0.005198617000132799i',\n",
       " '-0.00787561759352684 + -0.0033101574517786503i',\n",
       " '-0.008645371533930302 + -0.00095520936883986i',\n",
       " '-0.009041708894073963 + 0.0016764334868639708i',\n",
       " '-0.009261755272746086 + 0.004323830362409353i',\n",
       " '-0.008827989920973778 + 0.007035123649984598i',\n",
       " '-0.008099934086203575 + 0.009889801032841206i',\n",
       " '-0.006695843301713467 + 0.012543796561658382i',\n",
       " '-0.004597339779138565 + 0.01505270041525364i',\n",
       " '-0.0020193441305309534 + 0.016920054331421852i',\n",
       " '0.000578196020796895 + 0.017809662967920303i',\n",
       " '0.002826752606779337 + 0.017895273864269257i',\n",
       " '0.004452789667993784 + 0.017406292259693146i',\n",
       " '0.0053402455523610115 + 0.016281286254525185i',\n",
       " '0.005391580983996391 + 0.014489891938865185i',\n",
       " '0.0048454985953867435 + 0.011559728533029556i',\n",
       " '0.003850150154903531 + 0.008474007248878479i',\n",
       " '0.0020339421462267637 + 0.006185303907841444i',\n",
       " '-0.00041189207695424557 + 0.004650144837796688i',\n",
       " '-0.0024950045626610518 + 0.0035506130661815405i',\n",
       " '-0.0038793834391981363 + 0.0028762188740074635i',\n",
       " '-0.004333817400038242 + 0.0025834152474999428i',\n",
       " '-0.003767777467146516 + 0.0025278576649725437i',\n",
       " '-0.002242883201688528 + 0.0024542436003684998i',\n",
       " '4.186015576124191e-05 + 0.0021390842739492655i',\n",
       " '0.002787856850773096 + 0.0013882583007216454i',\n",
       " '0.005649941973388195 + 7.181035471148789e-05i',\n",
       " '0.008355516940355301 + -0.0013901044148951769i',\n",
       " '0.011229841969907284 + -0.0026204437017440796i',\n",
       " '0.013820652849972248 + -0.004536384716629982i',\n",
       " '0.015283521264791489 + -0.006927626673132181i',\n",
       " '0.015752481296658516 + -0.00912335142493248i',\n",
       " '0.015386464074254036 + -0.010692891664803028i',\n",
       " '0.014525948092341423 + -0.01150852907449007i',\n",
       " '0.01319710910320282 + -0.011253655888140202i',\n",
       " '0.011393574066460133 + -0.00988419633358717i',\n",
       " '0.008648773655295372 + -0.007798938080668449i',\n",
       " '0.005574247799813747 + -0.005004527512937784i',\n",
       " '0.003813405754044652 + -0.001244822982698679i',\n",
       " '0.001997569343075156 + 0.0026682917959988117i',\n",
       " '9.224505629390478e-05 + 0.006182825658470392i',\n",
       " '-0.00255811819806695 + 0.008887690491974354i',\n",
       " '-0.005583381280303001 + 0.010516894049942493i',\n",
       " '-0.008001899346709251 + 0.011216670274734497i',\n",
       " '-0.009623237885534763 + 0.011028834618628025i',\n",
       " '-0.010439506731927395 + 0.010349860414862633i',\n",
       " '-0.010521123185753822 + 0.009432931430637836i',\n",
       " '-0.009984973818063736 + 0.008512620814144611i',\n",
       " '-0.009221442975103855 + 0.0077358330599963665i',\n",
       " '-0.008523584343492985 + 0.0071199084632098675i',\n",
       " '-0.008222612552344799 + 0.00661357631906867i',\n",
       " '-0.008566230535507202 + 0.006088882219046354i',\n",
       " '-0.009673628024756908 + 0.005384490359574556i',\n",
       " '-0.011464563198387623 + 0.004287904594093561i',\n",
       " '-0.013767202384769917 + 0.002319855149835348i',\n",
       " '-0.015987707301974297 + 0.00024407499586232007i',\n",
       " '-0.01769413612782955 + -0.002085902029648423i',\n",
       " '-0.018719114363193512 + -0.004435406532138586i',\n",
       " '-0.01875944249331951 + -0.006916638929396868i',\n",
       " '-0.018040118739008904 + -0.009207965806126595i',\n",
       " '-0.016181526705622673 + -0.010960560292005539i',\n",
       " '-0.013275926001369953 + -0.011851419694721699i',\n",
       " '-0.010281655006110668 + -0.011817317456007004i',\n",
       " '-0.007480441592633724 + -0.010534883476793766i',\n",
       " '-0.0053315795958042145 + -0.008206897415220737i',\n",
       " '-0.004151209257543087 + -0.005816740449517965i',\n",
       " '-0.003768827533349395 + -0.0033477498218417168i',\n",
       " '-0.004588593728840351 + -0.0007724848110228777i',\n",
       " '-0.006567377597093582 + 0.00156016880646348i',\n",
       " '-0.009759001433849335 + 0.002741842530667782i',\n",
       " '-0.013646969571709633 + 0.0024655789602547884i',\n",
       " '-0.016267070546746254 + 0.002091266680508852i',\n",
       " '-0.017489846795797348 + 0.0010926397517323494i',\n",
       " '-0.01705499365925789 + -0.0006147793028503656i',\n",
       " '-0.014597505331039429 + -0.002637187484651804i',\n",
       " '-0.010624009184539318 + -0.004035553894937038i',\n",
       " '-0.006194876506924629 + -0.005633560009300709i',\n",
       " '-0.0012592833954840899 + -0.008019850589334965i',\n",
       " '0.004261163994669914 + -0.009656530804932117i',\n",
       " '0.009142193011939526 + -0.00990435853600502i',\n",
       " '0.011704565025866032 + -0.009363308548927307i',\n",
       " '0.01199919730424881 + -0.008243545889854431i',\n",
       " '0.00970900896936655 + -0.007133107166737318i',\n",
       " '0.005381963215768337 + -0.0055960011668503284i',\n",
       " '0.001411403063684702 + -0.0033175169955939054i',\n",
       " '-0.002839595777913928 + -0.0011807770933955908i',\n",
       " '-0.006752471439540386 + 6.757074152119458e-05i',\n",
       " '-0.009891973808407784 + -0.0005512314382940531i',\n",
       " '-0.011648673564195633 + -0.0022675020154565573i',\n",
       " '-0.011513583362102509 + -0.004335647448897362i',\n",
       " '-0.009777859784662724 + -0.006749886553734541i',\n",
       " '-0.0068157147616147995 + -0.009342584758996964i',\n",
       " '-0.002760248025879264 + -0.011918695643544197i',\n",
       " '0.0018757875077426434 + -0.01382051408290863i',\n",
       " '0.0057103270664811134 + -0.014367365278303623i',\n",
       " '0.008562886156141758 + -0.013599089346826077i',\n",
       " '0.010243600234389305 + -0.01180333737283945i',\n",
       " '0.01074746623635292 + -0.009287414140999317i',\n",
       " '0.009806680493056774 + -0.006557649001479149i',\n",
       " '0.008134027943015099 + -0.003937769215553999i',\n",
       " '0.006703972816467285 + -0.0014512958005070686i',\n",
       " '0.005260339006781578 + 0.0005198135040700436i',\n",
       " '0.0036840895190835 + 0.0014821989461779594i',\n",
       " '0.002113023307174444 + 0.001253778813406825i',\n",
       " '0.0006279757944867015 + -2.6247667847201228e-05i',\n",
       " '-0.0007380654569715261 + -0.00202773860655725i',\n",
       " '-0.001981314504519105 + -0.004299704451113939i',\n",
       " '-0.0029641648288816214 + -0.006395642179995775i',\n",
       " '-0.0035418744664639235 + -0.008146590553224087i',\n",
       " '-0.00416701752692461 + -0.008916559629142284i',\n",
       " '-0.004836337640881538 + -0.00867679063230753i',\n",
       " '-0.00547159556299448 + -0.007450548466295004i',\n",
       " '-0.005961129441857338 + -0.005634847097098827i',\n",
       " '-0.0059674642980098724 + -0.003867696737870574i',\n",
       " '-0.005559544079005718 + -0.002552757039666176i',\n",
       " '-0.00484891701489687 + -0.0019594349432736635i',\n",
       " '-0.0039009645115584135 + -0.0022271317429840565i',\n",
       " '-0.0027899236883968115 + -0.003267363877967i',\n",
       " '-0.0015780306421220303 + -0.004810701124370098i',\n",
       " '-0.0002495991066098213 + -0.0064081125892698765i',\n",
       " '0.001136621693149209 + -0.007636748719960451i',\n",
       " '0.002769729821011424 + -0.008123457431793213i',\n",
       " '0.00413477560505271 + -0.00760756665840745i',\n",
       " '0.005171430297195911 + -0.006123112980276346i',\n",
       " '0.006344403140246868 + -0.003920929506421089i',\n",
       " '0.007496620528399944 + -0.0012505364138633013i',\n",
       " '0.008318591862916946 + 0.001623180927708745i',\n",
       " '0.008721662685275078 + 0.0036789628211408854i',\n",
       " '0.00862667802721262 + 0.004230264108628035i',\n",
       " '0.008007440716028214 + 0.0027775743510574102i',\n",
       " '0.006662650033831596 + -0.00026499616797082126i',\n",
       " '0.00448935991153121 + -0.004235713742673397i',\n",
       " '0.0023464972618967295 + -0.008762361481785774i',\n",
       " '0.0016754709649831057 + -0.014546739868819714i',\n",
       " '-0.000680978992022574 + -0.01946215145289898i',\n",
       " '-0.0032212187070399523 + -0.022647319361567497i',\n",
       " '-0.005401637405157089 + -0.024268031120300293i',\n",
       " '-0.007067455910146236 + -0.024014055728912354i',\n",
       " '-0.008260965347290039 + -0.022051386535167694i',\n",
       " '-0.008576021529734135 + -0.0185962226241827i',\n",
       " '-0.008465822786092758 + -0.014283617958426476i',\n",
       " '-0.00784135702997446 + -0.009261295199394226i',\n",
       " '-0.00651088822633028 + -0.004364391323179007i',\n",
       " '-0.004347700625658035 + -0.0019141447264701128i',\n",
       " '-0.002155798487365246 + -0.0011361842043697834i',\n",
       " '-0.00027599569875746965 + -0.0017696095164865255i',\n",
       " '0.001199980266392231 + -0.003542254213243723i',\n",
       " '0.002289926167577505 + -0.005988499615341425i',\n",
       " '0.003650766098871827 + -0.00838344544172287i',\n",
       " '0.005219222046434879 + -0.01048962026834488i',\n",
       " '0.006556703709065914 + -0.012012344785034657i',\n",
       " '0.007286214269697666 + -0.012571766972541809i',\n",
       " '0.007642434909939766 + -0.01218929048627615i',\n",
       " '0.007672976702451706 + -0.011056940071284771i',\n",
       " '0.007382014766335487 + -0.009592891670763493i',\n",
       " '0.006964839994907379 + -0.008315281011164188i',\n",
       " '0.006252746097743511 + -0.007303169462829828i',\n",
       " '0.005270165391266346 + -0.006729819346219301i',\n",
       " '0.004078813828527927 + -0.006634008605033159i',\n",
       " '0.0026942852418869734 + -0.0068191844038665295i',\n",
       " '0.0011841228697448969 + -0.007070821709930897i',\n",
       " '-0.0003503162879496813 + -0.007230053190141916i',\n",
       " '-0.0016594205517321825 + -0.007113485597074032i',\n",
       " '-0.002674313960596919 + -0.006472725421190262i',\n",
       " '-0.0032704405020922422 + -0.0052320025861263275i',\n",
       " '-0.0031770782079547644 + -0.0035441003274172544i',\n",
       " '-0.002558002481237054 + -0.0016552393790334463i',\n",
       " '-0.0015409474726766348 + 0.00012780915130861104i',\n",
       " '-0.0003064163029193878 + 0.001477500656619668i',\n",
       " '0.0009464308386668563 + 0.0021213407162576914i',\n",
       " '0.0020408593118190765 + 0.001899241702631116i',\n",
       " '0.0028580783400684595 + 0.0007970000151544809i',\n",
       " '0.0033592930994927883 + -0.0010509388521313667i',\n",
       " '0.0035892464220523834 + -0.0033895750530064106i',\n",
       " '0.0036671950947493315 + -0.0058898054994642735i',\n",
       " '0.004318314604461193 + -0.007939750328660011i',\n",
       " '0.0052372487261891365 + -0.009632534347474575i',\n",
       " '0.0064332373440265656 + -0.010894330218434334i',\n",
       " '0.007542966865003109 + -0.01147072296589613i',\n",
       " '0.008449984714388847 + -0.011462168768048286i',\n",
       " '0.009175309911370277 + -0.011115381494164467i',\n",
       " '0.009590644389390945 + -0.01075349934399128i',\n",
       " '0.009425054304301739 + -0.010589432902634144i',\n",
       " '0.008475719951093197 + -0.010792904533445835i',\n",
       " '0.006656624376773834 + -0.011489524506032467i',\n",
       " '0.004085618536919355 + -0.012515734881162643i',\n",
       " '0.0008873345796018839 + -0.013396572321653366i',\n",
       " '-0.002911760238930583 + -0.014291772618889809i',\n",
       " '-0.006823012605309486 + -0.014323629438877106i',\n",
       " '-0.010579212568700314 + -0.01364173460751772i',\n",
       " '-0.013157886452972889 + -0.012012598104774952i',\n",
       " '-0.015278125181794167 + -0.009425821714103222i',\n",
       " '-0.016042493283748627 + -0.005996968597173691i',\n",
       " '-0.015422804281115532 + -0.001974149839952588i',\n",
       " '-0.01350587047636509 + 0.0022169225849211216i',\n",
       " '-0.010383249260485172 + 0.006018520332872868i',\n",
       " '-0.006896886043250561 + 0.008538291789591312i',\n",
       " '-0.00372252962552011 + 0.009740539826452732i',\n",
       " '-0.0011103153228759766 + 0.00954573042690754i',\n",
       " '0.0007138062501326203 + 0.008266872726380825i',\n",
       " '0.0010040414053946733 + 0.00612971605733037i',\n",
       " '-7.088528946042061e-05 + 0.0037103008944541216i',\n",
       " '-0.001981983194127679 + 0.0012937926221638918i',\n",
       " '-0.00432332418859005 + -0.000794809777289629i',\n",
       " '-0.006582665257155895 + -0.0024400160182267427i',\n",
       " '-0.008261963725090027 + -0.0038580370601266623i',\n",
       " '-0.009216347709298134 + -0.00446912320330739i',\n",
       " '-0.009300422854721546 + -0.004048505332320929i',\n",
       " '-0.008496787399053574 + -0.002754741348326206i',\n",
       " '-0.006875915452837944 + -0.0012344219721853733i',\n",
       " '-0.004758572205901146 + -5.993383820168674e-05i',\n",
       " '-0.0025208538863807917 + 0.0009112062398344278i',\n",
       " '-0.0005668248049914837 + 0.0017682677134871483i',\n",
       " '0.0008323586080223322 + 0.002541491761803627i',\n",
       " '0.0015500250738114119 + 0.003308792831376195i',\n",
       " '0.0016255483496934175 + 0.004163149278610945i',\n",
       " '0.0012501002056524158 + 0.005177308805286884i',\n",
       " '0.0007195472717285156 + 0.006374708842486143i',\n",
       " '0.00021948671201243997 + 0.007750082295387983i',\n",
       " '-0.00015051290392875671 + 0.009156822226941586i',\n",
       " '-0.00011964899022132158 + 0.010655555874109268i',\n",
       " '0.0006633092416450381 + 0.01200628187507391i',\n",
       " '0.002129901433363557 + 0.013032144866883755i',\n",
       " '0.004076707176864147 + 0.0138658182695508i',\n",
       " '0.005977028049528599 + 0.013976991176605225i',\n",
       " '0.007291574031114578 + 0.013295584358274937i',\n",
       " '0.007756446488201618 + 0.012083456851541996i',\n",
       " '0.007299929857254028 + 0.010093853808939457i',\n",
       " '0.005696013569831848 + 0.007741919253021479i',\n",
       " '0.0031679493840783834 + 0.006219681818038225i',\n",
       " '-0.00035895092878490686 + 0.0054487851448357105i',\n",
       " '-0.003927731886506081 + 0.004866139497607946i',\n",
       " '-0.007063128054141998 + 0.00421571871265769i',\n",
       " '-0.009302645921707153 + 0.0032622762955725193i',\n",
       " '-0.00979383010417223 + 0.0025166866835206747i',\n",
       " '-0.008572237566113472 + 0.0018907706253230572i',\n",
       " '-0.0062441155314445496 + 0.0009691709419712424i',\n",
       " '-0.003065794473513961 + -0.0006719615776091814i',\n",
       " '0.0005643574986606836 + -0.002545394003391266i',\n",
       " '0.004079476464539766 + -0.004302381072193384i',\n",
       " '0.007275656796991825 + -0.0051458911038935184i',\n",
       " '0.009774916805326939 + -0.005168331321328878i',\n",
       " '0.01051877811551094 + -0.004627509508281946i',\n",
       " '0.009581518359482288 + -0.0037113113794475794i',\n",
       " '0.007599961943924427 + -0.002441440476104617i',\n",
       " '0.005593577399849892 + -1.9682542188093066e-05i',\n",
       " '0.0034970089327543974 + 0.002773767802864313i',\n",
       " '0.00169494585134089 + 0.00522712292149663i',\n",
       " '0.0004556332132779062 + 0.006877716165035963i',\n",
       " '-6.668316200375557e-05 + 0.007332630921155214i',\n",
       " '0.0002132116351276636 + 0.006385149899870157i',\n",
       " '0.0008871917380020022 + 0.004074361640959978i',\n",
       " '0.0017083568964153528 + 0.0006263877730816603i',\n",
       " '0.0023171757347881794 + -0.0035293411929160357i',\n",
       " '0.002690056338906288 + -0.0076986802741885185i',\n",
       " '0.003839939832687378 + -0.011773845180869102i',\n",
       " '0.0037009038496762514 + -0.0151301184669137i',\n",
       " '0.0018719560466706753 + -0.017051074653863907i',\n",
       " '-0.0006482024909928441 + -0.017712583765387535i',\n",
       " '-0.0035917668137699366 + -0.016967283561825752i',\n",
       " '-0.0062947701662778854 + -0.01461069192737341i',\n",
       " '-0.008584938943386078 + -0.011403627693653107i',\n",
       " '-0.00977398082613945 + -0.0074365148320794106i',\n",
       " '-0.009678567759692669 + -0.0032816294115036726i',\n",
       " '-0.008256837725639343 + 0.0002163945755455643i',\n",
       " '-0.006074205040931702 + 0.0023143154103308916i',\n",
       " '-0.0035436542239040136 + 0.003251716960221529i',\n",
       " '-0.001102421898394823 + 0.0032624939922243357i',\n",
       " '0.0008740088669583201 + 0.002620653249323368i',\n",
       " '0.0020957605447620153 + 0.0016431414987891912i',\n",
       " '0.0024019202683120966 + 0.0006272578611969948i',\n",
       " '0.0017802343936637044 + -0.00018937393906526268i',\n",
       " '0.0003614666638895869 + -0.0006696765776723623i',\n",
       " '-0.0016086772084236145 + -0.0007911704014986753i',\n",
       " '-0.0038135817740112543 + -0.0006320294924080372i',\n",
       " '-0.005905717611312866 + -0.0003686716954689473i',\n",
       " '-0.007574006915092468 + -0.00031691600452177227i',\n",
       " '-0.008630173280835152 + -0.00044486692058853805i',\n",
       " '-0.008913159370422363 + -0.0006082954350858927i',\n",
       " '-0.008369366638362408 + -0.00056478101760149i',\n",
       " '-0.0071188537403941154 + -0.0004969560541212559i',\n",
       " '-0.00532404612749815 + -0.0003288206353317946i',\n",
       " '-0.0031331100035458803 + 2.780908835120499e-05i',\n",
       " '-0.000825934112071991 + 0.0008948745671659708i',\n",
       " '0.0013605919666588306 + 0.0023234314285218716i',\n",
       " '0.003220630344003439 + 0.004242837894707918i',\n",
       " '0.004345607478171587 + 0.006629412528127432i',\n",
       " '0.00445930240675807 + 0.009344887919723988i',\n",
       " '0.003952944651246071 + 0.011702651157975197i',\n",
       " '0.0030749463476240635 + 0.0129085723310709i',\n",
       " '0.0022530383430421352 + 0.01281024795025587i',\n",
       " '0.001772726303897798 + 0.011344061233103275i',\n",
       " '0.0016473435098305345 + 0.008470692671835423i',\n",
       " '0.0006924204062670469 + 0.0049308971501886845i',\n",
       " '-0.0005484211724251509 + 0.0007865037769079208i',\n",
       " '-0.0016060424968600273 + -0.0037188411224633455i',\n",
       " '-0.0019935534801334143 + -0.008279635570943356i',\n",
       " '-0.0008525743614882231 + -0.012895761989057064i',\n",
       " '-0.00022943969815969467 + -0.016219712793827057i',\n",
       " '-0.0002070253249257803 + -0.01800467260181904i',\n",
       " '-0.0004646880552172661 + -0.018521912395954132i',\n",
       " '-0.0010459774639457464 + -0.017819389700889587i',\n",
       " '-0.0020334783475846052 + -0.01604391448199749i',\n",
       " '-0.003369843354448676 + -0.013176115229725838i',\n",
       " '-0.005177909508347511 + -0.009228547103703022i',\n",
       " '-0.006334672681987286 + -0.005283800419420004i',\n",
       " '-0.006825271062552929 + -0.0019359999569132924i',\n",
       " '-0.006934558041393757 + 0.0007417099550366402i',\n",
       " '-0.0067721400409936905 + 0.0027797480579465628i',\n",
       " '-0.00633366871625185 + 0.004065892193466425i',\n",
       " '-0.005696374922990799 + 0.0045771063305437565i',\n",
       " '-0.004972958937287331 + 0.004368626046925783i',\n",
       " '-0.004284117370843887 + 0.003521923441439867i',\n",
       " '-0.003742649918422103 + 0.0021825358271598816i',\n",
       " '-0.003420369466766715 + 0.0004604050482157618i',\n",
       " '-0.0033249191474169493 + -0.0014267063234001398i',\n",
       " '-0.003390297992154956 + -0.003330632345750928i',\n",
       " '-0.0034776281099766493 + -0.005143860820680857i',\n",
       " '-0.0031934615690261126 + -0.00683389650657773i',\n",
       " '-0.0025623824913054705 + -0.00832507573068142i',\n",
       " '-0.0014260539319366217 + -0.009472785517573357i',\n",
       " '0.00015138904564082623 + -0.010235165245831013i',\n",
       " '0.0021752004977315664 + -0.010514716617763042i',\n",
       " '0.004320741631090641 + -0.010343515314161777i',\n",
       " '0.0065049296244978905 + -0.009690566919744015i',\n",
       " '0.008464261889457703 + -0.008890729397535324i',\n",
       " '0.009586472064256668 + -0.007849804125726223i',\n",
       " '0.009579310193657875 + -0.006462988909333944i',\n",
       " '0.008392280898988247 + -0.00491600064560771i',\n",
       " '0.006773845292627811 + -0.0030327083077281713i',\n",
       " '0.005108012817800045 + -0.0004554467450361699i',\n",
       " '0.0032482529059052467 + 0.00249466416426003i',\n",
       " '0.001584462122991681 + 0.0054230038076639175i',\n",
       " '0.0002780963550321758 + 0.008103185333311558i',\n",
       " '-0.0009449010249227285 + 0.010203547775745392i',\n",
       " '-0.0009771997574716806 + 0.011619983240962029i',\n",
       " '0.0003620971692726016 + 0.012060579843819141i',\n",
       " '0.0028582210652530193 + 0.011390875093638897i',\n",
       " '0.005597530864179134 + 0.009526727721095085i',\n",
       " '0.008009138517081738 + 0.006830194499343634i',\n",
       " '0.009657612070441246 + 0.0036027333699166775i',\n",
       " '0.01040506362915039 + 0.00023969708126969635i',\n",
       " '0.010026335716247559 + -0.0028942341450601816i',\n",
       " '0.008446026593446732 + -0.005637899972498417i',\n",
       " '0.006013990379869938 + -0.007152949925512075i',\n",
       " '0.0031324271112680435 + -0.007655995432287455i',\n",
       " '0.0006014280952513218 + -0.006872494239360094i',\n",
       " '-0.001035212306305766 + -0.005265489686280489i',\n",
       " '-0.001541690668091178 + -0.003126024967059493i',\n",
       " '-0.0009057533461600542 + -0.0009374963119626045i',\n",
       " '0.0007455756422132254 + 0.0009042031597346067i',\n",
       " '0.0030816877260804176 + 0.002086666878312826i',\n",
       " '0.005628155544400215 + 0.00243803346529603i',\n",
       " '0.007791714742779732 + 0.002281468827277422i',\n",
       " '0.00929840374737978 + 0.0016090238932520151i',\n",
       " '0.009932683780789375 + 4.3802690925076604e-05i',\n",
       " '0.009361636824905872 + -0.002024544170126319i',\n",
       " '0.00740775465965271 + -0.0041849249973893166i',\n",
       " '0.004612641409039497 + -0.0057709370739758015i',\n",
       " '0.001637408509850502 + -0.006543826311826706i',\n",
       " '-0.0012554503045976162 + -0.006638044957071543i',\n",
       " '-0.003585012862458825 + -0.0063429041765630245i',\n",
       " '-0.005263924598693848 + -0.005593337118625641i',\n",
       " '-0.006139905191957951 + -0.004456805530935526i',\n",
       " '-0.006301545538008213 + -0.0031208167783915997i',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = []\n",
    "\n",
    "for i in range(len(tf_output)):\n",
    "    arr.append(f'{tf_output[i][0]} + {tf_output[i][1]}i')\n",
    "\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(arr).to_csv('tf_model_output_test.csv',index=False,header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "# Function to parse complex numbers from the given format\n",
    "def parse_complex_number(s):\n",
    "    real, imag = s.split(' + ')\n",
    "    real = float(real)\n",
    "    imag = float(imag.replace('i', ''))\n",
    "    return np.complex64(real + 1j * imag)\n",
    "\n",
    "# Read the CSV file\n",
    "filename = './tf_model_output_test.csv'  # Replace with your CSV filename\n",
    "with open(filename, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Parse the lines into complex numbers\n",
    "complex_numbers = np.array([parse_complex_number(line.strip()) for line in lines], dtype=np.complex64)\n",
    "\n",
    "# Save the array to a .mat file\n",
    "output_filename = 'complex_data.mat'\n",
    "scipy.io.savemat(output_filename, {'complex_data': complex_numbers})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
