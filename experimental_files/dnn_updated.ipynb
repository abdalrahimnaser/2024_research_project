{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Input, Normalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Processing Input Data after extraction from the .mlx file script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.txt')\n",
    "df_val = pd.read_csv('val.txt')\n",
    "df_test = pd.read_csv('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "df_train['X_real'] = df_train['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_train['X_img'] = df_train['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "df_train['y_real'] = df_train['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_train['y_img'] = df_train['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "\n",
    "# test\n",
    "df_test['X_real'] = df_test['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_test['X_img'] = df_test['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "df_test['y_real'] = df_test['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_test['y_img'] = df_test['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "\n",
    "# val\n",
    "df_val['X_real'] = df_val['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_val['X_img'] = df_val['Var1_2'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)\n",
    "df_val['y_real'] = df_val['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)').astype(float)\n",
    "df_val['y_img'] = df_val['Var1_1'].str.extract(r'([+-]?\\d+\\.\\d+(?:[eE][+-]?\\d+)?)i').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[['X_real','X_img']].to_numpy()\n",
    "y_train = df_train[['y_real','y_img']].to_numpy()\n",
    "\n",
    "X_test = df_test[['X_real','X_img']].to_numpy()\n",
    "y_test = df_test[['y_real','y_img']].to_numpy()\n",
    "\n",
    "X_val = df_val[['X_real','X_img']].to_numpy()\n",
    "y_val = df_val[['y_real','y_img']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00150483,  0.00166845],\n",
       "       [ 0.00151624,  0.00048847],\n",
       "       [ 0.00165905, -0.00093272],\n",
       "       ...,\n",
       "       [ 0.00218926,  0.00537777],\n",
       "       [ 0.00462811,  0.00423432],\n",
       "       [ 0.00672882,  0.00265553]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(arr, n_previous = 4):\n",
    "    array_with_l1_norm  = np.hstack((arr, np.sum(np.abs(arr),axis=1)[:, np.newaxis]))\n",
    "\n",
    "    padding = np.zeros((n_previous, array_with_l1_norm.shape[1]))\n",
    "    padded_array = np.vstack((padding, array_with_l1_norm ))\n",
    "\n",
    "    windows = sliding_window_view(padded_array, window_shape=(n_previous + 1, array_with_l1_norm.shape[1]))\n",
    "    windows = windows.reshape(windows.shape[0], -1)\n",
    "    \n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess(X_train)\n",
    "X_test = preprocess(X_test)\n",
    "X_val = preprocess(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131520, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "\n",
    "# x_min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# y_min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# X_train = x_min_max_scaler.fit_transform(X_train)\n",
    "# y_train = y_min_max_scaler.fit_transform(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val = x_min_max_scaler.transform(X_val)\n",
    "# y_val = y_min_max_scaler.transform(y_val)\n",
    "\n",
    "# X_test = x_min_max_scaler.transform(X_test)\n",
    "# y_test = y_min_max_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the model using Sequential API\n",
    "# model = Sequential([\n",
    "#     Dense(30),\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     Dense(30),\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     Dense(30),\n",
    "#     LeakyReLU(alpha=0.01),\n",
    "#     Dense(2)\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=Adam(learning_rate=4e-4), \n",
    "#               loss='mean_squared_error', \n",
    "#               metrics=['mean_squared_error'])\n",
    "\n",
    "# # Define training parameters\n",
    "# maxEpochs = 200\n",
    "# miniBatchSize = 1024\n",
    "# iterPerEpoch = len(X_train) // miniBatchSize\n",
    "# validation_freq = 2 * iterPerEpoch\n",
    "\n",
    "# # Callbacks for learning rate adjustment, early stopping, and model checkpoint\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=5, verbose=1, mode='auto')\n",
    "# # early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     epochs=maxEpochs,\n",
    "#     batch_size=miniBatchSize,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     callbacks=[reduce_lr],\n",
    "#     shuffle=True,\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helperNMSE(y_true, y_pred):\n",
    "    \n",
    "    diff = y_pred - y_true\n",
    "    mse = tf.reduce_mean(tf.norm(diff,axis=1)**2) # NOTE THIS IS NOT A GOOD PRACICE AS TF.NORM ALREADY GETS THE SQUARED ERROR THEN SQRT IT\n",
    "    factor = tf.reduce_mean(tf.norm(y_true,axis=1)**2)\n",
    "    # nmse = 10 * tf.math.log(mse / factor) / tf.math.log(tf.constant(10,dtype=tf.float32))\n",
    "    \n",
    "    return mse\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Compute the L1 norms\n",
    "    norm_true = tf.reduce_sum(tf.abs(y_true), axis=1)\n",
    "    norm_pred = tf.reduce_sum(tf.abs(y_pred), axis=1)\n",
    "    \n",
    "    # Reshape to be compatible with the helperNMSE function\n",
    "    norm_true = tf.reshape(norm_true, (-1, 1))\n",
    "    norm_pred = tf.reshape(norm_pred, (-1, 1))\n",
    "    \n",
    "    # Compute the NMSE\n",
    "    loss = helperNMSE(norm_true, norm_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a  = tf.convert_to_tensor([[2,1],[5,6]],dtype=tf.float32)\n",
    "# b = tf.convert_to_tensor([[2,5],[2,4]],dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helperNMSE(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abood\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# defining layers\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "dense_layer_1 = Dense(units = 30, activation = LeakyReLU(alpha=0.01))(input_layer) \n",
    "dense_layer_2 = Dense(units = 24, activation = LeakyReLU(alpha=0.01))(dense_layer_1)\n",
    "dense_layer_3 = Dense(units = 19, activation = LeakyReLU(alpha=0.01))(dense_layer_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Y1 output\n",
    "y_output = Dense(units = 2, activation = \"linear\", name = \"y1_output\")(dense_layer_3)\n",
    "\n",
    "#Y2 output\n",
    "\n",
    "#Define the model with the input layer and a list of outputs\n",
    "model = Model(inputs = input_layer, outputs = [y_output])\n",
    "\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=5, verbose=1, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='bestmodel.keras',\n",
    "    monitor='val_loss',       # Metric to monitor\n",
    "    save_best_only=True,      # Only save the model if it is the best\n",
    "    verbose=0                 # Verbosity mode\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#specify the optimizer and compile with the loss function for both outputs\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=4e-4)\n",
    "\n",
    "model.compile(optimizer = optimizer,\n",
    "              loss = 'mse',\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('tf_model_33_5dB_nprev_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.0862e-05 - val_loss: 1.0797e-06 - learning_rate: 4.0000e-04\n",
      "Epoch 2/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.4882e-07 - val_loss: 5.2731e-07 - learning_rate: 4.0000e-04\n",
      "Epoch 3/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1766e-07 - val_loss: 4.1023e-07 - learning_rate: 4.0000e-04\n",
      "Epoch 4/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9616e-07 - val_loss: 3.6466e-07 - learning_rate: 4.0000e-04\n",
      "Epoch 5/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3540e-07 - val_loss: 2.8090e-07 - learning_rate: 4.0000e-04\n",
      "Epoch 6/700\n",
      "\u001b[1m 73/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 2.9063e-07\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00037999999040039256.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8394e-07 - val_loss: 2.7088e-07 - learning_rate: 4.0000e-04\n",
      "Epoch 7/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3817e-07 - val_loss: 2.0127e-07 - learning_rate: 3.8000e-04\n",
      "Epoch 8/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.0068e-07 - val_loss: 1.8144e-07 - learning_rate: 3.8000e-04\n",
      "Epoch 9/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.8715e-07 - val_loss: 1.8628e-07 - learning_rate: 3.8000e-04\n",
      "Epoch 10/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.6146e-07 - val_loss: 1.6492e-07 - learning_rate: 3.8000e-04\n",
      "Epoch 11/700\n",
      "\u001b[1m 69/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 1.5773e-07\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0003609999839682132.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.5646e-07 - val_loss: 1.3278e-07 - learning_rate: 3.8000e-04\n",
      "Epoch 12/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3213e-07 - val_loss: 1.3505e-07 - learning_rate: 3.6100e-04\n",
      "Epoch 13/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2095e-07 - val_loss: 1.0740e-07 - learning_rate: 3.6100e-04\n",
      "Epoch 14/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1995e-07 - val_loss: 1.1934e-07 - learning_rate: 3.6100e-04\n",
      "Epoch 15/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1047e-07 - val_loss: 1.0590e-07 - learning_rate: 3.6100e-04\n",
      "Epoch 16/700\n",
      "\u001b[1m 72/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 1.0932e-07\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.00034294998476980254.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0785e-07 - val_loss: 1.0165e-07 - learning_rate: 3.6100e-04\n",
      "Epoch 17/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.6474e-08 - val_loss: 1.0327e-07 - learning_rate: 3.4295e-04\n",
      "Epoch 18/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.1683e-08 - val_loss: 9.2648e-08 - learning_rate: 3.4295e-04\n",
      "Epoch 19/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.7274e-08 - val_loss: 9.8709e-08 - learning_rate: 3.4295e-04\n",
      "Epoch 20/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.7541e-08 - val_loss: 9.2523e-08 - learning_rate: 3.4295e-04\n",
      "Epoch 21/700\n",
      "\u001b[1m 73/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 8.0539e-08\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00032580249244347216.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.0232e-08 - val_loss: 1.1458e-07 - learning_rate: 3.4295e-04\n",
      "Epoch 22/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.0742e-08 - val_loss: 7.4352e-08 - learning_rate: 3.2580e-04\n",
      "Epoch 23/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.0591e-08 - val_loss: 9.4625e-08 - learning_rate: 3.2580e-04\n",
      "Epoch 24/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.9849e-08 - val_loss: 7.0646e-08 - learning_rate: 3.2580e-04\n",
      "Epoch 25/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.9972e-08 - val_loss: 6.2047e-08 - learning_rate: 3.2580e-04\n",
      "Epoch 26/700\n",
      "\u001b[1m 74/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 6.9462e-08\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00030951235676184296.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.9332e-08 - val_loss: 7.1439e-08 - learning_rate: 3.2580e-04\n",
      "Epoch 27/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.0905e-08 - val_loss: 7.0094e-08 - learning_rate: 3.0951e-04\n",
      "Epoch 28/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.9298e-08 - val_loss: 5.7969e-08 - learning_rate: 3.0951e-04\n",
      "Epoch 29/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.5853e-08 - val_loss: 5.7796e-08 - learning_rate: 3.0951e-04\n",
      "Epoch 30/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.2810e-08 - val_loss: 5.7954e-08 - learning_rate: 3.0951e-04\n",
      "Epoch 31/700\n",
      "\u001b[1m 72/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 6.2525e-08\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00029403674998320636.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.2323e-08 - val_loss: 5.5010e-08 - learning_rate: 3.0951e-04\n",
      "Epoch 32/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8591e-08 - val_loss: 6.1554e-08 - learning_rate: 2.9404e-04\n",
      "Epoch 33/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.1967e-08 - val_loss: 7.4318e-08 - learning_rate: 2.9404e-04\n",
      "Epoch 34/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.3917e-08 - val_loss: 6.4405e-08 - learning_rate: 2.9404e-04\n",
      "Epoch 35/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9585e-08 - val_loss: 6.7911e-08 - learning_rate: 2.9404e-04\n",
      "Epoch 36/700\n",
      "\u001b[1m 71/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 6.5713e-08\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.00027933491801377384.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.3171e-08 - val_loss: 5.6381e-08 - learning_rate: 2.9404e-04\n",
      "Epoch 37/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.5937e-08 - val_loss: 5.6718e-08 - learning_rate: 2.7933e-04\n",
      "Epoch 38/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6150e-08 - val_loss: 5.2270e-08 - learning_rate: 2.7933e-04\n",
      "Epoch 39/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4994e-08 - val_loss: 5.5472e-08 - learning_rate: 2.7933e-04\n",
      "Epoch 40/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4708e-08 - val_loss: 6.6674e-08 - learning_rate: 2.7933e-04\n",
      "Epoch 41/700\n",
      "\u001b[1m 56/129\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 5.8781e-08\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00026536818040767685.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7722e-08 - val_loss: 5.1313e-08 - learning_rate: 2.7933e-04\n",
      "Epoch 42/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2395e-08 - val_loss: 5.2666e-08 - learning_rate: 2.6537e-04\n",
      "Epoch 43/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4408e-08 - val_loss: 4.9639e-08 - learning_rate: 2.6537e-04\n",
      "Epoch 44/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4348e-08 - val_loss: 4.9406e-08 - learning_rate: 2.6537e-04\n",
      "Epoch 45/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0778e-08 - val_loss: 4.6316e-08 - learning_rate: 2.6537e-04\n",
      "Epoch 46/700\n",
      "\u001b[1m 75/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 4.8632e-08\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.00025209976447513325.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9474e-08 - val_loss: 5.8357e-08 - learning_rate: 2.6537e-04\n",
      "Epoch 47/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.1241e-08 - val_loss: 4.9841e-08 - learning_rate: 2.5210e-04\n",
      "Epoch 48/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8334e-08 - val_loss: 4.7201e-08 - learning_rate: 2.5210e-04\n",
      "Epoch 49/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6836e-08 - val_loss: 4.4083e-08 - learning_rate: 2.5210e-04\n",
      "Epoch 50/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9024e-08 - val_loss: 6.7417e-08 - learning_rate: 2.5210e-04\n",
      "Epoch 51/700\n",
      "\u001b[1m 73/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 5.7604e-08\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.00023949477763380854.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.4777e-08 - val_loss: 4.5771e-08 - learning_rate: 2.5210e-04\n",
      "Epoch 52/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6292e-08 - val_loss: 4.8223e-08 - learning_rate: 2.3949e-04\n",
      "Epoch 53/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8832e-08 - val_loss: 4.5275e-08 - learning_rate: 2.3949e-04\n",
      "Epoch 54/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6125e-08 - val_loss: 4.4847e-08 - learning_rate: 2.3949e-04\n",
      "Epoch 55/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5193e-08 - val_loss: 4.2263e-08 - learning_rate: 2.3949e-04\n",
      "Epoch 56/700\n",
      "\u001b[1m 70/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 739us/step - loss: 4.8661e-08\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.000227520041516982.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9125e-08 - val_loss: 4.2023e-08 - learning_rate: 2.3949e-04\n",
      "Epoch 57/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4184e-08 - val_loss: 4.1981e-08 - learning_rate: 2.2752e-04\n",
      "Epoch 58/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3569e-08 - val_loss: 4.8512e-08 - learning_rate: 2.2752e-04\n",
      "Epoch 59/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7135e-08 - val_loss: 4.0590e-08 - learning_rate: 2.2752e-04\n",
      "Epoch 60/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5882e-08 - val_loss: 4.2913e-08 - learning_rate: 2.2752e-04\n",
      "Epoch 61/700\n",
      "\u001b[1m 73/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 4.5002e-08\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.00021614403667626902.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5322e-08 - val_loss: 4.0776e-08 - learning_rate: 2.2752e-04\n",
      "Epoch 62/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2830e-08 - val_loss: 4.6957e-08 - learning_rate: 2.1614e-04\n",
      "Epoch 63/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3064e-08 - val_loss: 4.5609e-08 - learning_rate: 2.1614e-04\n",
      "Epoch 64/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2275e-08 - val_loss: 3.9003e-08 - learning_rate: 2.1614e-04\n",
      "Epoch 65/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1863e-08 - val_loss: 5.0016e-08 - learning_rate: 2.1614e-04\n",
      "Epoch 66/700\n",
      "\u001b[1m 67/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 4.4959e-08\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 0.00020533683346002363.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5115e-08 - val_loss: 3.9017e-08 - learning_rate: 2.1614e-04\n",
      "Epoch 67/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0982e-08 - val_loss: 4.2397e-08 - learning_rate: 2.0534e-04\n",
      "Epoch 68/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3747e-08 - val_loss: 4.1964e-08 - learning_rate: 2.0534e-04\n",
      "Epoch 69/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1346e-08 - val_loss: 3.9108e-08 - learning_rate: 2.0534e-04\n",
      "Epoch 70/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0887e-08 - val_loss: 3.9415e-08 - learning_rate: 2.0534e-04\n",
      "Epoch 71/700\n",
      "\u001b[1m 75/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 4.3662e-08\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0001950699952431023.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2920e-08 - val_loss: 4.5332e-08 - learning_rate: 2.0534e-04\n",
      "Epoch 72/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0373e-08 - val_loss: 3.6998e-08 - learning_rate: 1.9507e-04\n",
      "Epoch 73/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0171e-08 - val_loss: 3.8384e-08 - learning_rate: 1.9507e-04\n",
      "Epoch 74/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8950e-08 - val_loss: 3.9152e-08 - learning_rate: 1.9507e-04\n",
      "Epoch 75/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1347e-08 - val_loss: 4.3887e-08 - learning_rate: 1.9507e-04\n",
      "Epoch 76/700\n",
      "\u001b[1m 73/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 4.1513e-08\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0001853164954809472.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0426e-08 - val_loss: 3.6997e-08 - learning_rate: 1.9507e-04\n",
      "Epoch 77/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8711e-08 - val_loss: 3.5973e-08 - learning_rate: 1.8532e-04\n",
      "Epoch 78/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8010e-08 - val_loss: 3.7401e-08 - learning_rate: 1.8532e-04\n",
      "Epoch 79/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9463e-08 - val_loss: 3.5551e-08 - learning_rate: 1.8532e-04\n",
      "Epoch 80/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8694e-08 - val_loss: 3.6309e-08 - learning_rate: 1.8532e-04\n",
      "Epoch 81/700\n",
      "\u001b[1m 73/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 3.7710e-08\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 0.00017605067623662762.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8033e-08 - val_loss: 3.8434e-08 - learning_rate: 1.8532e-04\n",
      "Epoch 82/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9583e-08 - val_loss: 3.6371e-08 - learning_rate: 1.7605e-04\n",
      "Epoch 83/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6944e-08 - val_loss: 3.6477e-08 - learning_rate: 1.7605e-04\n",
      "Epoch 84/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7847e-08 - val_loss: 4.4042e-08 - learning_rate: 1.7605e-04\n",
      "Epoch 85/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7348e-08 - val_loss: 3.5414e-08 - learning_rate: 1.7605e-04\n",
      "Epoch 86/700\n",
      "\u001b[1m 68/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 3.6732e-08\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 0.00016724813758628443.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7052e-08 - val_loss: 3.7316e-08 - learning_rate: 1.7605e-04\n",
      "Epoch 87/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7100e-08 - val_loss: 3.4398e-08 - learning_rate: 1.6725e-04\n",
      "Epoch 88/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8116e-08 - val_loss: 3.6110e-08 - learning_rate: 1.6725e-04\n",
      "Epoch 89/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8411e-08 - val_loss: 3.6996e-08 - learning_rate: 1.6725e-04\n",
      "Epoch 90/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6529e-08 - val_loss: 3.4075e-08 - learning_rate: 1.6725e-04\n",
      "Epoch 91/700\n",
      "\u001b[1m 65/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 792us/step - loss: 3.6972e-08\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 0.00015888572379481046.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6736e-08 - val_loss: 3.5474e-08 - learning_rate: 1.6725e-04\n",
      "Epoch 92/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7170e-08 - val_loss: 3.3699e-08 - learning_rate: 1.5889e-04\n",
      "Epoch 93/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4842e-08 - val_loss: 3.6773e-08 - learning_rate: 1.5889e-04\n",
      "Epoch 94/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5966e-08 - val_loss: 3.3942e-08 - learning_rate: 1.5889e-04\n",
      "Epoch 95/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6576e-08 - val_loss: 3.3466e-08 - learning_rate: 1.5889e-04\n",
      "Epoch 96/700\n",
      "\u001b[1m 74/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 3.4979e-08\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 0.00015094144036993383.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5208e-08 - val_loss: 3.6256e-08 - learning_rate: 1.5889e-04\n",
      "Epoch 97/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6028e-08 - val_loss: 3.4218e-08 - learning_rate: 1.5094e-04\n",
      "Epoch 98/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4790e-08 - val_loss: 3.3806e-08 - learning_rate: 1.5094e-04\n",
      "Epoch 99/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4957e-08 - val_loss: 3.9417e-08 - learning_rate: 1.5094e-04\n",
      "Epoch 100/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5894e-08 - val_loss: 3.2648e-08 - learning_rate: 1.5094e-04\n",
      "Epoch 101/700\n",
      "\u001b[1m 75/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 3.5484e-08\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 0.00014339437111630105.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5713e-08 - val_loss: 3.4942e-08 - learning_rate: 1.5094e-04\n",
      "Epoch 102/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5430e-08 - val_loss: 3.2190e-08 - learning_rate: 1.4339e-04\n",
      "Epoch 103/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4014e-08 - val_loss: 3.4973e-08 - learning_rate: 1.4339e-04\n",
      "Epoch 104/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5418e-08 - val_loss: 3.4144e-08 - learning_rate: 1.4339e-04\n",
      "Epoch 105/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3919e-08 - val_loss: 3.2347e-08 - learning_rate: 1.4339e-04\n",
      "Epoch 106/700\n",
      "\u001b[1m 77/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 3.5543e-08\n",
      "Epoch 106: ReduceLROnPlateau reducing learning rate to 0.00013622465048683806.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5325e-08 - val_loss: 3.4474e-08 - learning_rate: 1.4339e-04\n",
      "Epoch 107/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4377e-08 - val_loss: 3.2109e-08 - learning_rate: 1.3622e-04\n",
      "Epoch 108/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4471e-08 - val_loss: 3.6450e-08 - learning_rate: 1.3622e-04\n",
      "Epoch 109/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3117e-08 - val_loss: 3.2920e-08 - learning_rate: 1.3622e-04\n",
      "Epoch 110/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3840e-08 - val_loss: 3.2532e-08 - learning_rate: 1.3622e-04\n",
      "Epoch 111/700\n",
      "\u001b[1m 67/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 3.2911e-08\n",
      "Epoch 111: ReduceLROnPlateau reducing learning rate to 0.00012941342210979201.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3239e-08 - val_loss: 4.2644e-08 - learning_rate: 1.3622e-04\n",
      "Epoch 112/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5027e-08 - val_loss: 3.2295e-08 - learning_rate: 1.2941e-04\n",
      "Epoch 113/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2980e-08 - val_loss: 3.1083e-08 - learning_rate: 1.2941e-04\n",
      "Epoch 114/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2659e-08 - val_loss: 3.1731e-08 - learning_rate: 1.2941e-04\n",
      "Epoch 115/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2412e-08 - val_loss: 3.0953e-08 - learning_rate: 1.2941e-04\n",
      "Epoch 116/700\n",
      "\u001b[1m117/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 3.3289e-08\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 0.00012294275584281422.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3263e-08 - val_loss: 3.0916e-08 - learning_rate: 1.2941e-04\n",
      "Epoch 117/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1975e-08 - val_loss: 3.0736e-08 - learning_rate: 1.2294e-04\n",
      "Epoch 118/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2328e-08 - val_loss: 3.1463e-08 - learning_rate: 1.2294e-04\n",
      "Epoch 119/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2326e-08 - val_loss: 3.0788e-08 - learning_rate: 1.2294e-04\n",
      "Epoch 120/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1737e-08 - val_loss: 3.8023e-08 - learning_rate: 1.2294e-04\n",
      "Epoch 121/700\n",
      "\u001b[1m 71/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 3.4122e-08\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 0.00011679562012432142.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3343e-08 - val_loss: 3.1147e-08 - learning_rate: 1.2294e-04\n",
      "Epoch 122/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1723e-08 - val_loss: 3.7274e-08 - learning_rate: 1.1680e-04\n",
      "Epoch 123/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2130e-08 - val_loss: 3.0949e-08 - learning_rate: 1.1680e-04\n",
      "Epoch 124/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1146e-08 - val_loss: 3.0688e-08 - learning_rate: 1.1680e-04\n",
      "Epoch 125/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2433e-08 - val_loss: 3.2426e-08 - learning_rate: 1.1680e-04\n",
      "Epoch 126/700\n",
      "\u001b[1m 72/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 3.2018e-08\n",
      "Epoch 126: ReduceLROnPlateau reducing learning rate to 0.0001109558405005373.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1906e-08 - val_loss: 3.0542e-08 - learning_rate: 1.1680e-04\n",
      "Epoch 127/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2449e-08 - val_loss: 3.0631e-08 - learning_rate: 1.1096e-04\n",
      "Epoch 128/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2118e-08 - val_loss: 3.0138e-08 - learning_rate: 1.1096e-04\n",
      "Epoch 129/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0655e-08 - val_loss: 3.2375e-08 - learning_rate: 1.1096e-04\n",
      "Epoch 130/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1018e-08 - val_loss: 2.9693e-08 - learning_rate: 1.1096e-04\n",
      "Epoch 131/700\n",
      "\u001b[1m 70/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 3.0500e-08\n",
      "Epoch 131: ReduceLROnPlateau reducing learning rate to 0.00010540805124037433.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0737e-08 - val_loss: 3.5541e-08 - learning_rate: 1.1096e-04\n",
      "Epoch 132/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1138e-08 - val_loss: 2.9669e-08 - learning_rate: 1.0541e-04\n",
      "Epoch 133/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0474e-08 - val_loss: 2.9565e-08 - learning_rate: 1.0541e-04\n",
      "Epoch 134/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0842e-08 - val_loss: 2.9359e-08 - learning_rate: 1.0541e-04\n",
      "Epoch 135/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0760e-08 - val_loss: 3.0073e-08 - learning_rate: 1.0541e-04\n",
      "Epoch 136/700\n",
      "\u001b[1m 75/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 3.0722e-08\n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 0.00010013764695031567.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0821e-08 - val_loss: 3.0123e-08 - learning_rate: 1.0541e-04\n",
      "Epoch 137/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1346e-08 - val_loss: 2.9396e-08 - learning_rate: 1.0014e-04\n",
      "Epoch 138/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1009e-08 - val_loss: 2.9574e-08 - learning_rate: 1.0014e-04\n",
      "Epoch 139/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0250e-08 - val_loss: 3.0600e-08 - learning_rate: 1.0014e-04\n",
      "Epoch 140/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0440e-08 - val_loss: 2.9121e-08 - learning_rate: 1.0014e-04\n",
      "Epoch 141/700\n",
      "\u001b[1m 68/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 2.9780e-08\n",
      "Epoch 141: ReduceLROnPlateau reducing learning rate to 9.5130761837936e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0351e-08 - val_loss: 2.9146e-08 - learning_rate: 1.0014e-04\n",
      "Epoch 142/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9871e-08 - val_loss: 2.8976e-08 - learning_rate: 9.5131e-05\n",
      "Epoch 143/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0204e-08 - val_loss: 3.2515e-08 - learning_rate: 9.5131e-05\n",
      "Epoch 144/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9811e-08 - val_loss: 2.8956e-08 - learning_rate: 9.5131e-05\n",
      "Epoch 145/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1013e-08 - val_loss: 3.4356e-08 - learning_rate: 9.5131e-05\n",
      "Epoch 146/700\n",
      "\u001b[1m119/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 3.0629e-08\n",
      "Epoch 146: ReduceLROnPlateau reducing learning rate to 9.037422132678329e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0605e-08 - val_loss: 2.9032e-08 - learning_rate: 9.5131e-05\n",
      "Epoch 147/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9474e-08 - val_loss: 2.8746e-08 - learning_rate: 9.0374e-05\n",
      "Epoch 148/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9694e-08 - val_loss: 3.0552e-08 - learning_rate: 9.0374e-05\n",
      "Epoch 149/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0246e-08 - val_loss: 2.8931e-08 - learning_rate: 9.0374e-05\n",
      "Epoch 150/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9326e-08 - val_loss: 2.8861e-08 - learning_rate: 9.0374e-05\n",
      "Epoch 151/700\n",
      "\u001b[1m 70/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 3.0268e-08\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 8.585550749558024e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0221e-08 - val_loss: 2.9090e-08 - learning_rate: 9.0374e-05\n",
      "Epoch 152/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9715e-08 - val_loss: 2.8689e-08 - learning_rate: 8.5856e-05\n",
      "Epoch 153/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9296e-08 - val_loss: 2.9324e-08 - learning_rate: 8.5856e-05\n",
      "Epoch 154/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9389e-08 - val_loss: 2.8422e-08 - learning_rate: 8.5856e-05\n",
      "Epoch 155/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.0859e-08 - val_loss: 2.9109e-08 - learning_rate: 8.5856e-05\n",
      "Epoch 156/700\n",
      "\u001b[1m 69/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 2.9988e-08\n",
      "Epoch 156: ReduceLROnPlateau reducing learning rate to 8.156273142958525e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9960e-08 - val_loss: 2.8497e-08 - learning_rate: 8.5856e-05\n",
      "Epoch 157/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8826e-08 - val_loss: 2.9666e-08 - learning_rate: 8.1563e-05\n",
      "Epoch 158/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9875e-08 - val_loss: 2.8091e-08 - learning_rate: 8.1563e-05\n",
      "Epoch 159/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8979e-08 - val_loss: 3.0305e-08 - learning_rate: 8.1563e-05\n",
      "Epoch 160/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9663e-08 - val_loss: 3.0838e-08 - learning_rate: 8.1563e-05\n",
      "Epoch 161/700\n",
      "\u001b[1m 74/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 2.9383e-08\n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 7.748459174763411e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9440e-08 - val_loss: 2.8733e-08 - learning_rate: 8.1563e-05\n",
      "Epoch 162/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9055e-08 - val_loss: 2.8591e-08 - learning_rate: 7.7485e-05\n",
      "Epoch 163/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9103e-08 - val_loss: 2.8519e-08 - learning_rate: 7.7485e-05\n",
      "Epoch 164/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9705e-08 - val_loss: 2.8055e-08 - learning_rate: 7.7485e-05\n",
      "Epoch 165/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8412e-08 - val_loss: 2.7925e-08 - learning_rate: 7.7485e-05\n",
      "Epoch 166/700\n",
      "\u001b[1m 66/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 2.8516e-08\n",
      "Epoch 166: ReduceLROnPlateau reducing learning rate to 7.361036077782046e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8449e-08 - val_loss: 3.1296e-08 - learning_rate: 7.7485e-05\n",
      "Epoch 167/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8969e-08 - val_loss: 2.7706e-08 - learning_rate: 7.3610e-05\n",
      "Epoch 168/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8573e-08 - val_loss: 2.9882e-08 - learning_rate: 7.3610e-05\n",
      "Epoch 169/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9080e-08 - val_loss: 3.0141e-08 - learning_rate: 7.3610e-05\n",
      "Epoch 170/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9822e-08 - val_loss: 2.7734e-08 - learning_rate: 7.3610e-05\n",
      "Epoch 171/700\n",
      "\u001b[1m 71/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 2.8236e-08\n",
      "Epoch 171: ReduceLROnPlateau reducing learning rate to 6.992984308453742e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8339e-08 - val_loss: 2.8115e-08 - learning_rate: 7.3610e-05\n",
      "Epoch 172/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8503e-08 - val_loss: 2.7848e-08 - learning_rate: 6.9930e-05\n",
      "Epoch 173/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7948e-08 - val_loss: 2.8722e-08 - learning_rate: 6.9930e-05\n",
      "Epoch 174/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8373e-08 - val_loss: 2.7722e-08 - learning_rate: 6.9930e-05\n",
      "Epoch 175/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8106e-08 - val_loss: 2.7585e-08 - learning_rate: 6.9930e-05\n",
      "Epoch 176/700\n",
      "\u001b[1m 77/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 2.7799e-08\n",
      "Epoch 176: ReduceLROnPlateau reducing learning rate to 6.643334781983866e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7888e-08 - val_loss: 2.7287e-08 - learning_rate: 6.9930e-05\n",
      "Epoch 177/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8189e-08 - val_loss: 2.9676e-08 - learning_rate: 6.6433e-05\n",
      "Epoch 178/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8435e-08 - val_loss: 2.9127e-08 - learning_rate: 6.6433e-05\n",
      "Epoch 179/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8438e-08 - val_loss: 2.8279e-08 - learning_rate: 6.6433e-05\n",
      "Epoch 180/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7945e-08 - val_loss: 2.7838e-08 - learning_rate: 6.6433e-05\n",
      "Epoch 181/700\n",
      "\u001b[1m 59/129\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 2.7861e-08\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 6.311168181127869e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7907e-08 - val_loss: 2.9595e-08 - learning_rate: 6.6433e-05\n",
      "Epoch 182/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8069e-08 - val_loss: 2.9011e-08 - learning_rate: 6.3112e-05\n",
      "Epoch 183/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7681e-08 - val_loss: 2.8442e-08 - learning_rate: 6.3112e-05\n",
      "Epoch 184/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7971e-08 - val_loss: 2.7142e-08 - learning_rate: 6.3112e-05\n",
      "Epoch 185/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7501e-08 - val_loss: 2.7842e-08 - learning_rate: 6.3112e-05\n",
      "Epoch 186/700\n",
      "\u001b[1m 70/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 2.7971e-08\n",
      "Epoch 186: ReduceLROnPlateau reducing learning rate to 5.995610117679462e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7832e-08 - val_loss: 2.6914e-08 - learning_rate: 6.3112e-05\n",
      "Epoch 187/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8088e-08 - val_loss: 2.7777e-08 - learning_rate: 5.9956e-05\n",
      "Epoch 188/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8016e-08 - val_loss: 2.8142e-08 - learning_rate: 5.9956e-05\n",
      "Epoch 189/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7904e-08 - val_loss: 2.8008e-08 - learning_rate: 5.9956e-05\n",
      "Epoch 190/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7667e-08 - val_loss: 2.7055e-08 - learning_rate: 5.9956e-05\n",
      "Epoch 191/700\n",
      "\u001b[1m 69/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 2.7197e-08\n",
      "Epoch 191: ReduceLROnPlateau reducing learning rate to 5.695829750038683e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7292e-08 - val_loss: 2.7055e-08 - learning_rate: 5.9956e-05\n",
      "Epoch 192/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7376e-08 - val_loss: 2.6832e-08 - learning_rate: 5.6958e-05\n",
      "Epoch 193/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7605e-08 - val_loss: 2.6615e-08 - learning_rate: 5.6958e-05\n",
      "Epoch 194/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7385e-08 - val_loss: 2.7538e-08 - learning_rate: 5.6958e-05\n",
      "Epoch 195/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7595e-08 - val_loss: 2.6576e-08 - learning_rate: 5.6958e-05\n",
      "Epoch 196/700\n",
      "\u001b[1m 75/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 2.6784e-08\n",
      "Epoch 196: ReduceLROnPlateau reducing learning rate to 5.411038400779944e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7152e-08 - val_loss: 2.6741e-08 - learning_rate: 5.6958e-05\n",
      "Epoch 197/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7627e-08 - val_loss: 2.7030e-08 - learning_rate: 5.4110e-05\n",
      "Epoch 198/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7241e-08 - val_loss: 2.6645e-08 - learning_rate: 5.4110e-05\n",
      "Epoch 199/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7382e-08 - val_loss: 2.6414e-08 - learning_rate: 5.4110e-05\n",
      "Epoch 200/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7326e-08 - val_loss: 2.7300e-08 - learning_rate: 5.4110e-05\n",
      "Epoch 201/700\n",
      "\u001b[1m 72/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 2.7206e-08\n",
      "Epoch 201: ReduceLROnPlateau reducing learning rate to 5.1404864461801476e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7137e-08 - val_loss: 2.7508e-08 - learning_rate: 5.4110e-05\n",
      "Epoch 202/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7374e-08 - val_loss: 2.6429e-08 - learning_rate: 5.1405e-05\n",
      "Epoch 203/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6859e-08 - val_loss: 2.6338e-08 - learning_rate: 5.1405e-05\n",
      "Epoch 204/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7057e-08 - val_loss: 2.9752e-08 - learning_rate: 5.1405e-05\n",
      "Epoch 205/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7788e-08 - val_loss: 2.6530e-08 - learning_rate: 5.1405e-05\n",
      "Epoch 206/700\n",
      "\u001b[1m 74/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 2.6929e-08\n",
      "Epoch 206: ReduceLROnPlateau reducing learning rate to 4.883462279394734e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6992e-08 - val_loss: 2.7438e-08 - learning_rate: 5.1405e-05\n",
      "Epoch 207/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6912e-08 - val_loss: 2.6281e-08 - learning_rate: 4.8835e-05\n",
      "Epoch 208/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6743e-08 - val_loss: 2.7277e-08 - learning_rate: 4.8835e-05\n",
      "Epoch 209/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7111e-08 - val_loss: 2.6786e-08 - learning_rate: 4.8835e-05\n",
      "Epoch 210/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6919e-08 - val_loss: 2.6217e-08 - learning_rate: 4.8835e-05\n",
      "Epoch 211/700\n",
      "\u001b[1m 76/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 2.6646e-08\n",
      "Epoch 211: ReduceLROnPlateau reducing learning rate to 4.639289199985796e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6725e-08 - val_loss: 2.6596e-08 - learning_rate: 4.8835e-05\n",
      "Epoch 212/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6709e-08 - val_loss: 2.6174e-08 - learning_rate: 4.6393e-05\n",
      "Epoch 213/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6398e-08 - val_loss: 2.8268e-08 - learning_rate: 4.6393e-05\n",
      "Epoch 214/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6964e-08 - val_loss: 2.7749e-08 - learning_rate: 4.6393e-05\n",
      "Epoch 215/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7405e-08 - val_loss: 2.6409e-08 - learning_rate: 4.6393e-05\n",
      "Epoch 216/700\n",
      "\u001b[1m 72/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 2.6608e-08\n",
      "Epoch 216: ReduceLROnPlateau reducing learning rate to 4.407324722706107e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6731e-08 - val_loss: 2.6055e-08 - learning_rate: 4.6393e-05\n",
      "Epoch 217/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6527e-08 - val_loss: 2.7265e-08 - learning_rate: 4.4073e-05\n",
      "Epoch 218/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6968e-08 - val_loss: 2.6697e-08 - learning_rate: 4.4073e-05\n",
      "Epoch 219/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6354e-08 - val_loss: 2.6185e-08 - learning_rate: 4.4073e-05\n",
      "Epoch 220/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6550e-08 - val_loss: 2.6201e-08 - learning_rate: 4.4073e-05\n",
      "Epoch 221/700\n",
      "\u001b[1m 69/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 2.6362e-08\n",
      "Epoch 221: ReduceLROnPlateau reducing learning rate to 4.1869585038512015e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6629e-08 - val_loss: 2.5973e-08 - learning_rate: 4.4073e-05\n",
      "Epoch 222/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6553e-08 - val_loss: 2.6014e-08 - learning_rate: 4.1870e-05\n",
      "Epoch 223/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6382e-08 - val_loss: 2.5866e-08 - learning_rate: 4.1870e-05\n",
      "Epoch 224/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6475e-08 - val_loss: 2.6125e-08 - learning_rate: 4.1870e-05\n",
      "Epoch 225/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6401e-08 - val_loss: 2.6207e-08 - learning_rate: 4.1870e-05\n",
      "Epoch 226/700\n",
      "\u001b[1m 76/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 2.6433e-08\n",
      "Epoch 226: ReduceLROnPlateau reducing learning rate to 3.9776106132194396e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6456e-08 - val_loss: 2.6006e-08 - learning_rate: 4.1870e-05\n",
      "Epoch 227/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6274e-08 - val_loss: 2.6321e-08 - learning_rate: 3.9776e-05\n",
      "Epoch 228/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6519e-08 - val_loss: 2.7454e-08 - learning_rate: 3.9776e-05\n",
      "Epoch 229/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6937e-08 - val_loss: 2.5802e-08 - learning_rate: 3.9776e-05\n",
      "Epoch 230/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6418e-08 - val_loss: 2.5923e-08 - learning_rate: 3.9776e-05\n",
      "Epoch 231/700\n",
      "\u001b[1m114/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 896us/step - loss: 2.6683e-08\n",
      "Epoch 231: ReduceLROnPlateau reducing learning rate to 3.778730151680065e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6646e-08 - val_loss: 2.5747e-08 - learning_rate: 3.9776e-05\n",
      "Epoch 232/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6120e-08 - val_loss: 2.5675e-08 - learning_rate: 3.7787e-05\n",
      "Epoch 233/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6033e-08 - val_loss: 2.6522e-08 - learning_rate: 3.7787e-05\n",
      "Epoch 234/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6653e-08 - val_loss: 2.5844e-08 - learning_rate: 3.7787e-05\n",
      "Epoch 235/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6406e-08 - val_loss: 2.5972e-08 - learning_rate: 3.7787e-05\n",
      "Epoch 236/700\n",
      "\u001b[1m 77/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 2.6243e-08\n",
      "Epoch 236: ReduceLROnPlateau reducing learning rate to 3.589793523133266e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6277e-08 - val_loss: 2.5811e-08 - learning_rate: 3.7787e-05\n",
      "Epoch 237/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5991e-08 - val_loss: 2.5952e-08 - learning_rate: 3.5898e-05\n",
      "Epoch 238/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6277e-08 - val_loss: 2.5639e-08 - learning_rate: 3.5898e-05\n",
      "Epoch 239/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5944e-08 - val_loss: 2.6642e-08 - learning_rate: 3.5898e-05\n",
      "Epoch 240/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6132e-08 - val_loss: 2.5834e-08 - learning_rate: 3.5898e-05\n",
      "Epoch 241/700\n",
      "\u001b[1m 73/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 2.6100e-08\n",
      "Epoch 241: ReduceLROnPlateau reducing learning rate to 3.410303743294207e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6129e-08 - val_loss: 2.6243e-08 - learning_rate: 3.5898e-05\n",
      "Epoch 242/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6131e-08 - val_loss: 2.6002e-08 - learning_rate: 3.4103e-05\n",
      "Epoch 243/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6154e-08 - val_loss: 2.6024e-08 - learning_rate: 3.4103e-05\n",
      "Epoch 244/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6273e-08 - val_loss: 2.5654e-08 - learning_rate: 3.4103e-05\n",
      "Epoch 245/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5887e-08 - val_loss: 2.5807e-08 - learning_rate: 3.4103e-05\n",
      "Epoch 246/700\n",
      "\u001b[1m 62/129\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 834us/step - loss: 2.5845e-08\n",
      "Epoch 246: ReduceLROnPlateau reducing learning rate to 3.239788711653091e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6023e-08 - val_loss: 2.5661e-08 - learning_rate: 3.4103e-05\n",
      "Epoch 247/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6210e-08 - val_loss: 2.5484e-08 - learning_rate: 3.2398e-05\n",
      "Epoch 248/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5914e-08 - val_loss: 2.5549e-08 - learning_rate: 3.2398e-05\n",
      "Epoch 249/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6103e-08 - val_loss: 2.6293e-08 - learning_rate: 3.2398e-05\n",
      "Epoch 250/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6208e-08 - val_loss: 2.5547e-08 - learning_rate: 3.2398e-05\n",
      "Epoch 251/700\n",
      "\u001b[1m 64/129\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 2.5606e-08\n",
      "Epoch 251: ReduceLROnPlateau reducing learning rate to 3.0777991378272417e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5799e-08 - val_loss: 2.6169e-08 - learning_rate: 3.2398e-05\n",
      "Epoch 252/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5697e-08 - val_loss: 2.5709e-08 - learning_rate: 3.0778e-05\n",
      "Epoch 253/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5791e-08 - val_loss: 2.5562e-08 - learning_rate: 3.0778e-05\n",
      "Epoch 254/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6184e-08 - val_loss: 2.5580e-08 - learning_rate: 3.0778e-05\n",
      "Epoch 255/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6130e-08 - val_loss: 2.5967e-08 - learning_rate: 3.0778e-05\n",
      "Epoch 256/700\n",
      "\u001b[1m 68/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 2.6214e-08\n",
      "Epoch 256: ReduceLROnPlateau reducing learning rate to 2.9239092327770777e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.6121e-08 - val_loss: 2.5880e-08 - learning_rate: 3.0778e-05\n",
      "Epoch 257/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5763e-08 - val_loss: 2.5310e-08 - learning_rate: 2.9239e-05\n",
      "Epoch 258/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5843e-08 - val_loss: 2.5372e-08 - learning_rate: 2.9239e-05\n",
      "Epoch 259/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5762e-08 - val_loss: 2.6306e-08 - learning_rate: 2.9239e-05\n",
      "Epoch 260/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5772e-08 - val_loss: 2.6085e-08 - learning_rate: 2.9239e-05\n",
      "Epoch 261/700\n",
      "\u001b[1m 63/129\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 2.5698e-08\n",
      "Epoch 261: ReduceLROnPlateau reducing learning rate to 2.7777137711382237e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5704e-08 - val_loss: 2.5254e-08 - learning_rate: 2.9239e-05\n",
      "Epoch 262/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5595e-08 - val_loss: 2.5171e-08 - learning_rate: 2.7777e-05\n",
      "Epoch 263/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5839e-08 - val_loss: 2.5220e-08 - learning_rate: 2.7777e-05\n",
      "Epoch 264/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5883e-08 - val_loss: 2.5242e-08 - learning_rate: 2.7777e-05\n",
      "Epoch 265/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5922e-08 - val_loss: 2.5156e-08 - learning_rate: 2.7777e-05\n",
      "Epoch 266/700\n",
      "\u001b[1m 62/129\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 2.5510e-08\n",
      "Epoch 266: ReduceLROnPlateau reducing learning rate to 2.6388280912215122e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5571e-08 - val_loss: 2.5795e-08 - learning_rate: 2.7777e-05\n",
      "Epoch 267/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5659e-08 - val_loss: 2.5136e-08 - learning_rate: 2.6388e-05\n",
      "Epoch 268/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5634e-08 - val_loss: 2.5177e-08 - learning_rate: 2.6388e-05\n",
      "Epoch 269/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5449e-08 - val_loss: 2.5299e-08 - learning_rate: 2.6388e-05\n",
      "Epoch 270/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5324e-08 - val_loss: 2.5684e-08 - learning_rate: 2.6388e-05\n",
      "Epoch 271/700\n",
      "\u001b[1m 70/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 2.5401e-08\n",
      "Epoch 271: ReduceLROnPlateau reducing learning rate to 2.5068867125810355e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5428e-08 - val_loss: 2.5058e-08 - learning_rate: 2.6388e-05\n",
      "Epoch 272/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5548e-08 - val_loss: 2.5038e-08 - learning_rate: 2.5069e-05\n",
      "Epoch 273/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5339e-08 - val_loss: 2.5026e-08 - learning_rate: 2.5069e-05\n",
      "Epoch 274/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5392e-08 - val_loss: 2.5124e-08 - learning_rate: 2.5069e-05\n",
      "Epoch 275/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5620e-08 - val_loss: 2.5593e-08 - learning_rate: 2.5069e-05\n",
      "Epoch 276/700\n",
      "\u001b[1m123/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 2.5620e-08\n",
      "Epoch 276: ReduceLROnPlateau reducing learning rate to 2.381542299190187e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5615e-08 - val_loss: 2.4996e-08 - learning_rate: 2.5069e-05\n",
      "Epoch 277/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5199e-08 - val_loss: 2.5423e-08 - learning_rate: 2.3815e-05\n",
      "Epoch 278/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5354e-08 - val_loss: 2.5554e-08 - learning_rate: 2.3815e-05\n",
      "Epoch 279/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5508e-08 - val_loss: 2.5010e-08 - learning_rate: 2.3815e-05\n",
      "Epoch 280/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5238e-08 - val_loss: 2.5232e-08 - learning_rate: 2.3815e-05\n",
      "Epoch 281/700\n",
      "\u001b[1m 66/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 2.5804e-08\n",
      "Epoch 281: ReduceLROnPlateau reducing learning rate to 2.262465141029679e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5653e-08 - val_loss: 2.5172e-08 - learning_rate: 2.3815e-05\n",
      "Epoch 282/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5267e-08 - val_loss: 2.5208e-08 - learning_rate: 2.2625e-05\n",
      "Epoch 283/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5417e-08 - val_loss: 2.5079e-08 - learning_rate: 2.2625e-05\n",
      "Epoch 284/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5362e-08 - val_loss: 2.5104e-08 - learning_rate: 2.2625e-05\n",
      "Epoch 285/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5394e-08 - val_loss: 2.5006e-08 - learning_rate: 2.2625e-05\n",
      "Epoch 286/700\n",
      "\u001b[1m 63/129\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 2.5255e-08\n",
      "Epoch 286: ReduceLROnPlateau reducing learning rate to 2.1493419444595927e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5277e-08 - val_loss: 2.4956e-08 - learning_rate: 2.2625e-05\n",
      "Epoch 287/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5468e-08 - val_loss: 2.4990e-08 - learning_rate: 2.1493e-05\n",
      "Epoch 288/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5327e-08 - val_loss: 2.4851e-08 - learning_rate: 2.1493e-05\n",
      "Epoch 289/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5461e-08 - val_loss: 2.5062e-08 - learning_rate: 2.1493e-05\n",
      "Epoch 290/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4846e-08 - val_loss: 2.5316e-08 - learning_rate: 2.1493e-05\n",
      "Epoch 291/700\n",
      "\u001b[1m120/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 2.5627e-08\n",
      "Epoch 291: ReduceLROnPlateau reducing learning rate to 2.0418747953954153e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5599e-08 - val_loss: 2.4873e-08 - learning_rate: 2.1493e-05\n",
      "Epoch 292/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5301e-08 - val_loss: 2.4955e-08 - learning_rate: 2.0419e-05\n",
      "Epoch 293/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5389e-08 - val_loss: 2.4927e-08 - learning_rate: 2.0419e-05\n",
      "Epoch 294/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5360e-08 - val_loss: 2.5186e-08 - learning_rate: 2.0419e-05\n",
      "Epoch 295/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5135e-08 - val_loss: 2.4861e-08 - learning_rate: 2.0419e-05\n",
      "Epoch 296/700\n",
      "\u001b[1m122/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 2.5305e-08\n",
      "Epoch 296: ReduceLROnPlateau reducing learning rate to 1.9397809865040472e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5303e-08 - val_loss: 2.4859e-08 - learning_rate: 2.0419e-05\n",
      "Epoch 297/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5139e-08 - val_loss: 2.5061e-08 - learning_rate: 1.9398e-05\n",
      "Epoch 298/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4904e-08 - val_loss: 2.5594e-08 - learning_rate: 1.9398e-05\n",
      "Epoch 299/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5071e-08 - val_loss: 2.4929e-08 - learning_rate: 1.9398e-05\n",
      "Epoch 300/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5246e-08 - val_loss: 2.4883e-08 - learning_rate: 1.9398e-05\n",
      "Epoch 301/700\n",
      "\u001b[1m 64/129\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 2.5729e-08\n",
      "Epoch 301: ReduceLROnPlateau reducing learning rate to 1.842791980379843e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5478e-08 - val_loss: 2.5014e-08 - learning_rate: 1.9398e-05\n",
      "Epoch 302/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5039e-08 - val_loss: 2.5021e-08 - learning_rate: 1.8428e-05\n",
      "Epoch 303/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5276e-08 - val_loss: 2.5049e-08 - learning_rate: 1.8428e-05\n",
      "Epoch 304/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5119e-08 - val_loss: 2.5354e-08 - learning_rate: 1.8428e-05\n",
      "Epoch 305/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5363e-08 - val_loss: 2.4722e-08 - learning_rate: 1.8428e-05\n",
      "Epoch 306/700\n",
      "\u001b[1m 68/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 751us/step - loss: 2.4959e-08\n",
      "Epoch 306: ReduceLROnPlateau reducing learning rate to 1.7506523727206512e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5038e-08 - val_loss: 2.4851e-08 - learning_rate: 1.8428e-05\n",
      "Epoch 307/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5367e-08 - val_loss: 2.4778e-08 - learning_rate: 1.7507e-05\n",
      "Epoch 308/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5136e-08 - val_loss: 2.5652e-08 - learning_rate: 1.7507e-05\n",
      "Epoch 309/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5467e-08 - val_loss: 2.5081e-08 - learning_rate: 1.7507e-05\n",
      "Epoch 310/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5111e-08 - val_loss: 2.5045e-08 - learning_rate: 1.7507e-05\n",
      "Epoch 311/700\n",
      "\u001b[1m 68/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 2.5001e-08\n",
      "Epoch 311: ReduceLROnPlateau reducing learning rate to 1.66311971952382e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5015e-08 - val_loss: 2.4652e-08 - learning_rate: 1.7507e-05\n",
      "Epoch 312/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5126e-08 - val_loss: 2.4637e-08 - learning_rate: 1.6631e-05\n",
      "Epoch 313/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4954e-08 - val_loss: 2.4619e-08 - learning_rate: 1.6631e-05\n",
      "Epoch 314/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4792e-08 - val_loss: 2.4790e-08 - learning_rate: 1.6631e-05\n",
      "Epoch 315/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4953e-08 - val_loss: 2.4930e-08 - learning_rate: 1.6631e-05\n",
      "Epoch 316/700\n",
      "\u001b[1m 74/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 2.5010e-08\n",
      "Epoch 316: ReduceLROnPlateau reducing learning rate to 1.5799636730662315e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5010e-08 - val_loss: 2.4780e-08 - learning_rate: 1.6631e-05\n",
      "Epoch 317/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4993e-08 - val_loss: 2.5042e-08 - learning_rate: 1.5800e-05\n",
      "Epoch 318/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5098e-08 - val_loss: 2.4612e-08 - learning_rate: 1.5800e-05\n",
      "Epoch 319/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5143e-08 - val_loss: 2.4670e-08 - learning_rate: 1.5800e-05\n",
      "Epoch 320/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4988e-08 - val_loss: 2.4606e-08 - learning_rate: 1.5800e-05\n",
      "Epoch 321/700\n",
      "\u001b[1m 68/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 2.5011e-08\n",
      "Epoch 321: ReduceLROnPlateau reducing learning rate to 1.5009654634923207e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5026e-08 - val_loss: 2.4566e-08 - learning_rate: 1.5800e-05\n",
      "Epoch 322/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5062e-08 - val_loss: 2.4722e-08 - learning_rate: 1.5010e-05\n",
      "Epoch 323/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4928e-08 - val_loss: 2.4561e-08 - learning_rate: 1.5010e-05\n",
      "Epoch 324/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4922e-08 - val_loss: 2.4589e-08 - learning_rate: 1.5010e-05\n",
      "Epoch 325/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5057e-08 - val_loss: 2.4620e-08 - learning_rate: 1.5010e-05\n",
      "Epoch 326/700\n",
      "\u001b[1m 70/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 2.5019e-08\n",
      "Epoch 326: ReduceLROnPlateau reducing learning rate to 1.4259172075981041e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5003e-08 - val_loss: 2.4591e-08 - learning_rate: 1.5010e-05\n",
      "Epoch 327/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4765e-08 - val_loss: 2.4548e-08 - learning_rate: 1.4259e-05\n",
      "Epoch 328/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4746e-08 - val_loss: 2.4823e-08 - learning_rate: 1.4259e-05\n",
      "Epoch 329/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4874e-08 - val_loss: 2.4510e-08 - learning_rate: 1.4259e-05\n",
      "Epoch 330/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5072e-08 - val_loss: 2.4652e-08 - learning_rate: 1.4259e-05\n",
      "Epoch 331/700\n",
      "\u001b[1m128/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 2.4992e-08\n",
      "Epoch 331: ReduceLROnPlateau reducing learning rate to 1.3546213904191972e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4991e-08 - val_loss: 2.4742e-08 - learning_rate: 1.4259e-05\n",
      "Epoch 332/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5113e-08 - val_loss: 2.4733e-08 - learning_rate: 1.3546e-05\n",
      "Epoch 333/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4790e-08 - val_loss: 2.4563e-08 - learning_rate: 1.3546e-05\n",
      "Epoch 334/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4809e-08 - val_loss: 2.4606e-08 - learning_rate: 1.3546e-05\n",
      "Epoch 335/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5018e-08 - val_loss: 2.4862e-08 - learning_rate: 1.3546e-05\n",
      "Epoch 336/700\n",
      "\u001b[1m120/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 850us/step - loss: 2.4882e-08\n",
      "Epoch 336: ReduceLROnPlateau reducing learning rate to 1.2868903468188364e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4879e-08 - val_loss: 2.4477e-08 - learning_rate: 1.3546e-05\n",
      "Epoch 337/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4761e-08 - val_loss: 2.4459e-08 - learning_rate: 1.2869e-05\n",
      "Epoch 338/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4834e-08 - val_loss: 2.4443e-08 - learning_rate: 1.2869e-05\n",
      "Epoch 339/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4752e-08 - val_loss: 2.4643e-08 - learning_rate: 1.2869e-05\n",
      "Epoch 340/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4965e-08 - val_loss: 2.4716e-08 - learning_rate: 1.2869e-05\n",
      "Epoch 341/700\n",
      "\u001b[1m 76/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 2.4486e-08\n",
      "Epoch 341: ReduceLROnPlateau reducing learning rate to 1.2225458294778945e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4586e-08 - val_loss: 2.4455e-08 - learning_rate: 1.2869e-05\n",
      "Epoch 342/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4752e-08 - val_loss: 2.4593e-08 - learning_rate: 1.2225e-05\n",
      "Epoch 343/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4740e-08 - val_loss: 2.4503e-08 - learning_rate: 1.2225e-05\n",
      "Epoch 344/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4961e-08 - val_loss: 2.4472e-08 - learning_rate: 1.2225e-05\n",
      "Epoch 345/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4779e-08 - val_loss: 2.4461e-08 - learning_rate: 1.2225e-05\n",
      "Epoch 346/700\n",
      "\u001b[1m 77/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.4964e-08\n",
      "Epoch 346: ReduceLROnPlateau reducing learning rate to 1.1614185768848983e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4917e-08 - val_loss: 2.4568e-08 - learning_rate: 1.2225e-05\n",
      "Epoch 347/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4594e-08 - val_loss: 2.4914e-08 - learning_rate: 1.1614e-05\n",
      "Epoch 348/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4980e-08 - val_loss: 2.4424e-08 - learning_rate: 1.1614e-05\n",
      "Epoch 349/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4912e-08 - val_loss: 2.4691e-08 - learning_rate: 1.1614e-05\n",
      "Epoch 350/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4547e-08 - val_loss: 2.4496e-08 - learning_rate: 1.1614e-05\n",
      "Epoch 351/700\n",
      "\u001b[1m 71/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 2.4414e-08\n",
      "Epoch 351: ReduceLROnPlateau reducing learning rate to 1.1033476221200543e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4537e-08 - val_loss: 2.4530e-08 - learning_rate: 1.1614e-05\n",
      "Epoch 352/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4981e-08 - val_loss: 2.4632e-08 - learning_rate: 1.1033e-05\n",
      "Epoch 353/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4637e-08 - val_loss: 2.4525e-08 - learning_rate: 1.1033e-05\n",
      "Epoch 354/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4597e-08 - val_loss: 2.4361e-08 - learning_rate: 1.1033e-05\n",
      "Epoch 355/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4667e-08 - val_loss: 2.4503e-08 - learning_rate: 1.1033e-05\n",
      "Epoch 356/700\n",
      "\u001b[1m125/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 2.4916e-08\n",
      "Epoch 356: ReduceLROnPlateau reducing learning rate to 1.048180206453253e-05.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4908e-08 - val_loss: 2.4523e-08 - learning_rate: 1.1033e-05\n",
      "Epoch 357/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4951e-08 - val_loss: 2.4441e-08 - learning_rate: 1.0482e-05\n",
      "Epoch 358/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4739e-08 - val_loss: 2.4396e-08 - learning_rate: 1.0482e-05\n",
      "Epoch 359/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4676e-08 - val_loss: 2.4433e-08 - learning_rate: 1.0482e-05\n",
      "Epoch 360/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4489e-08 - val_loss: 2.4392e-08 - learning_rate: 1.0482e-05\n",
      "Epoch 361/700\n",
      "\u001b[1m122/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 843us/step - loss: 2.4852e-08\n",
      "Epoch 361: ReduceLROnPlateau reducing learning rate to 9.957711745300912e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4843e-08 - val_loss: 2.4625e-08 - learning_rate: 1.0482e-05\n",
      "Epoch 362/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4790e-08 - val_loss: 2.4439e-08 - learning_rate: 9.9577e-06\n",
      "Epoch 363/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4459e-08 - val_loss: 2.4505e-08 - learning_rate: 9.9577e-06\n",
      "Epoch 364/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4906e-08 - val_loss: 2.4348e-08 - learning_rate: 9.9577e-06\n",
      "Epoch 365/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4607e-08 - val_loss: 2.4521e-08 - learning_rate: 9.9577e-06\n",
      "Epoch 366/700\n",
      "\u001b[1m 69/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 2.4193e-08\n",
      "Epoch 366: ReduceLROnPlateau reducing learning rate to 9.459826287638862e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4362e-08 - val_loss: 2.4417e-08 - learning_rate: 9.9577e-06\n",
      "Epoch 367/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4718e-08 - val_loss: 2.4620e-08 - learning_rate: 9.4598e-06\n",
      "Epoch 368/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4628e-08 - val_loss: 2.4314e-08 - learning_rate: 9.4598e-06\n",
      "Epoch 369/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4719e-08 - val_loss: 2.4310e-08 - learning_rate: 9.4598e-06\n",
      "Epoch 370/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4871e-08 - val_loss: 2.4384e-08 - learning_rate: 9.4598e-06\n",
      "Epoch 371/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 2.4593e-08\n",
      "Epoch 371: ReduceLROnPlateau reducing learning rate to 8.986834973256918e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4594e-08 - val_loss: 2.4392e-08 - learning_rate: 9.4598e-06\n",
      "Epoch 372/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4626e-08 - val_loss: 2.4481e-08 - learning_rate: 8.9868e-06\n",
      "Epoch 373/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4488e-08 - val_loss: 2.4329e-08 - learning_rate: 8.9868e-06\n",
      "Epoch 374/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4376e-08 - val_loss: 2.4277e-08 - learning_rate: 8.9868e-06\n",
      "Epoch 375/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4339e-08 - val_loss: 2.4482e-08 - learning_rate: 8.9868e-06\n",
      "Epoch 376/700\n",
      "\u001b[1m 76/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 2.4755e-08\n",
      "Epoch 376: ReduceLROnPlateau reducing learning rate to 8.537493613403058e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4730e-08 - val_loss: 2.4343e-08 - learning_rate: 8.9868e-06\n",
      "Epoch 377/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4581e-08 - val_loss: 2.4375e-08 - learning_rate: 8.5375e-06\n",
      "Epoch 378/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4520e-08 - val_loss: 2.4313e-08 - learning_rate: 8.5375e-06\n",
      "Epoch 379/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4405e-08 - val_loss: 2.4327e-08 - learning_rate: 8.5375e-06\n",
      "Epoch 380/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4779e-08 - val_loss: 2.4280e-08 - learning_rate: 8.5375e-06\n",
      "Epoch 381/700\n",
      "\u001b[1m 71/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 2.4368e-08\n",
      "Epoch 381: ReduceLROnPlateau reducing learning rate to 8.110619364742887e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4466e-08 - val_loss: 2.4256e-08 - learning_rate: 8.5375e-06\n",
      "Epoch 382/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4527e-08 - val_loss: 2.4232e-08 - learning_rate: 8.1106e-06\n",
      "Epoch 383/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4502e-08 - val_loss: 2.4232e-08 - learning_rate: 8.1106e-06\n",
      "Epoch 384/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4413e-08 - val_loss: 2.4286e-08 - learning_rate: 8.1106e-06\n",
      "Epoch 385/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4793e-08 - val_loss: 2.4277e-08 - learning_rate: 8.1106e-06\n",
      "Epoch 386/700\n",
      "\u001b[1m 70/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 2.4325e-08\n",
      "Epoch 386: ReduceLROnPlateau reducing learning rate to 7.705088137299754e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4433e-08 - val_loss: 2.4305e-08 - learning_rate: 8.1106e-06\n",
      "Epoch 387/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4489e-08 - val_loss: 2.4209e-08 - learning_rate: 7.7051e-06\n",
      "Epoch 388/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4327e-08 - val_loss: 2.4197e-08 - learning_rate: 7.7051e-06\n",
      "Epoch 389/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4705e-08 - val_loss: 2.4329e-08 - learning_rate: 7.7051e-06\n",
      "Epoch 390/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4756e-08 - val_loss: 2.4354e-08 - learning_rate: 7.7051e-06\n",
      "Epoch 391/700\n",
      "\u001b[1m 71/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 727us/step - loss: 2.4789e-08\n",
      "Epoch 391: ReduceLROnPlateau reducing learning rate to 7.319833730434766e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4654e-08 - val_loss: 2.4232e-08 - learning_rate: 7.7051e-06\n",
      "Epoch 392/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4549e-08 - val_loss: 2.4426e-08 - learning_rate: 7.3198e-06\n",
      "Epoch 393/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4626e-08 - val_loss: 2.4197e-08 - learning_rate: 7.3198e-06\n",
      "Epoch 394/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4523e-08 - val_loss: 2.4183e-08 - learning_rate: 7.3198e-06\n",
      "Epoch 395/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4558e-08 - val_loss: 2.4204e-08 - learning_rate: 7.3198e-06\n",
      "Epoch 396/700\n",
      "\u001b[1m126/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 2.4550e-08\n",
      "Epoch 396: ReduceLROnPlateau reducing learning rate to 6.953842216717021e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4549e-08 - val_loss: 2.4460e-08 - learning_rate: 7.3198e-06\n",
      "Epoch 397/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4489e-08 - val_loss: 2.4217e-08 - learning_rate: 6.9538e-06\n",
      "Epoch 398/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4348e-08 - val_loss: 2.4200e-08 - learning_rate: 6.9538e-06\n",
      "Epoch 399/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4296e-08 - val_loss: 2.4212e-08 - learning_rate: 6.9538e-06\n",
      "Epoch 400/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4911e-08 - val_loss: 2.4173e-08 - learning_rate: 6.9538e-06\n",
      "Epoch 401/700\n",
      "\u001b[1m 66/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 2.4110e-08\n",
      "Epoch 401: ReduceLROnPlateau reducing learning rate to 6.6061502138836655e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4268e-08 - val_loss: 2.4161e-08 - learning_rate: 6.9538e-06\n",
      "Epoch 402/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4650e-08 - val_loss: 2.4195e-08 - learning_rate: 6.6062e-06\n",
      "Epoch 403/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4538e-08 - val_loss: 2.4191e-08 - learning_rate: 6.6062e-06\n",
      "Epoch 404/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4677e-08 - val_loss: 2.4175e-08 - learning_rate: 6.6062e-06\n",
      "Epoch 405/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4410e-08 - val_loss: 2.4179e-08 - learning_rate: 6.6062e-06\n",
      "Epoch 406/700\n",
      "\u001b[1m 71/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 2.4332e-08\n",
      "Epoch 406: ReduceLROnPlateau reducing learning rate to 6.2758427247899816e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4384e-08 - val_loss: 2.4180e-08 - learning_rate: 6.6062e-06\n",
      "Epoch 407/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4336e-08 - val_loss: 2.4196e-08 - learning_rate: 6.2758e-06\n",
      "Epoch 408/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4687e-08 - val_loss: 2.4416e-08 - learning_rate: 6.2758e-06\n",
      "Epoch 409/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4576e-08 - val_loss: 2.4284e-08 - learning_rate: 6.2758e-06\n",
      "Epoch 410/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4466e-08 - val_loss: 2.4173e-08 - learning_rate: 6.2758e-06\n",
      "Epoch 411/700\n",
      "\u001b[1m 66/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 2.4369e-08\n",
      "Epoch 411: ReduceLROnPlateau reducing learning rate to 5.962050545349484e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4382e-08 - val_loss: 2.4147e-08 - learning_rate: 6.2758e-06\n",
      "Epoch 412/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4415e-08 - val_loss: 2.4187e-08 - learning_rate: 5.9621e-06\n",
      "Epoch 413/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4494e-08 - val_loss: 2.4124e-08 - learning_rate: 5.9621e-06\n",
      "Epoch 414/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4285e-08 - val_loss: 2.4217e-08 - learning_rate: 5.9621e-06\n",
      "Epoch 415/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4719e-08 - val_loss: 2.4155e-08 - learning_rate: 5.9621e-06\n",
      "Epoch 416/700\n",
      "\u001b[1m 71/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 2.4645e-08\n",
      "Epoch 416: ReduceLROnPlateau reducing learning rate to 5.663948104484007e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4625e-08 - val_loss: 2.4157e-08 - learning_rate: 5.9621e-06\n",
      "Epoch 417/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4375e-08 - val_loss: 2.4217e-08 - learning_rate: 5.6639e-06\n",
      "Epoch 418/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4320e-08 - val_loss: 2.4285e-08 - learning_rate: 5.6639e-06\n",
      "Epoch 419/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4148e-08 - val_loss: 2.4106e-08 - learning_rate: 5.6639e-06\n",
      "Epoch 420/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4239e-08 - val_loss: 2.4111e-08 - learning_rate: 5.6639e-06\n",
      "Epoch 421/700\n",
      "\u001b[1m 72/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 710us/step - loss: 2.4376e-08\n",
      "Epoch 421: ReduceLROnPlateau reducing learning rate to 5.380750872063799e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4354e-08 - val_loss: 2.4130e-08 - learning_rate: 5.6639e-06\n",
      "Epoch 422/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4197e-08 - val_loss: 2.4140e-08 - learning_rate: 5.3808e-06\n",
      "Epoch 423/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4330e-08 - val_loss: 2.4263e-08 - learning_rate: 5.3808e-06\n",
      "Epoch 424/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4257e-08 - val_loss: 2.4223e-08 - learning_rate: 5.3808e-06\n",
      "Epoch 425/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4448e-08 - val_loss: 2.4301e-08 - learning_rate: 5.3808e-06\n",
      "Epoch 426/700\n",
      "\u001b[1m 69/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 2.4583e-08\n",
      "Epoch 426: ReduceLROnPlateau reducing learning rate to 5.1117131988576144e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4549e-08 - val_loss: 2.4134e-08 - learning_rate: 5.3808e-06\n",
      "Epoch 427/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4412e-08 - val_loss: 2.4238e-08 - learning_rate: 5.1117e-06\n",
      "Epoch 428/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4186e-08 - val_loss: 2.4084e-08 - learning_rate: 5.1117e-06\n",
      "Epoch 429/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4610e-08 - val_loss: 2.4106e-08 - learning_rate: 5.1117e-06\n",
      "Epoch 430/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4387e-08 - val_loss: 2.4112e-08 - learning_rate: 5.1117e-06\n",
      "Epoch 431/700\n",
      "\u001b[1m 79/129\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 2.4447e-08\n",
      "Epoch 431: ReduceLROnPlateau reducing learning rate to 4.856127452512737e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4464e-08 - val_loss: 2.4088e-08 - learning_rate: 5.1117e-06\n",
      "Epoch 432/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4520e-08 - val_loss: 2.4258e-08 - learning_rate: 4.8561e-06\n",
      "Epoch 433/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4469e-08 - val_loss: 2.4112e-08 - learning_rate: 4.8561e-06\n",
      "Epoch 434/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4312e-08 - val_loss: 2.4098e-08 - learning_rate: 4.8561e-06\n",
      "Epoch 435/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4160e-08 - val_loss: 2.4185e-08 - learning_rate: 4.8561e-06\n",
      "Epoch 436/700\n",
      "\u001b[1m 78/129\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 2.4401e-08\n",
      "Epoch 436: ReduceLROnPlateau reducing learning rate to 4.613320993485104e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4386e-08 - val_loss: 2.4069e-08 - learning_rate: 4.8561e-06\n",
      "Epoch 437/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4177e-08 - val_loss: 2.4088e-08 - learning_rate: 4.6133e-06\n",
      "Epoch 438/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4320e-08 - val_loss: 2.4094e-08 - learning_rate: 4.6133e-06\n",
      "Epoch 439/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4437e-08 - val_loss: 2.4057e-08 - learning_rate: 4.6133e-06\n",
      "Epoch 440/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4506e-08 - val_loss: 2.4105e-08 - learning_rate: 4.6133e-06\n",
      "Epoch 441/700\n",
      "\u001b[1m 76/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 2.4546e-08\n",
      "Epoch 441: ReduceLROnPlateau reducing learning rate to 4.382654879009351e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4474e-08 - val_loss: 2.4084e-08 - learning_rate: 4.6133e-06\n",
      "Epoch 442/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4414e-08 - val_loss: 2.4220e-08 - learning_rate: 4.3827e-06\n",
      "Epoch 443/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4365e-08 - val_loss: 2.4168e-08 - learning_rate: 4.3827e-06\n",
      "Epoch 444/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4373e-08 - val_loss: 2.4089e-08 - learning_rate: 4.3827e-06\n",
      "Epoch 445/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4410e-08 - val_loss: 2.4042e-08 - learning_rate: 4.3827e-06\n",
      "Epoch 446/700\n",
      "\u001b[1m 78/129\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 2.4323e-08\n",
      "Epoch 446: ReduceLROnPlateau reducing learning rate to 4.163522135058884e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4330e-08 - val_loss: 2.4239e-08 - learning_rate: 4.3827e-06\n",
      "Epoch 447/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4119e-08 - val_loss: 2.4062e-08 - learning_rate: 4.1635e-06\n",
      "Epoch 448/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4248e-08 - val_loss: 2.4045e-08 - learning_rate: 4.1635e-06\n",
      "Epoch 449/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4656e-08 - val_loss: 2.4137e-08 - learning_rate: 4.1635e-06\n",
      "Epoch 450/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4395e-08 - val_loss: 2.4023e-08 - learning_rate: 4.1635e-06\n",
      "Epoch 451/700\n",
      "\u001b[1m 75/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 2.4060e-08\n",
      "Epoch 451: ReduceLROnPlateau reducing learning rate to 3.9553460283059396e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4162e-08 - val_loss: 2.4174e-08 - learning_rate: 4.1635e-06\n",
      "Epoch 452/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4143e-08 - val_loss: 2.4093e-08 - learning_rate: 3.9553e-06\n",
      "Epoch 453/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4295e-08 - val_loss: 2.4028e-08 - learning_rate: 3.9553e-06\n",
      "Epoch 454/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4170e-08 - val_loss: 2.4019e-08 - learning_rate: 3.9553e-06\n",
      "Epoch 455/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4273e-08 - val_loss: 2.4097e-08 - learning_rate: 3.9553e-06\n",
      "Epoch 456/700\n",
      "\u001b[1m 78/129\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.4251e-08\n",
      "Epoch 456: ReduceLROnPlateau reducing learning rate to 3.7575787700916406e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4256e-08 - val_loss: 2.4031e-08 - learning_rate: 3.9553e-06\n",
      "Epoch 457/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4319e-08 - val_loss: 2.4035e-08 - learning_rate: 3.7576e-06\n",
      "Epoch 458/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4475e-08 - val_loss: 2.4064e-08 - learning_rate: 3.7576e-06\n",
      "Epoch 459/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4141e-08 - val_loss: 2.4160e-08 - learning_rate: 3.7576e-06\n",
      "Epoch 460/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4101e-08 - val_loss: 2.4078e-08 - learning_rate: 3.7576e-06\n",
      "Epoch 461/700\n",
      "\u001b[1m 78/129\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 2.4405e-08\n",
      "Epoch 461: ReduceLROnPlateau reducing learning rate to 3.5696997883860602e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4367e-08 - val_loss: 2.4019e-08 - learning_rate: 3.7576e-06\n",
      "Epoch 462/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4324e-08 - val_loss: 2.4003e-08 - learning_rate: 3.5697e-06\n",
      "Epoch 463/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4285e-08 - val_loss: 2.4021e-08 - learning_rate: 3.5697e-06\n",
      "Epoch 464/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4425e-08 - val_loss: 2.4014e-08 - learning_rate: 3.5697e-06\n",
      "Epoch 465/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4516e-08 - val_loss: 2.4062e-08 - learning_rate: 3.5697e-06\n",
      "Epoch 466/700\n",
      "\u001b[1m 69/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 741us/step - loss: 2.3917e-08\n",
      "Epoch 466: ReduceLROnPlateau reducing learning rate to 3.3912148637682547e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4054e-08 - val_loss: 2.4012e-08 - learning_rate: 3.5697e-06\n",
      "Epoch 467/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4252e-08 - val_loss: 2.3998e-08 - learning_rate: 3.3912e-06\n",
      "Epoch 468/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4369e-08 - val_loss: 2.4008e-08 - learning_rate: 3.3912e-06\n",
      "Epoch 469/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4033e-08 - val_loss: 2.4062e-08 - learning_rate: 3.3912e-06\n",
      "Epoch 470/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4226e-08 - val_loss: 2.3996e-08 - learning_rate: 3.3912e-06\n",
      "Epoch 471/700\n",
      "\u001b[1m 77/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 2.4321e-08\n",
      "Epoch 471: ReduceLROnPlateau reducing learning rate to 3.2216541853813395e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4303e-08 - val_loss: 2.4009e-08 - learning_rate: 3.3912e-06\n",
      "Epoch 472/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4277e-08 - val_loss: 2.4040e-08 - learning_rate: 3.2217e-06\n",
      "Epoch 473/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4390e-08 - val_loss: 2.4087e-08 - learning_rate: 3.2217e-06\n",
      "Epoch 474/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4386e-08 - val_loss: 2.3998e-08 - learning_rate: 3.2217e-06\n",
      "Epoch 475/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4249e-08 - val_loss: 2.3996e-08 - learning_rate: 3.2217e-06\n",
      "Epoch 476/700\n",
      "\u001b[1m 71/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 2.4246e-08\n",
      "Epoch 476: ReduceLROnPlateau reducing learning rate to 3.0605714869125222e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4298e-08 - val_loss: 2.4062e-08 - learning_rate: 3.2217e-06\n",
      "Epoch 477/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4256e-08 - val_loss: 2.4020e-08 - learning_rate: 3.0606e-06\n",
      "Epoch 478/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4129e-08 - val_loss: 2.3976e-08 - learning_rate: 3.0606e-06\n",
      "Epoch 479/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4197e-08 - val_loss: 2.3994e-08 - learning_rate: 3.0606e-06\n",
      "Epoch 480/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4059e-08 - val_loss: 2.4034e-08 - learning_rate: 3.0606e-06\n",
      "Epoch 481/700\n",
      "\u001b[1m 75/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 2.4355e-08\n",
      "Epoch 481: ReduceLROnPlateau reducing learning rate to 2.907542966568144e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4299e-08 - val_loss: 2.4045e-08 - learning_rate: 3.0606e-06\n",
      "Epoch 482/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4385e-08 - val_loss: 2.3996e-08 - learning_rate: 2.9075e-06\n",
      "Epoch 483/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4337e-08 - val_loss: 2.4035e-08 - learning_rate: 2.9075e-06\n",
      "Epoch 484/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4392e-08 - val_loss: 2.4024e-08 - learning_rate: 2.9075e-06\n",
      "Epoch 485/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4155e-08 - val_loss: 2.4011e-08 - learning_rate: 2.9075e-06\n",
      "Epoch 486/700\n",
      "\u001b[1m 78/129\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.4310e-08\n",
      "Epoch 486: ReduceLROnPlateau reducing learning rate to 2.7621657750387382e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4239e-08 - val_loss: 2.3979e-08 - learning_rate: 2.9075e-06\n",
      "Epoch 487/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4171e-08 - val_loss: 2.3983e-08 - learning_rate: 2.7622e-06\n",
      "Epoch 488/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4257e-08 - val_loss: 2.3984e-08 - learning_rate: 2.7622e-06\n",
      "Epoch 489/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4431e-08 - val_loss: 2.3988e-08 - learning_rate: 2.7622e-06\n",
      "Epoch 490/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4237e-08 - val_loss: 2.4085e-08 - learning_rate: 2.7622e-06\n",
      "Epoch 491/700\n",
      "\u001b[1m 76/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 2.3945e-08\n",
      "Epoch 491: ReduceLROnPlateau reducing learning rate to 2.6240575834890476e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4053e-08 - val_loss: 2.3976e-08 - learning_rate: 2.7622e-06\n",
      "Epoch 492/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4305e-08 - val_loss: 2.3969e-08 - learning_rate: 2.6241e-06\n",
      "Epoch 493/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4163e-08 - val_loss: 2.3986e-08 - learning_rate: 2.6241e-06\n",
      "Epoch 494/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4392e-08 - val_loss: 2.4021e-08 - learning_rate: 2.6241e-06\n",
      "Epoch 495/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4124e-08 - val_loss: 2.4014e-08 - learning_rate: 2.6241e-06\n",
      "Epoch 496/700\n",
      "\u001b[1m 71/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 2.4416e-08\n",
      "Epoch 496: ReduceLROnPlateau reducing learning rate to 2.492854639513098e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4374e-08 - val_loss: 2.3981e-08 - learning_rate: 2.6241e-06\n",
      "Epoch 497/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4270e-08 - val_loss: 2.4011e-08 - learning_rate: 2.4929e-06\n",
      "Epoch 498/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4436e-08 - val_loss: 2.4043e-08 - learning_rate: 2.4929e-06\n",
      "Epoch 499/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4374e-08 - val_loss: 2.3993e-08 - learning_rate: 2.4929e-06\n",
      "Epoch 500/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4254e-08 - val_loss: 2.3949e-08 - learning_rate: 2.4929e-06\n",
      "Epoch 501/700\n",
      "\u001b[1m 79/129\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 2.4195e-08\n",
      "Epoch 501: ReduceLROnPlateau reducing learning rate to 2.36821198313919e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4185e-08 - val_loss: 2.4001e-08 - learning_rate: 2.4929e-06\n",
      "Epoch 502/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4280e-08 - val_loss: 2.3960e-08 - learning_rate: 2.3682e-06\n",
      "Epoch 503/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4416e-08 - val_loss: 2.3957e-08 - learning_rate: 2.3682e-06\n",
      "Epoch 504/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4029e-08 - val_loss: 2.3939e-08 - learning_rate: 2.3682e-06\n",
      "Epoch 505/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4204e-08 - val_loss: 2.3976e-08 - learning_rate: 2.3682e-06\n",
      "Epoch 506/700\n",
      "\u001b[1m 73/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 2.4350e-08\n",
      "Epoch 506: ReduceLROnPlateau reducing learning rate to 2.249801286779984e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4257e-08 - val_loss: 2.3939e-08 - learning_rate: 2.3682e-06\n",
      "Epoch 507/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4337e-08 - val_loss: 2.3972e-08 - learning_rate: 2.2498e-06\n",
      "Epoch 508/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4115e-08 - val_loss: 2.3986e-08 - learning_rate: 2.2498e-06\n",
      "Epoch 509/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4248e-08 - val_loss: 2.3953e-08 - learning_rate: 2.2498e-06\n",
      "Epoch 510/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4205e-08 - val_loss: 2.3933e-08 - learning_rate: 2.2498e-06\n",
      "Epoch 511/700\n",
      "\u001b[1m 71/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 2.4052e-08\n",
      "Epoch 511: ReduceLROnPlateau reducing learning rate to 2.1373112872424825e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4093e-08 - val_loss: 2.3992e-08 - learning_rate: 2.2498e-06\n",
      "Epoch 512/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4294e-08 - val_loss: 2.3995e-08 - learning_rate: 2.1373e-06\n",
      "Epoch 513/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4370e-08 - val_loss: 2.3949e-08 - learning_rate: 2.1373e-06\n",
      "Epoch 514/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4102e-08 - val_loss: 2.3964e-08 - learning_rate: 2.1373e-06\n",
      "Epoch 515/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4040e-08 - val_loss: 2.3979e-08 - learning_rate: 2.1373e-06\n",
      "Epoch 516/700\n",
      "\u001b[1m 72/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 2.3934e-08\n",
      "Epoch 516: ReduceLROnPlateau reducing learning rate to 2.030445625678112e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4000e-08 - val_loss: 2.3942e-08 - learning_rate: 2.1373e-06\n",
      "Epoch 517/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4407e-08 - val_loss: 2.3985e-08 - learning_rate: 2.0304e-06\n",
      "Epoch 518/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4185e-08 - val_loss: 2.4030e-08 - learning_rate: 2.0304e-06\n",
      "Epoch 519/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4452e-08 - val_loss: 2.3983e-08 - learning_rate: 2.0304e-06\n",
      "Epoch 520/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4358e-08 - val_loss: 2.3982e-08 - learning_rate: 2.0304e-06\n",
      "Epoch 521/700\n",
      "\u001b[1m123/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 2.4342e-08\n",
      "Epoch 521: ReduceLROnPlateau reducing learning rate to 1.928923279592709e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4335e-08 - val_loss: 2.3961e-08 - learning_rate: 2.0304e-06\n",
      "Epoch 522/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4272e-08 - val_loss: 2.3949e-08 - learning_rate: 1.9289e-06\n",
      "Epoch 523/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4135e-08 - val_loss: 2.3931e-08 - learning_rate: 1.9289e-06\n",
      "Epoch 524/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4291e-08 - val_loss: 2.4055e-08 - learning_rate: 1.9289e-06\n",
      "Epoch 525/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4128e-08 - val_loss: 2.4001e-08 - learning_rate: 1.9289e-06\n",
      "Epoch 526/700\n",
      "\u001b[1m 66/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 2.3997e-08\n",
      "Epoch 526: ReduceLROnPlateau reducing learning rate to 1.8324770508115761e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4063e-08 - val_loss: 2.3945e-08 - learning_rate: 1.9289e-06\n",
      "Epoch 527/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3974e-08 - val_loss: 2.3952e-08 - learning_rate: 1.8325e-06\n",
      "Epoch 528/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4260e-08 - val_loss: 2.4090e-08 - learning_rate: 1.8325e-06\n",
      "Epoch 529/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4027e-08 - val_loss: 2.3931e-08 - learning_rate: 1.8325e-06\n",
      "Epoch 530/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4065e-08 - val_loss: 2.3914e-08 - learning_rate: 1.8325e-06\n",
      "Epoch 531/700\n",
      "\u001b[1m114/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 897us/step - loss: 2.4153e-08\n",
      "Epoch 531: ReduceLROnPlateau reducing learning rate to 1.7408532414719956e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4158e-08 - val_loss: 2.3949e-08 - learning_rate: 1.8325e-06\n",
      "Epoch 532/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4244e-08 - val_loss: 2.3911e-08 - learning_rate: 1.7409e-06\n",
      "Epoch 533/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4257e-08 - val_loss: 2.3921e-08 - learning_rate: 1.7409e-06\n",
      "Epoch 534/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4253e-08 - val_loss: 2.3930e-08 - learning_rate: 1.7409e-06\n",
      "Epoch 535/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4230e-08 - val_loss: 2.3909e-08 - learning_rate: 1.7409e-06\n",
      "Epoch 536/700\n",
      "\u001b[1m125/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 2.4296e-08\n",
      "Epoch 536: ReduceLROnPlateau reducing learning rate to 1.653810573998271e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4292e-08 - val_loss: 2.3922e-08 - learning_rate: 1.7409e-06\n",
      "Epoch 537/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4000e-08 - val_loss: 2.3926e-08 - learning_rate: 1.6538e-06\n",
      "Epoch 538/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4180e-08 - val_loss: 2.3929e-08 - learning_rate: 1.6538e-06\n",
      "Epoch 539/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4120e-08 - val_loss: 2.3933e-08 - learning_rate: 1.6538e-06\n",
      "Epoch 540/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4199e-08 - val_loss: 2.3904e-08 - learning_rate: 1.6538e-06\n",
      "Epoch 541/700\n",
      "\u001b[1m 74/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 2.4075e-08\n",
      "Epoch 541: ReduceLROnPlateau reducing learning rate to 1.571120083099231e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4093e-08 - val_loss: 2.3933e-08 - learning_rate: 1.6538e-06\n",
      "Epoch 542/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4246e-08 - val_loss: 2.3916e-08 - learning_rate: 1.5711e-06\n",
      "Epoch 543/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4193e-08 - val_loss: 2.3919e-08 - learning_rate: 1.5711e-06\n",
      "Epoch 544/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3888e-08 - val_loss: 2.3908e-08 - learning_rate: 1.5711e-06\n",
      "Epoch 545/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4111e-08 - val_loss: 2.4017e-08 - learning_rate: 1.5711e-06\n",
      "Epoch 546/700\n",
      "\u001b[1m124/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 2.4103e-08\n",
      "Epoch 546: ReduceLROnPlateau reducing learning rate to 1.4925640357432712e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4108e-08 - val_loss: 2.3978e-08 - learning_rate: 1.5711e-06\n",
      "Epoch 547/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4021e-08 - val_loss: 2.3905e-08 - learning_rate: 1.4926e-06\n",
      "Epoch 548/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4280e-08 - val_loss: 2.3906e-08 - learning_rate: 1.4926e-06\n",
      "Epoch 549/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4313e-08 - val_loss: 2.3997e-08 - learning_rate: 1.4926e-06\n",
      "Epoch 550/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4225e-08 - val_loss: 2.3932e-08 - learning_rate: 1.4926e-06\n",
      "Epoch 551/700\n",
      "\u001b[1m 69/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 2.4201e-08\n",
      "Epoch 551: ReduceLROnPlateau reducing learning rate to 1.417935823155858e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4190e-08 - val_loss: 2.3910e-08 - learning_rate: 1.4926e-06\n",
      "Epoch 552/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4369e-08 - val_loss: 2.3916e-08 - learning_rate: 1.4179e-06\n",
      "Epoch 553/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4360e-08 - val_loss: 2.3893e-08 - learning_rate: 1.4179e-06\n",
      "Epoch 554/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4153e-08 - val_loss: 2.3930e-08 - learning_rate: 1.4179e-06\n",
      "Epoch 555/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4303e-08 - val_loss: 2.3899e-08 - learning_rate: 1.4179e-06\n",
      "Epoch 556/700\n",
      "\u001b[1m 75/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 2.3991e-08\n",
      "Epoch 556: ReduceLROnPlateau reducing learning rate to 1.3470389887970668e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4073e-08 - val_loss: 2.3909e-08 - learning_rate: 1.4179e-06\n",
      "Epoch 557/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4170e-08 - val_loss: 2.3988e-08 - learning_rate: 1.3470e-06\n",
      "Epoch 558/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4148e-08 - val_loss: 2.3956e-08 - learning_rate: 1.3470e-06\n",
      "Epoch 559/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4121e-08 - val_loss: 2.3906e-08 - learning_rate: 1.3470e-06\n",
      "Epoch 560/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4155e-08 - val_loss: 2.3926e-08 - learning_rate: 1.3470e-06\n",
      "Epoch 561/700\n",
      "\u001b[1m 77/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 2.3878e-08\n",
      "Epoch 561: ReduceLROnPlateau reducing learning rate to 1.2796870123565895e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3978e-08 - val_loss: 2.3909e-08 - learning_rate: 1.3470e-06\n",
      "Epoch 562/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4111e-08 - val_loss: 2.3895e-08 - learning_rate: 1.2797e-06\n",
      "Epoch 563/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4152e-08 - val_loss: 2.3907e-08 - learning_rate: 1.2797e-06\n",
      "Epoch 564/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4146e-08 - val_loss: 2.3911e-08 - learning_rate: 1.2797e-06\n",
      "Epoch 565/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4108e-08 - val_loss: 2.3903e-08 - learning_rate: 1.2797e-06\n",
      "Epoch 566/700\n",
      "\u001b[1m 76/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 2.4200e-08\n",
      "Epoch 566: ReduceLROnPlateau reducing learning rate to 1.21570266173876e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4187e-08 - val_loss: 2.3960e-08 - learning_rate: 1.2797e-06\n",
      "Epoch 567/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4155e-08 - val_loss: 2.3895e-08 - learning_rate: 1.2157e-06\n",
      "Epoch 568/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4095e-08 - val_loss: 2.3903e-08 - learning_rate: 1.2157e-06\n",
      "Epoch 569/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4012e-08 - val_loss: 2.3898e-08 - learning_rate: 1.2157e-06\n",
      "Epoch 570/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4210e-08 - val_loss: 2.3941e-08 - learning_rate: 1.2157e-06\n",
      "Epoch 571/700\n",
      "\u001b[1m 79/129\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 2.3990e-08\n",
      "Epoch 571: ReduceLROnPlateau reducing learning rate to 1.1549175610525707e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4031e-08 - val_loss: 2.3886e-08 - learning_rate: 1.2157e-06\n",
      "Epoch 572/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4281e-08 - val_loss: 2.3896e-08 - learning_rate: 1.1549e-06\n",
      "Epoch 573/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4270e-08 - val_loss: 2.3915e-08 - learning_rate: 1.1549e-06\n",
      "Epoch 574/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4145e-08 - val_loss: 2.3975e-08 - learning_rate: 1.1549e-06\n",
      "Epoch 575/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4129e-08 - val_loss: 2.3883e-08 - learning_rate: 1.1549e-06\n",
      "Epoch 576/700\n",
      "\u001b[1m 62/129\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 2.4091e-08\n",
      "Epoch 576: ReduceLROnPlateau reducing learning rate to 1.0971716505991936e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4097e-08 - val_loss: 2.3919e-08 - learning_rate: 1.1549e-06\n",
      "Epoch 577/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4123e-08 - val_loss: 2.3936e-08 - learning_rate: 1.0972e-06\n",
      "Epoch 578/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3850e-08 - val_loss: 2.3899e-08 - learning_rate: 1.0972e-06\n",
      "Epoch 579/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4338e-08 - val_loss: 2.3889e-08 - learning_rate: 1.0972e-06\n",
      "Epoch 580/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3961e-08 - val_loss: 2.3910e-08 - learning_rate: 1.0972e-06\n",
      "Epoch 581/700\n",
      "\u001b[1m 76/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 2.4151e-08\n",
      "Epoch 581: ReduceLROnPlateau reducing learning rate to 1.0423130788694833e-06.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4139e-08 - val_loss: 2.3907e-08 - learning_rate: 1.0972e-06\n",
      "Epoch 582/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4132e-08 - val_loss: 2.3890e-08 - learning_rate: 1.0423e-06\n",
      "Epoch 583/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4034e-08 - val_loss: 2.3892e-08 - learning_rate: 1.0423e-06\n",
      "Epoch 584/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4144e-08 - val_loss: 2.3875e-08 - learning_rate: 1.0423e-06\n",
      "Epoch 585/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3957e-08 - val_loss: 2.3880e-08 - learning_rate: 1.0423e-06\n",
      "Epoch 586/700\n",
      "\u001b[1m 81/129\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 632us/step - loss: 2.4461e-08\n",
      "Epoch 586: ReduceLROnPlateau reducing learning rate to 9.901974465265084e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4368e-08 - val_loss: 2.3897e-08 - learning_rate: 1.0423e-06\n",
      "Epoch 587/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4094e-08 - val_loss: 2.3877e-08 - learning_rate: 9.9020e-07\n",
      "Epoch 588/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4282e-08 - val_loss: 2.3876e-08 - learning_rate: 9.9020e-07\n",
      "Epoch 589/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4077e-08 - val_loss: 2.3881e-08 - learning_rate: 9.9020e-07\n",
      "Epoch 590/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4060e-08 - val_loss: 2.3906e-08 - learning_rate: 9.9020e-07\n",
      "Epoch 591/700\n",
      "\u001b[1m 76/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 2.4289e-08\n",
      "Epoch 591: ReduceLROnPlateau reducing learning rate to 9.406875904005574e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4264e-08 - val_loss: 2.3876e-08 - learning_rate: 9.9020e-07\n",
      "Epoch 592/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4053e-08 - val_loss: 2.3877e-08 - learning_rate: 9.4069e-07\n",
      "Epoch 593/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4160e-08 - val_loss: 2.3891e-08 - learning_rate: 9.4069e-07\n",
      "Epoch 594/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4107e-08 - val_loss: 2.3889e-08 - learning_rate: 9.4069e-07\n",
      "Epoch 595/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3977e-08 - val_loss: 2.3874e-08 - learning_rate: 9.4069e-07\n",
      "Epoch 596/700\n",
      "\u001b[1m 76/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 2.4288e-08\n",
      "Epoch 596: ReduceLROnPlateau reducing learning rate to 8.936532054804047e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4238e-08 - val_loss: 2.3876e-08 - learning_rate: 9.4069e-07\n",
      "Epoch 597/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3976e-08 - val_loss: 2.3876e-08 - learning_rate: 8.9365e-07\n",
      "Epoch 598/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3871e-08 - val_loss: 2.3875e-08 - learning_rate: 8.9365e-07\n",
      "Epoch 599/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4153e-08 - val_loss: 2.3869e-08 - learning_rate: 8.9365e-07\n",
      "Epoch 600/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4098e-08 - val_loss: 2.3896e-08 - learning_rate: 8.9365e-07\n",
      "Epoch 601/700\n",
      "\u001b[1m124/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 2.4106e-08\n",
      "Epoch 601: ReduceLROnPlateau reducing learning rate to 8.489705209058229e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4107e-08 - val_loss: 2.3866e-08 - learning_rate: 8.9365e-07\n",
      "Epoch 602/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3870e-08 - val_loss: 2.3887e-08 - learning_rate: 8.4897e-07\n",
      "Epoch 603/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4039e-08 - val_loss: 2.3874e-08 - learning_rate: 8.4897e-07\n",
      "Epoch 604/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4259e-08 - val_loss: 2.3913e-08 - learning_rate: 8.4897e-07\n",
      "Epoch 605/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4118e-08 - val_loss: 2.3867e-08 - learning_rate: 8.4897e-07\n",
      "Epoch 606/700\n",
      "\u001b[1m 77/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 2.4030e-08\n",
      "Epoch 606: ReduceLROnPlateau reducing learning rate to 8.065219759600949e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4081e-08 - val_loss: 2.3882e-08 - learning_rate: 8.4897e-07\n",
      "Epoch 607/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4148e-08 - val_loss: 2.3864e-08 - learning_rate: 8.0652e-07\n",
      "Epoch 608/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4339e-08 - val_loss: 2.3866e-08 - learning_rate: 8.0652e-07\n",
      "Epoch 609/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4093e-08 - val_loss: 2.3864e-08 - learning_rate: 8.0652e-07\n",
      "Epoch 610/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4012e-08 - val_loss: 2.3867e-08 - learning_rate: 8.0652e-07\n",
      "Epoch 611/700\n",
      "\u001b[1m 77/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 2.4257e-08\n",
      "Epoch 611: ReduceLROnPlateau reducing learning rate to 7.66195896062527e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4234e-08 - val_loss: 2.3862e-08 - learning_rate: 8.0652e-07\n",
      "Epoch 612/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4231e-08 - val_loss: 2.3889e-08 - learning_rate: 7.6620e-07\n",
      "Epoch 613/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4061e-08 - val_loss: 2.3880e-08 - learning_rate: 7.6620e-07\n",
      "Epoch 614/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4080e-08 - val_loss: 2.3867e-08 - learning_rate: 7.6620e-07\n",
      "Epoch 615/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4017e-08 - val_loss: 2.3864e-08 - learning_rate: 7.6620e-07\n",
      "Epoch 616/700\n",
      "\u001b[1m 63/129\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 2.4117e-08\n",
      "Epoch 616: ReduceLROnPlateau reducing learning rate to 7.278861147597126e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4124e-08 - val_loss: 2.3865e-08 - learning_rate: 7.6620e-07\n",
      "Epoch 617/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4076e-08 - val_loss: 2.3876e-08 - learning_rate: 7.2789e-07\n",
      "Epoch 618/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4199e-08 - val_loss: 2.3879e-08 - learning_rate: 7.2789e-07\n",
      "Epoch 619/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4063e-08 - val_loss: 2.3863e-08 - learning_rate: 7.2789e-07\n",
      "Epoch 620/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3964e-08 - val_loss: 2.3860e-08 - learning_rate: 7.2789e-07\n",
      "Epoch 621/700\n",
      "\u001b[1m 71/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 722us/step - loss: 2.4312e-08\n",
      "Epoch 621: ReduceLROnPlateau reducing learning rate to 6.914918117217894e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4231e-08 - val_loss: 2.3861e-08 - learning_rate: 7.2789e-07\n",
      "Epoch 622/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4372e-08 - val_loss: 2.3858e-08 - learning_rate: 6.9149e-07\n",
      "Epoch 623/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4112e-08 - val_loss: 2.3871e-08 - learning_rate: 6.9149e-07\n",
      "Epoch 624/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4135e-08 - val_loss: 2.3901e-08 - learning_rate: 6.9149e-07\n",
      "Epoch 625/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3795e-08 - val_loss: 2.3866e-08 - learning_rate: 6.9149e-07\n",
      "Epoch 626/700\n",
      "\u001b[1m 75/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 2.4451e-08\n",
      "Epoch 626: ReduceLROnPlateau reducing learning rate to 6.569172427361991e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4306e-08 - val_loss: 2.3863e-08 - learning_rate: 6.9149e-07\n",
      "Epoch 627/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4075e-08 - val_loss: 2.3859e-08 - learning_rate: 6.5692e-07\n",
      "Epoch 628/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4119e-08 - val_loss: 2.3857e-08 - learning_rate: 6.5692e-07\n",
      "Epoch 629/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4345e-08 - val_loss: 2.3859e-08 - learning_rate: 6.5692e-07\n",
      "Epoch 630/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4298e-08 - val_loss: 2.3857e-08 - learning_rate: 6.5692e-07\n",
      "Epoch 631/700\n",
      "\u001b[1m 74/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 2.4087e-08\n",
      "Epoch 631: ReduceLROnPlateau reducing learning rate to 6.240713616989524e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4085e-08 - val_loss: 2.3858e-08 - learning_rate: 6.5692e-07\n",
      "Epoch 632/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4045e-08 - val_loss: 2.3859e-08 - learning_rate: 6.2407e-07\n",
      "Epoch 633/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4226e-08 - val_loss: 2.3855e-08 - learning_rate: 6.2407e-07\n",
      "Epoch 634/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3995e-08 - val_loss: 2.3870e-08 - learning_rate: 6.2407e-07\n",
      "Epoch 635/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4281e-08 - val_loss: 2.3859e-08 - learning_rate: 6.2407e-07\n",
      "Epoch 636/700\n",
      "\u001b[1m 76/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 2.4191e-08\n",
      "Epoch 636: ReduceLROnPlateau reducing learning rate to 5.928678206146287e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4150e-08 - val_loss: 2.3854e-08 - learning_rate: 6.2407e-07\n",
      "Epoch 637/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4365e-08 - val_loss: 2.3857e-08 - learning_rate: 5.9287e-07\n",
      "Epoch 638/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4277e-08 - val_loss: 2.3876e-08 - learning_rate: 5.9287e-07\n",
      "Epoch 639/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4150e-08 - val_loss: 2.3855e-08 - learning_rate: 5.9287e-07\n",
      "Epoch 640/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4139e-08 - val_loss: 2.3871e-08 - learning_rate: 5.9287e-07\n",
      "Epoch 641/700\n",
      "\u001b[1m 73/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 2.3996e-08\n",
      "Epoch 641: ReduceLROnPlateau reducing learning rate to 5.632244295838972e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4028e-08 - val_loss: 2.3860e-08 - learning_rate: 5.9287e-07\n",
      "Epoch 642/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4299e-08 - val_loss: 2.3894e-08 - learning_rate: 5.6322e-07\n",
      "Epoch 643/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4238e-08 - val_loss: 2.3865e-08 - learning_rate: 5.6322e-07\n",
      "Epoch 644/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3903e-08 - val_loss: 2.3859e-08 - learning_rate: 5.6322e-07\n",
      "Epoch 645/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4055e-08 - val_loss: 2.3858e-08 - learning_rate: 5.6322e-07\n",
      "Epoch 646/700\n",
      "\u001b[1m 73/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 2.4176e-08\n",
      "Epoch 646: ReduceLROnPlateau reducing learning rate to 5.350632108047648e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4139e-08 - val_loss: 2.3859e-08 - learning_rate: 5.6322e-07\n",
      "Epoch 647/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3996e-08 - val_loss: 2.3862e-08 - learning_rate: 5.3506e-07\n",
      "Epoch 648/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4118e-08 - val_loss: 2.3854e-08 - learning_rate: 5.3506e-07\n",
      "Epoch 649/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3995e-08 - val_loss: 2.3857e-08 - learning_rate: 5.3506e-07\n",
      "Epoch 650/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3947e-08 - val_loss: 2.3873e-08 - learning_rate: 5.3506e-07\n",
      "Epoch 651/700\n",
      "\u001b[1m 71/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 2.4265e-08\n",
      "Epoch 651: ReduceLROnPlateau reducing learning rate to 5.083100745650882e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4223e-08 - val_loss: 2.3886e-08 - learning_rate: 5.3506e-07\n",
      "Epoch 652/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4161e-08 - val_loss: 2.3858e-08 - learning_rate: 5.0831e-07\n",
      "Epoch 653/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4046e-08 - val_loss: 2.3892e-08 - learning_rate: 5.0831e-07\n",
      "Epoch 654/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3875e-08 - val_loss: 2.3869e-08 - learning_rate: 5.0831e-07\n",
      "Epoch 655/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4187e-08 - val_loss: 2.3863e-08 - learning_rate: 5.0831e-07\n",
      "Epoch 656/700\n",
      "\u001b[1m 72/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 2.4133e-08\n",
      "Epoch 656: ReduceLROnPlateau reducing learning rate to 4.828945492363346e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4118e-08 - val_loss: 2.3850e-08 - learning_rate: 5.0831e-07\n",
      "Epoch 657/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3834e-08 - val_loss: 2.3862e-08 - learning_rate: 4.8289e-07\n",
      "Epoch 658/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4062e-08 - val_loss: 2.3851e-08 - learning_rate: 4.8289e-07\n",
      "Epoch 659/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4065e-08 - val_loss: 2.3867e-08 - learning_rate: 4.8289e-07\n",
      "Epoch 660/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3946e-08 - val_loss: 2.3860e-08 - learning_rate: 4.8289e-07\n",
      "Epoch 661/700\n",
      "\u001b[1m 71/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 2.3991e-08\n",
      "Epoch 661: ReduceLROnPlateau reducing learning rate to 4.587498352748298e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4033e-08 - val_loss: 2.3853e-08 - learning_rate: 4.8289e-07\n",
      "Epoch 662/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3989e-08 - val_loss: 2.3849e-08 - learning_rate: 4.5875e-07\n",
      "Epoch 663/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4015e-08 - val_loss: 2.3848e-08 - learning_rate: 4.5875e-07\n",
      "Epoch 664/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4142e-08 - val_loss: 2.3852e-08 - learning_rate: 4.5875e-07\n",
      "Epoch 665/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4103e-08 - val_loss: 2.3847e-08 - learning_rate: 4.5875e-07\n",
      "Epoch 666/700\n",
      "\u001b[1m 75/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 2.4422e-08\n",
      "Epoch 666: ReduceLROnPlateau reducing learning rate to 4.358123462111507e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4318e-08 - val_loss: 2.3848e-08 - learning_rate: 4.5875e-07\n",
      "Epoch 667/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3931e-08 - val_loss: 2.3846e-08 - learning_rate: 4.3581e-07\n",
      "Epoch 668/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4000e-08 - val_loss: 2.3895e-08 - learning_rate: 4.3581e-07\n",
      "Epoch 669/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4004e-08 - val_loss: 2.3876e-08 - learning_rate: 4.3581e-07\n",
      "Epoch 670/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4049e-08 - val_loss: 2.3847e-08 - learning_rate: 4.3581e-07\n",
      "Epoch 671/700\n",
      "\u001b[1m 72/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 2.4084e-08\n",
      "Epoch 671: ReduceLROnPlateau reducing learning rate to 4.1402173565074915e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4117e-08 - val_loss: 2.3873e-08 - learning_rate: 4.3581e-07\n",
      "Epoch 672/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4035e-08 - val_loss: 2.3847e-08 - learning_rate: 4.1402e-07\n",
      "Epoch 673/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4233e-08 - val_loss: 2.3845e-08 - learning_rate: 4.1402e-07\n",
      "Epoch 674/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4126e-08 - val_loss: 2.3865e-08 - learning_rate: 4.1402e-07\n",
      "Epoch 675/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4296e-08 - val_loss: 2.3846e-08 - learning_rate: 4.1402e-07\n",
      "Epoch 676/700\n",
      "\u001b[1m 77/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 2.4320e-08\n",
      "Epoch 676: ReduceLROnPlateau reducing learning rate to 3.933206542683365e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4272e-08 - val_loss: 2.3844e-08 - learning_rate: 4.1402e-07\n",
      "Epoch 677/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4080e-08 - val_loss: 2.3851e-08 - learning_rate: 3.9332e-07\n",
      "Epoch 678/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4003e-08 - val_loss: 2.3853e-08 - learning_rate: 3.9332e-07\n",
      "Epoch 679/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3899e-08 - val_loss: 2.3855e-08 - learning_rate: 3.9332e-07\n",
      "Epoch 680/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4077e-08 - val_loss: 2.3866e-08 - learning_rate: 3.9332e-07\n",
      "Epoch 681/700\n",
      "\u001b[1m 68/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 2.4285e-08\n",
      "Epoch 681: ReduceLROnPlateau reducing learning rate to 3.736546148047637e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4211e-08 - val_loss: 2.3852e-08 - learning_rate: 3.9332e-07\n",
      "Epoch 682/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3948e-08 - val_loss: 2.3845e-08 - learning_rate: 3.7365e-07\n",
      "Epoch 683/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4078e-08 - val_loss: 2.3853e-08 - learning_rate: 3.7365e-07\n",
      "Epoch 684/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4272e-08 - val_loss: 2.3847e-08 - learning_rate: 3.7365e-07\n",
      "Epoch 685/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3919e-08 - val_loss: 2.3851e-08 - learning_rate: 3.7365e-07\n",
      "Epoch 686/700\n",
      "\u001b[1m 75/129\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 2.4328e-08\n",
      "Epoch 686: ReduceLROnPlateau reducing learning rate to 3.549718840645255e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4208e-08 - val_loss: 2.3868e-08 - learning_rate: 3.7365e-07\n",
      "Epoch 687/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4297e-08 - val_loss: 2.3850e-08 - learning_rate: 3.5497e-07\n",
      "Epoch 688/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4026e-08 - val_loss: 2.3844e-08 - learning_rate: 3.5497e-07\n",
      "Epoch 689/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4102e-08 - val_loss: 2.3849e-08 - learning_rate: 3.5497e-07\n",
      "Epoch 690/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4442e-08 - val_loss: 2.3856e-08 - learning_rate: 3.5497e-07\n",
      "Epoch 691/700\n",
      "\u001b[1m123/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 2.4141e-08\n",
      "Epoch 691: ReduceLROnPlateau reducing learning rate to 3.372232939113928e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4138e-08 - val_loss: 2.3860e-08 - learning_rate: 3.5497e-07\n",
      "Epoch 692/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3986e-08 - val_loss: 2.3864e-08 - learning_rate: 3.3722e-07\n",
      "Epoch 693/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4125e-08 - val_loss: 2.3843e-08 - learning_rate: 3.3722e-07\n",
      "Epoch 694/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4046e-08 - val_loss: 2.3849e-08 - learning_rate: 3.3722e-07\n",
      "Epoch 695/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3968e-08 - val_loss: 2.3850e-08 - learning_rate: 3.3722e-07\n",
      "Epoch 696/700\n",
      "\u001b[1m 70/129\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 2.4131e-08\n",
      "Epoch 696: ReduceLROnPlateau reducing learning rate to 3.203621332659168e-07.\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4132e-08 - val_loss: 2.3849e-08 - learning_rate: 3.3722e-07\n",
      "Epoch 697/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4289e-08 - val_loss: 2.3844e-08 - learning_rate: 3.2036e-07\n",
      "Epoch 698/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4205e-08 - val_loss: 2.3844e-08 - learning_rate: 3.2036e-07\n",
      "Epoch 699/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3816e-08 - val_loss: 2.3849e-08 - learning_rate: 3.2036e-07\n",
      "Epoch 700/700\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4118e-08 - val_loss: 2.3863e-08 - learning_rate: 3.2036e-07\n"
     ]
    }
   ],
   "source": [
    "# Define training parameters\n",
    "maxEpochs = 700 # val_loss ~ 3.5e-8, NMSE=-33.416, n_prev = 4. [increase to 700 if didn't achieve this]\n",
    "miniBatchSize = 1024\n",
    "iterPerEpoch = len(X_train) // miniBatchSize\n",
    "validation_freq = 2 * iterPerEpoch\n",
    "\n",
    "\n",
    "# history = model.fit(X_train, (y_train.y_real, y_train.y_img), epochs = 200, batch_size = 10,\n",
    "#                     validation_data = (X_val, (y_val.y_real, y_val.y_img)))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=maxEpochs,\n",
    "    batch_size=miniBatchSize,\n",
    "    callbacks=[checkpoint_callback,reduce_lr],\n",
    "    validation_data = (X_val, y_val),\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('tf_model_33_5dB_nprev_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('tf_model_single_y.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = tf.reshape(model.predict(X_test),(131520,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helperNMSE(tf.convert_to_tensor(y_test,dtype=tf.float32),y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(history.history['loss'])).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4110/4110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 443us/step\n"
     ]
    }
   ],
   "source": [
    "tf_output = model.predict(preprocess(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tf_output).to_csv('tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "\n",
    "for i in range(len(tf_output)):\n",
    "    arr.append(f'{tf_output[i][0]} + {tf_output[i][1]}i')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(arr).to_csv('tf_model_output_test.csv',index=False,header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "# Function to parse complex numbers from the given format\n",
    "def parse_complex_number(s):\n",
    "    real, imag = s.split(' + ')\n",
    "    real = float(real)\n",
    "    imag = float(imag.replace('i', ''))\n",
    "    return np.complex64(real + 1j * imag)\n",
    "\n",
    "# Read the CSV file\n",
    "filename = './tf_model_output_test.csv'  # Replace with your CSV filename\n",
    "with open(filename, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Parse the lines into complex numbers\n",
    "complex_numbers = np.array([parse_complex_number(line.strip()) for line in lines], dtype=np.complex64)\n",
    "\n",
    "# Save the array to a .mat file\n",
    "output_filename = 'complex_data.mat'\n",
    "scipy.io.savemat(output_filename, {'complex_data': complex_numbers})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to ONNX For the use in fpgaConvnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">744</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">475</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ y1_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ y2_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │        \u001b[38;5;34m480\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)        │        \u001b[38;5;34m744\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)        │        \u001b[38;5;34m475\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ y1_output (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m20\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ y2_output (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m20\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,739</span> (6.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,739\u001b[0m (6.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,739</span> (6.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,739\u001b[0m (6.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "\n",
    "input_signature =[tf.TensorSpec([None,15], tf.float32, name='x')]\n",
    "\n",
    "# Use from_function for tf functions\n",
    "\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature)\n",
    "\n",
    "onnx.save(onnx_model, \"tf_model_33_5dB_nprev_4.onnx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
